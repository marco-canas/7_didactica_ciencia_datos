{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/innovaciones/blob/main/prop/reduccion_dim_clasificacion_regresion/0_intro_python/dia_9_first_model/pca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marco-canas/innovaciones/blob/main/prop/reduccion_dim_clasificacion_regresion/0_intro_python/dia_9_first_model/pca.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`class sklearn.decomposition.PCA(n_components=None, *, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)[source]` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Análisis de componentes principales (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Reducción de dimensionalidad lineal usando \"Descomposición de valores singulares\" (Singular Value Decomposition) de los datos para proyectarlos a un espacio dimensional inferior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los datos de entrada se centran pero no se escalan para cada característica antes de aplicar el SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Read more in the [User Guide](https://scikit-learn.org/stable/modules/decomposition.html#pca)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Guía de usuario "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2.5. Decomposing signals in components (matrix factorization problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.5.1. Principal component analysis (PCA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.5.1.1. Exact PCA and probabilistic interpretation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`PCA` se utiliza para descomponer un conjunto de datos multivariante en un conjunto de componentes ortogonales sucesivos que explican una cantidad máxima de la varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In `scikit-learn`, `PCA` is implemented as a transformer object that learns  components in its fit method, and can be used on new data to project it on these components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`PCA` centers but does not scale the input data for each feature before applying the SVD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The optional parameter whiten=True makes it possible to project the data onto the singular space while scaling each component to unit variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A continuación se muestra un ejemplo del conjunto de datos del iris, que se compone de 4 características, proyectadas en las 2 dimensiones que explican la mayor variación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_lda_0011.png'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `PCA` object also provides a probabilistic interpretation of the PCA that can give a likelihood of data based on the amount of variance it explains. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As such it implements a score method that can be used in cross-validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_fa_model_selection_0011.png'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples:\n",
    "\n",
    "* [Comparison of LDA and PCA 2D projection of Iris dataset](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py)\n",
    "\n",
    "* [Model selection with Probabilistic PCA and Factor Analysis (FA)](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.5.1.2. Incremental PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The PCA object is very useful, but has certain limitations for large datasets. The biggest limitation is that PCA only supports batch processing, which means all of the data to be processed must fit in main memory. The IncrementalPCA object uses a different form of processing and allows for partial computations which almost exactly match the results of PCA while processing the data in a minibatch fashion. IncrementalPCA makes it possible to implement out-of-core Principal Component Analysis either by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using its partial_fit method on chunks of data fetched sequentially from the local hard drive or a network database.\n",
    "\n",
    "Calling its fit method on a sparse matrix or a memory mapped file using numpy.memmap.\n",
    "\n",
    "IncrementalPCA only stores estimates of component and noise variances, in order update explained_variance_ratio_ incrementally. This is why memory usage depends on the number of samples per batch, rather than the number of samples to be processed in the dataset.\n",
    "\n",
    "As in PCA, IncrementalPCA centers but does not scale the input data for each feature before applying the SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://scikit-learn.org/stable/_images/sphx_glr_plot_incremental_pca_0011.png'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://scikit-learn.org/stable/_images/sphx_glr_plot_incremental_pca_0021.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.375"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "55*2.5/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "The Motivation Part\n",
    "Well, we all know that clear naming is crucial for readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "La parte de la motivación\n",
    "Bueno, todos sabemos que la nomenclatura clara es crucial para la legibilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Still, I see a function named get_user in every project. I renamed it to fetch_user_from_db in one project and toclear_user_data in another. As you can see, the functions had the same name but had nothing in common.\n",
    "The most important part of a function name is a verb. The most popular verb is “get”, and it is a very lazy choice: it gives us none of the additional info. For example, you can replace “get” with “fetch”, then I would know that the function receives data from the network/disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Aún así, veo una función llamada get _user en cada proyecto. Lo renombré para buscar_ usuario _de_ db en un proyecto y borrar los datos de _usuario_ en otro. Como puede ver, las funciones tenían el mismo nombre pero no tenían nada en común.\n",
    "La parte más importante del nombre de una función es un verbo. El verbo más popular es \"obtener\", y es una elección muy perezosa: no nos da ninguna información adicional. Por ejemplo, puede reemplazar \"get\" con \"fetch\", entonces sabría que la función recibe datos de la red / disco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Below you can see The Naming Algorithm: a connection between common verbs and their meanings. Use it for great good and forget about “get” and “process”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "A continuación puede ver El algoritmo de denominación: una conexión entre los verbos comunes y sus significados. Úselo para un gran bien y olvídese de \"obtener\" y \"procesar\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "The Naming Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "El algoritmo de denominación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "3) From Google Drive via PyDrive\n",
    "This is the most complicated of the three methods. I’ll show it for those that have uploaded CSV files into their Google Drive for workflow control. First, type in the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "3) Desde Google Drive a través de PyDrive\n",
    "Este es el más complicado de los tres métodos. Lo mostraré para aquellos que hayan subido archivos CSV a su Google Drive para el control del flujo de trabajo. Primero, escriba el siguiente código:"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "es",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
