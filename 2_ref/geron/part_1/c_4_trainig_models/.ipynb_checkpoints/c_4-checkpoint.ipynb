{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"center\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/c_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Lista de reproducción en YouTube para acompañar la lectura experimental de este capítulo](https://www.youtube.com/watch?v=FRqaBqy3prU&list=PLbk60veMSVKv7dAGRFIBwUf34TFDL0FdW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Capítulo 4\n",
    "## ¿Qué significa entrenar un modelo? \n",
    "Aurelien Gerón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hemos tratado \n",
    "* los **modelos de Machine Learning** y \n",
    "* sus **algoritmos de entrenamiento** \n",
    "\n",
    "en su mayoría como cajas negras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es mucho todo lo que puede hacer sin saber nada sobre lo que hay debajo del capó:  \n",
    "\n",
    "* optimizar un **sistema de regresión**,   \n",
    "* mejorar un **clasificador de imágenes de dígitos** e incluso \n",
    "* construir un clasificador de spam desde cero,   \n",
    "\n",
    "todo esto sin saber cómo funcionan realmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "De hecho, en muchas situaciones no es necesario conocer los detalles de implementación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sin embargo, tener una buena comprensión de cómo funcionan las cosas puede ayudarlo a:\n",
    "\n",
    "* encontrar rápidamente el **modelo** apropiado, \n",
    "* el **algoritmo de entrenamiento** adecuado para usar y \n",
    "* un buen **conjunto de hiperparámetros** para su tarea. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comprender qué hay debajo del capó también lo ayudará a \n",
    "* depurar problemas y \n",
    "* realizar análisis de errores de manera más eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "la mayoría de los temas discutidos en este capítulo serán esenciales para \n",
    "\n",
    "* comprender, \n",
    "* construir y \n",
    "* entrenar redes neuronales \n",
    "\n",
    "(discutidos en la Parte II de este libro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Habrá bastantes ecuaciones matemáticas en este capítulo, \n",
    "* usando nociones básicas de álgebra lineal y cálculo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para comprender estas ecuaciones, necesitarás saber:  \n",
    "\n",
    "* qué son los vectores y las matrices; \n",
    "* cómo transponerlos, multiplicarlos e invertirlos; y \n",
    "* qué son las derivadas parciales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si no está familiarizado con estos conceptos, consulte los tutoriales introductorios de cálculo y álgebra lineal disponibles como cuadernos de Jupyter en el material complementario en línea: https://github.com/ageron/handson-ml3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para aquellos que son realmente alérgicos a las matemáticas, aún deben leer este capítulo y simplemente omitir las ecuaciones; \n",
    "* con suerte, el texto será suficiente para ayudarlo a comprender la mayoría de los conceptos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En el Capítulo 1 analizamos un modelo de regresión simple de calidad de vida:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejemplo de Entrenamiento y ejecución de un modelo lineal con Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_country_stats(oecd_bli, gdp_per_capita):\n",
    "    '''\n",
    "    esta función prepara estadísticas del pais\n",
    "    '''\n",
    "    oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\n",
    "    oecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n",
    "    gdp_per_capita.rename(columns={\"2015\": \"GDP per capita\"}, inplace=True)\n",
    "    gdp_per_capita.set_index(\"Country\", inplace=True)\n",
    "    full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita,\n",
    "                                  left_index=True, right_index=True)\n",
    "    full_country_stats.sort_values(by=\"GDP per capita\", inplace=True)\n",
    "    remove_indices = [0, 1, 6, 8, 33, 34, 35]\n",
    "    keep_indices = list(set(range(36)) - set(remove_indices))\n",
    "    return full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[keep_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.96242338]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model\n",
    "# Load the data and \n",
    "oecd_bli = pd.read_csv(\"oecd_bli_2015.csv\", thousands=',')\n",
    "# thousand significa miles \n",
    "gdp_per_capita = pd.read_csv(\"gdp_per_capita.csv\", thousands=',', delimiter='\\t', encoding='latin1', na_values=\"n/a\")\n",
    "# Prepare the data\n",
    "country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\n",
    "X = np.c_[country_stats[\"GDP per capita\"]]\n",
    "y = np.c_[country_stats[\"Life satisfaction\"]]\n",
    "# Visualize the data\n",
    "country_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\n",
    "plt.show()\n",
    "# Select a linear model\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "# Make a prediction for Cyprus \n",
    "X_new = [[22587]] # Cyprus's GDP per capita\n",
    "print(model.predict(X_new)) # outputs [[ 5.96242338]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ¿Cómo funciona un modelo lineal?\n",
    "* un modelo lineal hace una predicción simplemente calculando una **suma ponderada** de los atributos de entrada, \n",
    "* más una constante llamada **término de sesgo** (también llamado **término de intersección**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 4-1. Modelo predictivo de regresión lineal\n",
    "\n",
    "$$ \\hat{y} = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\cdots + \\theta_{n}x_{n} $$\n",
    "\n",
    "$$ \\hat{y} = \\theta_{0}(1) + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\cdots + \\theta_{n}x_{n} $$\n",
    "\n",
    "$$ \\hat{y} = \\mathbf{\\theta} \\cdot \\mathbf{x}_{b}, \\ \\ \\ \\ \\ \\ \\ \\mathbf{x}_{b} = \\begin{pmatrix} 1 \\\\ x_{1} \\\\ \\vdots \\end{pmatrix} $$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\hat{y}$ es el valor predicho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$n$ es el número de características o atributos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$x_{i}$ es el valor del $i$ - ésimo atributo. () Geron utiliza subíndice para las columnas de la matriz de datos `X`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\theta_{j}$ es el $j$ - ésimo parámetro del modelo \n",
    "(incluido el término de sesgo $\\theta_{0}$ y los pesos de los atributos $\\theta_{1},\\theta_{2},\\ldots, \\theta_{n}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto se puede escribir de manera mucho más concisa usando una forma vectorizada, como se muestra en la siguiente ecuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Predicción del modelo de regresión lineal (forma vectorizada)   \n",
    "\n",
    "\n",
    "$$ \\hat{y} = h_{\\mathbf{\\theta}}(\\mathbf{x}_{b}) = \\mathbf{\\theta} \\cdot \\mathbf{x}_{b} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En esta ecuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\mathbf{\\theta}$ es el vector de parámetros del modelo, que contiene el término de sesgo $\\theta_{0}$ y el de los pesos de los atributos $\\theta_{1}$ a $\\theta_{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\mathbf{x}_{b}$ es el **vector de atributos** o de instancias, que contiene $x_{0}$ a $x_{n}$, con $x_{0}$ siempre igual a $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\mathbf{\\theta} \\cdot \\mathbf{x}_{b}$ es el producto escalar de los vectores $\\mathbf{\\theta}$ y $\\mathbf{x}_{b}$, que por supuesto es igual a $\\theta_{0}x_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\cdots + \\theta_{n}x_{n}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $h_{\\mathbf{\\theta}}$ es la función hipótesis asociada a los parámetros $\\mathbf{\\theta}$ del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NOTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Si $\\mathbf{\\theta}$ y $\\mathbf{x}_{b}$ son vectores de columna, entonces la predicción es \n",
    "\n",
    "$$ \\hat{y} = \\mathbf{\\theta}^{T}\\mathbf{x}_{b}, $$\n",
    "\n",
    "donde $\\mathbf{\\theta}^{T}$ es la transposición de $\\mathbf{\\theta}$ (un vector de fila en lugar de un vector de columna) y   \n",
    "$\\mathbf{\\theta}^{T}\\mathbf{x}_{b}$ es el producto matricial las matrices $\\theta^{T}$ y $\\mathbf{x}_{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* este es el modelo de regresión lineal,   \n",
    "* pero ¿cómo lo entrenamos?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recuerde que **entrenar un modelo** significa **establecer sus parámetros** para que el modelo se ajuste mejor al **conjunto de entrenamiento**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para este propósito, primero necesitamos una medida de qué tan bien (o mal) el modelo se ajusta a los **datos de entrenamiento**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La medida de rendimiento más común de un modelo de regresión es **la raíz del error cuadrático medio (RMSE)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ RMSE(\\theta) = \\sqrt{\\frac{1}{m}\\sum_{j = 0}^{n} (\\theta^{T} x^{j} - y^{j})^{2}} $$\n",
    "\n",
    "donde $x^{j}$ es la $j$ - ésima fila o instancia de la matriz de datos `X_train`.  Es decir, para Geron, el super índice es para denotar las filas de la matriz. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por lo tanto, para entrenar un modelo de regresión lineal, necesitamos encontrar el valor de $\\theta$ que minimice el RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* En la práctica, es más sencillo minimizar el error cuadrático medio (MSE) que el RMSE, \n",
    "* y conduce al mismo resultado (pues el valor que minimiza a la función, minimiza su raíz cuadrada). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Se calcula el MSE de una hipótesis de regresión lineal $h$ en un conjunto de entrenamiento `X_train` usando la ecuación:  \n",
    "\n",
    "$$ RMSE(\\theta) = \\sqrt{\\frac{1}{m}\\sum_{j = 0}^{n} (\\theta^{T} x^{j} - y^{j})^{2}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "o la ecuación:\n",
    "\n",
    "$$ MSE(X,h_{\\theta}) = \\frac{1}{m} \\sum_{i = 1}^{n} (\\mathbf{\\theta}^{T}\\mathbf{x}^{i} - y^{i} )^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La única diferencia es que escribimos $h_{\\mathbf{\\theta}}$ en lugar de solo $h$ para dejar en claro que el modelo está parametrizado por el vector $\\mathbf{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para simplificar las notaciones, simplemente escribiremos $MSE(\\mathbf{\\theta})$ en lugar de $MSE(\\mathbf{X}, h_{\\mathbf{\\theta}})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## La ecuación normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para encontrar el valor de $\\mathbf{\\theta}$ que minimiza la función de costo, existe una solución de forma cerrada, \n",
    "* en otras palabras, una ecuación matemática que da el resultado directamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto se llama **ecuación normal**\n",
    "\n",
    "$$ \\hat{\\mathbf{\\theta}} = (\\mathbf{X}^{T}\\mathbf{X})^{-1} \\mathbf{X}^{T}\\mathbf{y} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En esta ecuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\hat{\\mathbf{\\theta}}$ es el valor de $\\mathbf{\\theta}$ que minimiza la función de costo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\mathbf{y}$ es el vector de valores objetivo que contiene $y^{(1)}$ a $y^{(m)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generemos algunos datos de apariencia lineal para probar esta ecuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#importación de las librerías necesarias \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Fijamos un estado aleatorio para reproducibilidad\n",
    "np.random.seed(48)\n",
    "X = 2 * np.random.rand(100, 1) \n",
    "# Crea una matriz de orden 100 x 1 con valores aleatorios en el intervalo [0,2) \n",
    "# que distribuyen uniforme\n",
    "y = 4 + 3 * X + np.random.randn(100, 1) # crea un vector de etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03498054],\n",
       "       [1.78314653],\n",
       "       [0.56972233],\n",
       "       [0.59795275],\n",
       "       [1.58406852]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5)) \n",
    "#plt.style.use('dark_background') \n",
    "plt.scatter(X,y, color = 'red', label = 'datos artificiales')    \n",
    "plt.xlabel('instancias $X$')\n",
    "plt.ylabel('etiquetas $y$')\n",
    "plt.title('Conjunto de datos lineales generados aleatoriamente')\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(alpha = 0.3) \n",
    "plt.savefig('dataset_lineal_aleatorio.jpg')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora calculemos $\\hat{\\mathbf{\\theta}}$ usando la Ecuación Normal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Usaremos la función ``inv()`` del módulo de álgebra lineal de NumPy `` (np.linalg) `` para calcular la inversa de una matriz, y el método ``.dot()`` para la multiplicación de matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.0117793 ],\n",
       "       [2.92444191]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "# np.c_ concatena matrices \n",
    "X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La función que usamos para generar los datos es \n",
    "\n",
    "$$ y = 4 + 3x + \\text{Ruido Gausiano}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Veamos qué encontró la ecuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.0117793 ],\n",
       "       [2.92444191]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Habríamos esperado $\\theta_{0} = 4$ y $\\theta_{1}=3$ en lugar de $\\theta_{0} = 4.01$ y $\\theta_{1} = 2.924$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lo suficientemente cerca, pero el ruido hizo imposible recuperar los parámetros exactos de la función original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora podemos hacer predicciones usando $\\hat{\\mathbf{\\theta}}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.0117793 ],\n",
       "       [9.86066312]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]])               #  Un nuevo par de datos \n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]    # adiciona x0 = 1 a cada instancia\n",
    "y_predict = X_new_b.dot(theta_best)    # efectúa el producto escalar \n",
    "y_predict # muestra las dos predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Grafiquemos las predicciones de este modelo (Figura 4-2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.scatter(X, y, color = 'blue', label = 'datos artificiales') \n",
    "plt.plot(X_new, y_predict, \"r-\", label = 'regresor')\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('que_significa_entrenar_un_modelo.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Realizar una regresión lineal con Scikit-Learn es simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.0117793]), array([[2.92444191]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression # tomar la clase \n",
    "lin_reg = LinearRegression()            # instanciamos la clase \n",
    "lin_reg.fit(X, y)                # constituyes la función predictora. \n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.0117793 ],\n",
       "       [9.86066312]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]]) # datos nuevos \n",
    "lin_reg.predict(X_new) # predicciones para los dos datos anteriores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La clase ``LinearRegression`` se basa en la función ``scipy.linalg.lstsq()`` (el nombre significa \"mínimos cuadrados\"), a la que puede llamar directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 4.98 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.0117793 ],\n",
       "       [2.92444191]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta función calcula $\\hat{\\theta} = X^{+}y$ donde $X^{+}$ es la pseudoinversa de $X$ (específicamente, la inversa de Moore-Penrose).\n",
    "\n",
    "$$ \\hat{\\theta}  = X^{+}y = (X^{T}X)^{-1}X^{T}y $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Puede usar `np.linalg.pinv()` para calcular la pseudoinversa directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.0117793 ],\n",
       "       [2.92444191]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La pseudoinversa en sí misma se calcula usando una técnica estándar de factorización de matrices llamada *Descomposición en valores singulares* (SVD) que puede descomponer la matriz del conjunto de entrenamiento $\\mathbf{X}$ en la multiplicación de tres matrices $U\\Sigma V^{T}$ (ver ``numpy.linalg.svd()``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La pseudoinversa se calcula como $\\mathbf{X}^{+} = \\mathbf{V\\Sigma}^{+}\\mathbf{U}^{T}$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para calcular la matriz $\\Sigma^{+}$, \n",
    "* el algoritmo toma $\\Sigma$ y \n",
    "* establece en cero todos los valores menores que un pequeño valor de umbral, \n",
    "* luego reemplaza todos los valores distintos de cero con su inverso y \n",
    "* finalmente transpone la matriz resultante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Este enfoque es más eficiente que calcular la Ecuación Normal, además maneja muy bien los casos extremos: \n",
    "* de hecho, la Ecuación Normal puede no funcionar si la matriz $\\mathbf{X}^{T}\\mathbf{X}$ no es invertible (es decir, singular), como  cuando $m < n$ o si tenemos atributos redundantes, pero la pseudoinversa siempre está definida.\n",
    "\n",
    "Donde $n$ es el número de atributos y  \n",
    "$m$ es el número de instancias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La ecuación normal calcula la inversa de $X\\_b^{T}X\\_b$, que es una matriz $(n + 1) \\times (n + 1)$ (donde $n$ es el número de atributos).\n",
    "\n",
    "$$X\\_b^{T}_{(n+1) \\times m} X\\_b_{m\\times (n+1)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compejidad computacional para determinar la inversa de una matriz\n",
    "La complejidad computacional de invertir una matriz de este tipo suele ser de $O(n^{2.4})$ a $O(n^{3})$, según la implementación. \n",
    "\n",
    "Donde $n$ es el número de atributos o columnas del vector $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En otras palabras, si duplica la cantidad $n$ de atributos, se multiplica **el tiempo de cálculo** en un rango aproximado de $2^{2.4} = 5.3$ a $2^{3} = 8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El enfoque SVD utilizado por la clase `LinearRegression` de Scikit-Learn es aproximadamente $O(n^{2})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si duplica la cantidad de atributos, multiplica el **tiempo de cálculo** por aproximadamente 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advertencia "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tanto la ecuación normal como el enfoque SVD se vuelven muy lentos cuando la cantidad $n$ de atributos aumenta (por ejemplo a $100.000$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En el lado positivo, ambos son lineales con respecto a la cantidad de instancias en el conjunto de entrenamiento (son $O(m)$), por lo que manejan grandes conjuntos de entrenamiento de manera eficiente, siempre que quepan en la memoria.  \n",
    "\n",
    "Donde $m$ es el número de instancias o filas de la matriz de datos $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Además, una vez que haya entrenado su modelo de regresión lineal (usando la ecuación normal o cualquier otro algoritmo),  \n",
    "* las predicciones son muy rápidas: la complejidad computacional es lineal con respecto a la cantidad $m$ de instancias en las que desea hacer predicciones y la cantidad de atributos $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En otras palabras, hacer **predicciones** sobre el doble de instancias $m$ (o el doble de atributos $n$) llevará aproximadamente el doble de tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora veremos una forma muy diferente de entrenar un modelo de regresión lineal, que se adapta mejor a los casos en los que \n",
    "\n",
    "* hay una gran cantidad de atributos o \n",
    "* demasiadas instancias de entrenamiento para caber en la memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Gradient Descent es un **algoritmo de optimización** genérico capaz de encontrar soluciones óptimas a una amplia gama de problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La idea general de Gradient Descent es ajustar los parámetros **iterativamente** para minimizar una función de costo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Suponga que está perdido en las montañas en una densa niebla, y   \n",
    "* solo puede sentir la pendiente del suelo debajo de sus pies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Una buena estrategia para llegar rápidamente al fondo del valle es descender en dirección a la pendiente más pronunciada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto es exactamente lo que hace Gradient Descent: \n",
    "* mide el gradiente local de la **función de error** con respecto al vector de parámetros $\\theta$, y va en la dirección del gradiente descendente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Una vez que el gradiente es cero, ¡ha alcanzado un mínimo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Concretamente, comienza inicializando $\\theta$ con valores aleatorios (esto se llama **inicialización aleatoria**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Luego lo mejora gradualmente, dando un pequeño paso a la vez, cada paso intentando disminuir la función de costo (por ejemplo, el MSE), hasta que el algoritmo converge a un mínimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/fig_4_3.JPG?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Un parámetro importante en Gradient Descent es el **tamaño de los pasos**,   \n",
    "\n",
    "* determinado por el hiperparámetro de **tasa de aprendizaje**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si la tasa de aprendizaje es demasiado pequeña, el algoritmo tendrá que pasar por muchas iteraciones para converger, lo que llevará mucho tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/fig_4_4.JPG?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por otro lado, si la tasa de aprendizaje es demasiado alta, es posible que saltes al otro lado del valle y termines en el otro lado, posiblemente incluso más alto de lo que estabas antes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto podría hacer que el algoritmo diverja, con valores cada vez mayores, y no pueda encontrar una buena solución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/fig_4_5.JPG?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por último, no todas las **funciones de costos** se ven como buenos cuencos (son convexas o concavas hacia arriba). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Puede haber agujeros, crestas, mesetas y todo tipo de terrenos irregulares, haciendo dificil la convergencia al mínimo global. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La figura 4-6 muestra los dos desafíos principales del Gradiente descendente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/fig_4_6.JPG?raw=true'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si la inicialización aleatoria comienza el algoritmo por el lado izquierdo, entonces convergen a un mínimo local, que no es tan bueno como el mínimo global. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Si se comienza a la derecha, entonces tomará mucho tiempo cruzar la meseta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Y si si se detiene demasiado pronto, nunca alcanzará el mínimo global."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Afortunadamente, **la función de costo MSE** para un modelo de regresión lineal resulta ser una **función convexa**, lo que significa que si selecciona dos puntos en la curva, el segmento de línea que los une nunca cruza la curva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Esto implica que no hay mínimos locales, solo un mínimo global."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "También es una función continua con una pendiente que nunca cambia abruptamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estos dos hechos tienen una gran consecuencia:\n",
    "* Se garantiza que Gradient Descent se acerque arbitrariamente al mínimo global (si espera lo suficiente y si la tasa de aprendizaje no es demasiado alta)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "De hecho, la **función de costo** tiene forma de cuenco o recipiente, pero puede ser un cuenco alargado si los atributos tienen escalas muy diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* La figura 4-7 muestra el descenso de gradiente en un conjunto de entrenamiento donde los atributos 1 y 2 tienen la misma escala (a la izquierda)   \n",
    "* y en un conjunto de entrenamiento donde el atributo 1 tiene valores mucho más pequeños que el atributo 2 (a la derecha)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/fig_4_7.JPG?raw=true'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Como puedes ver, a la izquierda el algoritmo de Descenso de Gradiente va directo hacia el mínimo, alcanzándolo rápidamente,  \n",
    "* mientras que a la derecha primero va en una dirección casi ortogonal a la dirección del mínimo global, y termina con una marcha larga por un valle casi plano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Eventualmente alcanzará el mínimo, pero tomará mucho tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advertencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Al usar Gradient Descent, debe asegurarse de que todas los atributos tengan una escala similar\n",
    "\n",
    "* por ejemplo usando la clase `StandardScaler` de Scikit-Learn o,\n",
    "\n",
    "de lo contrario, tardará mucho más en converger.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este diagrama también ilustra el hecho de que entrenar un modelo significa buscar una combinación de parámetros del modelo que minimice una función de costo (sobre el conjunto de entrenamiento)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Es una búsqueda en el espacio de parámetros del modelo: \n",
    "* cuantos más parámetros tiene un modelo, más dimensiones tiene este espacio y más difícil es la búsqueda: \n",
    "\n",
    "* buscar una aguja en un pajar de 300 dimensiones es mucho más complicado que en 3 dimensiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Afortunadamente, dado que la función de costo es convexa en el caso de la regresión lineal, la aguja simplemente está en el fondo del recipiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradiente descendente incremental "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Para implementar Gradient Descent, debe calcular el gradiente de la función de costo con respecto a cada parámetro del modelo $\\theta_{j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En otras palabras, debe calcular cuánto cambiará la función de costo si cambia $\\theta_{j}$ solo un poco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A esto se le llama **derivada parcial**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Es como preguntar \"¿Cuál es la pendiente de la montaña bajo mis pies si miro hacia el este?\" \n",
    "* y luego hacer la misma pregunta mirando al norte \n",
    "* (y así sucesivamente para todas las demás dimensiones, si puedes imaginar un universo con más de tres dimensiones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La ecuación 4-5 calcula la derivada parcial de la función de costo con respecto al parámetro $\\theta_{j}$, y la denotamos con $\\frac{\\partial}{\\partial \\theta_{j}} MSE(\\mathbf{\\theta})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ecuación 4-5. Derivadas parciales de la función de costo  \n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta_{j}} MSE(\\theta) = \\frac{2}{m} \\sum_{i = 1}^{m} (\\theta^{T}x^{(i)} - y^{(i)})x_{j}^{(i)}     $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En lugar de calcular estas derivadas parciales individualmente, puede usar la Ecuación 4-6 para calcularlas todas de una sola vez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El vector gradiente, denotado con $\\nabla MSE(\\theta)$, contiene todas las derivadas parciales de la función de costo (una para cada parámetro del modelo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ecuación 4-6. Vector gradiente de la función de costo\n",
    "\n",
    "\n",
    "\n",
    "$$ \\nabla_{\\theta}MSE(\\theta) = \\begin{pmatrix} \\frac{\\partial}{\\partial \\theta_{0}}MSE(\\theta) \\\\ \\frac{\\partial}{\\partial \\theta_{1}}MSE(\\theta) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial \\theta_{n}}MSE(\\theta)   \\end{pmatrix} = \\frac{2}{m} X^{T}(X\\theta - y) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advertencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tenga en cuenta que esta fórmula implica cálculos sobre el conjunto de entrenamiento completo `X_train`, en cada paso de descenso de gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Es por eso que el algoritmo se llama Batch Gradient Descent: \n",
    "* utiliza el lote completo de datos de entrenamiento en cada paso \n",
    "* (en realidad, Full Gradient Descent probablemente sería un mejor nombre)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Como resultado, es terriblemente lento en conjuntos de entrenamiento muy grandes   \n",
    "\n",
    "* (pero pronto veremos algoritmos de descenso de gradiente mucho más rápidos). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Sin embargo, Gradient Descent se escala bien con la cantidad de atributos; \n",
    "\n",
    "* Entrenar un modelo de regresión lineal cuando hay cientos de miles de entidades es mucho más rápido usando el descenso de gradiente que usando la ecuación normal o la descomposición SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una vez que tenga el vector de gradiente, que apunta cuesta arriba, simplemente vaya en la dirección opuesta para ir cuesta abajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto significa restar $\\nabla MSE(\\theta)$ de $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aquí es donde entra en juego la tasa de aprendizaje $\\eta$:  \n",
    "\n",
    "* multiplica el vector de gradiente por $\\eta$ para determinar el tamaño del paso cuesta abajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ecuación 4-7. Paso de descenso de gradiente\n",
    "\n",
    "$$ \\theta^{\\text{next step}} = \\theta - \\eta \\nabla_{\\theta} MSE(\\theta) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Veamos una implementación rápida de este algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 8.98 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.05534262],\n",
       "       [2.90301153]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "import numpy as np \n",
    "np.random.seed(513)\n",
    "X = 2*np.random.rand(100,1)\n",
    "\n",
    "y = 4 + 3*X + np.random.randn(100,1)\n",
    "\n",
    "X_b = np.c_[np.ones((100,1)), X]\n",
    "\n",
    "eta = 0.1      # learning rate\n",
    "n_iterations = 1000  # establezca un número de iteraciones\n",
    "m = 100 # número de instancias \n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "    \n",
    "theta     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "¡Oye, eso es exactamente lo que encontró la ecuación normal! \n",
    "\n",
    "* El Descenso de gradiente funcionó perfectamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pero, ¿y si hubieras utilizado una tasa de aprendizaje eta diferente?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La Figura 4-8 muestra los primeros 10 pasos de Gradient Descent utilizando tres tasas de aprendizaje diferentes (la línea discontinua representa el punto de partida)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "figura 4-8\n",
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/fig_4_8.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent with various learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 28.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.05534262],\n",
       "       [2.90301153]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# importe las librerías necesarias \n",
    "import numpy as np \n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "np.random.seed(513)\n",
    "\n",
    "# cree los datos artificiales \n",
    "\n",
    "X = 2*np.random.rand(100,1)\n",
    "X_b = np.c_[np.ones((100,1)), X]\n",
    "y = 4 + 3*X + np.random.randn(100,1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, y, 'bo')\n",
    "\n",
    "# establecer el rectángulo de visualización\n",
    "a_,b_ = np.min(X) - 0.1, np.max(X) + 0.1\n",
    "c_,d_ = 0,12\n",
    "\n",
    "\n",
    "eta = 0.1      # tasa de aprendizaje pequeña\n",
    "n_iterations = 1000   # establezca un número de iteraciones\n",
    "m = 100 # número de instancias \n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "\n",
    "\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "    #b, m = theta[0][0], theta[1][0]  \n",
    "    #ax = plt.gca()\n",
    "    #ax.plot([a_, b_], [a_*m + b, b_*m+b], 'r-')\n",
    "    #ax.axis([a_, b_, c_, d_])\n",
    "    #plt.grid(alpha = 0.3)\n",
    "    #plt.pause(0.5) \n",
    "     \n",
    "theta     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A la izquierda, la tasa de aprendizaje es demasiado baja: el algoritmo finalmente llegará a la solución, pero llevará mucho tiempo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En el medio, la tasa de aprendizaje parece bastante buena: en solo unas pocas iteraciones, ya ha convergido a la solución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A la derecha, la tasa de aprendizaje es demasiado alta: el algoritmo diverge, salta por todos lados y, de hecho, se aleja más y más de la solución en cada paso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para encontrar una buena tasa de aprendizaje, puede utilizar la búsqueda en cuadrícula (consulte el Capítulo 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sin embargo, es posible que desee limitar el número de iteraciones para que la búsqueda en cuadrícula pueda eliminar los modelos que tardan demasiado en converger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Quizás se pregunte cómo establecer el número de iteraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si es demasiado bajo, aún estará lejos de la solución óptima cuando el algoritmo se detenga; pero si es demasiado alto, perderá tiempo mientras los parámetros del modelo ya no cambian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una solución simple es establecer una gran cantidad de iteraciones pero interrumpir el algoritmo cuando el vector de gradiente se vuelve pequeño, es decir, cuando su norma se vuelve más pequeña que un número pequeño $\\epsilon$ (llamado tolerancia), porque esto sucede cuando Gradient Descent ha (casi) alcanzado el mínimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tasa de convergencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cuando la función de costo es convexa y su pendiente no cambia abruptamente (como es el caso de la función de costo MSE), el descenso de gradiente por lotes con una tasa de aprendizaje fija eventualmente convergerá a la solución óptima, pero es posible que tenga que esperar un poco: puede tomar $O(1/\\epsilon)$ iteraciones para alcanzar el óptimo dentro de un rango de $\\epsilon$, dependiendo de la forma de la función de costo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si divide la tolerancia por 10 para tener una solución más precisa, es posible que el algoritmo deba ejecutarse unas 10 veces más."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El principal problema con Batch Gradient Descent es el hecho de que **utiliza todo el conjunto de entrenamiento para calcular los gradientes en cada paso,** \n",
    "* lo que lo hace muy lento cuando el conjunto de entrenamiento es grande."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En el extremo opuesto, Stochastic Gradient Descent elige una instancia aleatoria en el conjunto de entrenamiento en cada paso y calcula los gradientes basándose únicamente en esa única instancia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Obviamente, trabajar en una sola instancia a la vez hace que el algoritmo sea mucho más rápido porque tiene muy pocos datos para manipular en cada iteración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "También hace posible entrenar en grandes conjuntos de entrenamiento, ya que solo una instancia necesita estar en la memoria en cada iteración (Stochastic GD puede implementarse como un algoritmo fuera del núcleo; consulte el Capítulo 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por otro lado, debido a su naturaleza estocástica (es decir, aleatoria), este algoritmo es mucho menos regular que el descenso de gradiente por lotes: \n",
    "* en lugar de disminuir suavemente hasta llegar al mínimo, la función de costo rebotará hacia arriba y hacia abajo, disminuyendo solo en promedio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Con el tiempo, terminará muy cerca del mínimo, pero una vez que llegue allí, continuará rebotando, sin asentarse nunca (vea la Figura 4-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entonces, una vez que el algoritmo se detiene, los valores finales de los parámetros son buenos, pero no óptimos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/fig_4_9.png?raw=true'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cuando la función de costo es muy irregular (como en la Figura 4-6), esto puede ayudar al algoritmo a saltar fuera de los mínimos locales, por lo que Stochastic Gradient Descent tiene más posibilidades de encontrar el mínimo global que Batch Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por lo tanto, la aleatoriedad es buena para escapar de los óptimos locales, pero mala porque significa que el algoritmo nunca puede establecerse en el mínimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una solución a este dilema es reducir gradualmente la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los pasos comienzan grandes (lo que ayuda a progresar rápidamente y escapar de los mínimos locales), luego se vuelven cada vez más pequeños, lo que permite que el algoritmo se asiente en el mínimo global."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este proceso es similar al recocido simulado, un algoritmo inspirado en el proceso metalúrgico del recocido, donde el metal fundido se enfría lentamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La función que determina la tasa de aprendizaje en cada iteración se denomina programa de aprendizaje. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si la tasa de aprendizaje se reduce demasiado rápido, es posible que se quede atascado en un mínimo local o incluso quede congelado a la mitad del mínimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si la tasa de aprendizaje se reduce demasiado lentamente, puede saltar alrededor del mínimo durante mucho tiempo y terminar con una solución subóptima si detiene el entrenamiento demasiado pronto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este código implementa Stochastic Gradient Descent utilizando un programa de aprendizaje simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 109 ms\n",
      "Wall time: 91.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.03987186],\n",
       "       [2.90644793]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "np.random.seed(513)\n",
    "m = 100 # número de instancias \n",
    "n_epochs = 50 # número de epocas \n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        \n",
    "theta         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por convención iteramos por rondas de iteraciones; cada ronda se llama una época."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mientras que el código Batch Gradient Descent iteró 1000 veces a través de todo el conjunto de entrenamiento, este código pasa por el conjunto de entrenamiento solo 50 veces y llega a una solución bastante buena:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La figura 4-10 muestra los primeros 20 pasos del entrenamiento (observe cuán irregulares son los pasos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "figure 4.10\n",
    "\n",
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/fig_4_10.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tenga en cuenta que dado que las instancias se eligen al azar, algunas instancias pueden elegirse varias veces por época, mientras que otras pueden no elegirse en absoluto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si quiere estar seguro de que el algoritmo pasa por cada instancia en cada época, otro enfoque es barajar el conjunto de entrenamiento (asegurándose de barajar las características de entrada y las etiquetas juntas), luego revisarlo instancia por instancia, luego barajarlo de nuevo, y así sucesivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sin embargo, este enfoque generalmente converge más lentamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advertencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cuando se usa Stochastic Gradient Descent, las instancias de entrenamiento deben ser independientes e idénticamente distribuidas (IID) para garantizar que los parámetros se acerquen al óptimo global, en promedio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una forma sencilla de garantizar esto es barajar las instancias durante el entrenamiento (por ejemplo, elegir cada instancia al azar o barajar el conjunto de entrenamiento al comienzo de cada época)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si no mezcla las instancias, por ejemplo, si las instancias están ordenadas por etiqueta, entonces SGD comenzará optimizando para una etiqueta, luego la siguiente, y así sucesivamente, y no se establecerá cerca del mínimo global."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para realizar una regresión lineal usando Stochastic GD con Scikit-Learn, puede usar la clase `SGDRegressor`, que por defecto optimiza la función de costo del error cuadrático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic gradient descent wit sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El siguiente código se ejecuta durante un máximo de 1000 épocas o hasta que la pérdida cae menos de 0,001 durante una época (`max_iter=1000, tol=1e-3`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comienza con una tasa de aprendizaje de 0,1 (`eta0=0,1`), utilizando el programa de aprendizaje predeterminado (diferente al anterior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por último, no utiliza ninguna regularización (`penalty=None`; más detalles sobre esto en breve):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una vez más, encuentra una solución bastante cercana a la devuelta por la Ecuación Normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El último algoritmo de descenso de gradiente que veremos se llama descenso de gradiente de mini lotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es fácil de entender una vez que conoce Batch y Stochastic Gradient Descent: en cada paso, en lugar de calcular los gradientes basados en el conjunto de entrenamiento completo (como en Batch GD) o en una sola instancia (como en Stochastic GD), Mini- batch GD calcula los gradientes en pequeños conjuntos aleatorios de instancias llamados mini lotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La principal ventaja de Mini-batch GD sobre Stochastic GD es que puede obtener un aumento de rendimiento de la optimización de hardware de las operaciones matriciales, especialmente cuando se usan GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El progreso del algoritmo en el espacio de parámetros es menos errático que con Stochastic GD, especialmente con mini lotes bastante grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Como resultado, Mini-batch GD terminará caminando un poco más cerca del mínimo que Stochastic GD, pero puede ser más difícil para escapar de los mínimos locales (en el caso de problemas que sufren de mínimos locales, a diferencia de la regresión lineal). )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La figura 4-11 muestra las rutas tomadas por los tres algoritmos de descenso de gradiente en el espacio de parámetros durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Todos terminan cerca del mínimo, pero el camino de Batch GD en realidad se detiene en el mínimo, mientras que Stochastic GD y Mini-batch GD continúan caminando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sin embargo, no olvide que Batch GD toma mucho tiempo para dar cada paso, y Stochastic GD y Mini-batch GD también alcanzarían el mínimo si usara un buen programa de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/fig_4_11.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comparemos los algoritmos que hemos discutido hasta ahora para la regresión lineal (recuerde que m es el número de instancias de entrenamiento y n es el número de funciones); consulte la Tabla 4-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "tabla 4.1\n",
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/tabla_4_1.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NOTA  \n",
    "\n",
    "Casi no hay diferencia después del entrenamiento: todos estos algoritmos terminan con modelos muy similares y hacen predicciones exactamente de la misma manera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¿Qué sucede si sus datos son más complejos que una línea recta? Sorprendentemente, puede usar un modelo lineal para ajustar datos no lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una forma sencilla de hacer esto es agregar potencias de cada atributo como atributos nuevos y luego entrenar un modelo lineal en este conjunto ampliado de atributos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta técnica se llama regresión polinomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Veamos un ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Primero, generemos algunos datos no lineales, basados en una ecuación cuadrática simple (más algo de ruido; vea la Figura 4-12):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "figura 4.12  \n",
    "\n",
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/fig_4_12.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Claramente, una línea recta nunca ajustará correctamente estos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entonces, usemos la clase `PolynomialFeatures` de Scikit-Learn para transformar nuestros datos de entrenamiento, agregando el cuadrado (polinomio de segundo grado) de cada función en el conjunto de entrenamiento como una nueva función (en este caso, solo hay una función):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_poly[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`X_poly` ahora contiene el atributo original de `X` más el cuadrado de este atributo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora puede ajustar un modelo de 'Regresión lineal' a estos datos de entrenamiento extendidos (Figura 4-13):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "figura 4.13\n",
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/fig_4_13.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "No está mal: el modelo estima $\\hat{y} = 0,56x_{1}^{2} + 0,93x_{1} + 1,78$ cuando en realidad la función original era $y = 0,5x_{1} 2 + 1,0x_ {1} + 2,0 + $ Ruido gaussiano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tenga en cuenta que cuando hay varias funciones, la regresión polinómica es capaz de encontrar relaciones entre las funciones (que es algo que un modelo de regresión lineal simple no puede hacer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto es posible gracias al hecho de que `PolynomialFeatures` también agrega todas las combinaciones de atributos hasta el grado dado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por ejemplo, si hubiera dos funciones $a$ y $b$, `PolynomialFeatures` con `degree=3` no solo agregaría las funciones $a^{2}$, $a^{3}$, $b^ {2}$ y $b^{3}$, pero también las combinaciones $ab, a^{2}b$ y $ab^{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advertencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`PolynomialFeatures(degree=d)` transforma una matriz que contiene $n$ características en una matriz que contiene $\\frac{(n + d)!}{d! n!}$ características, donde $n!$ es el factorial de $n$, igual a $1 \\times 2 \\times 3 \\times \\cdots \\times n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¡Cuidado con la explosión combinatoria del número de funciones!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si realiza una regresión polinomial de alto grado, es probable que ajuste los datos de entrenamiento mucho mejor que con una regresión lineal simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por ejemplo, la figura 4-14 aplica un modelo polinomial de 300 grados a los datos de entrenamiento anteriores y compara el resultado con un modelo lineal puro y un modelo cuadrático (polinomio de segundo grado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Observe cómo el modelo polinomial de 300 grados se mueve para acercarse lo más posible a las instancias de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "figura 4.14\n",
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/fig_4_14.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este modelo de regresión polinomial de alto grado sobreajusta severamente los datos de entrenamiento, mientras que el modelo lineal no los ajusta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El modelo que generalizará mejor en este caso es el modelo cuadrático, lo cual tiene sentido porque los datos se generaron usando un modelo cuadrático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pero, en general, no sabrá qué función generó los datos, entonces, ¿cómo puede decidir qué tan complejo debe ser su modelo? ¿Cómo puede saber si su modelo está sobreajustando o no ajustando los datos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En el Capítulo 2, utilizó la validación cruzada para obtener una estimación del rendimiento de generalización de un modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then your model is overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If it performs poorly on both, then it is underfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is one way to tell when a model is too simple or too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another way to tell is to look at the learning curves: these are plots of the model’s performance on the training set and the validation set as a function of the training set size (or the training iteration). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To generate the plots, train the model several times on different sized subsets of the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following code defines a function that, given some training data, plots the learning curves of a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "        plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "        plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s look at the learning curves of the plain Linear Regression model (a straight\n",
    "line; see Figure 4-15):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "figure 5.15\n",
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/fig_4_15.png?raw=true'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "plot_learning_curves(lin_reg, X, y)\n",
    "Figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This model that’s underfitting deserves a bit of explanation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Primero, veamos el rendimiento de los datos de entrenamiento: cuando solo hay una o dos instancias en el conjunto de entrenamiento, el modelo puede ajustarlas perfectamente, razón por la cual la curva comienza en cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pero a medida que se agregan nuevas instancias al conjunto de entrenamiento, se vuelve imposible que el modelo se ajuste perfectamente a los datos de entrenamiento, tanto porque los datos son ruidosos como porque no son lineales en absoluto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por lo tanto, el error en los datos de entrenamiento aumenta hasta que llega a una meseta, momento en el que agregar nuevas instancias al conjunto de entrenamiento no hace que el error promedio sea mucho mejor o peor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let’s look at the performance of the model on the validation data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cuando el modelo se entrena en muy pocas instancias de entrenamiento, es incapaz de generalizar correctamente, por lo que el error de validación es inicialmente bastante grande."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then, as the model is shown more training examples, it learns, and thus the validation error slowly goes down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, once again a straight line cannot do a good job modeling the data, so the error ends up at a plateau, very close to the other curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These learning curves are typical of a model that’s underfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Both curves have reached a plateau; they are close and fairly high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sugerencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si su modelo no se ajusta bien a los datos de entrenamiento, agregar más ejemplos de entrenamiento no ayudará."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You need to use a more complex model or come up with better features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let’s look at the learning curves of a 10th-degree polynomial model on the same data (Figure 4-16):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/fig_4_16.png?raw=true'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "polynomial_regression = Pipeline([\n",
    "(\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "(\"lin_reg\", LinearRegression()),\n",
    "])\n",
    "plot_learning_curves(polynomial_regression, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estas curvas de aprendizaje se parecen un poco a las anteriores, pero hay dos diferencias muy importantes:\n",
    "El error en los datos de entrenamiento es mucho menor que con el modelo de regresión lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hay un espacio entre las curvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto significa que el modelo funciona significativamente mejor con los datos de entrenamiento que con los datos de validación, que es el sello distintivo de un modelo de sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you used a much larger training set, however, the two curves would continue to get closer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sugerencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Una forma de mejorar un modelo sobreajustado es alimentarlo con más datos de entrenamiento hasta que el error de validación alcance el error de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LA COMPENSACIÓN DE SESGO/VARIANZA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Un resultado teórico importante de la estadística y el aprendizaje automático es el hecho de que el error de generalización de un modelo se puede expresar como la suma de tres errores muy diferentes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prejuicio o parcialidad (bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta parte del error de generalización se debe a suposiciones incorrectas, como suponer que los datos son lineales cuando en realidad son cuadráticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es más probable que un modelo de alto sesgo se ajuste por debajo de los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta parte se debe a la excesiva sensibilidad del modelo a pequeñas variaciones en los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A model with many degrees of freedom (such as a high-degree polynomial model) is likely to have high variance and thus\n",
    "overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Irreducible error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This part is due to the noisiness of the data itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The only way to reduce this part of the error is to clean up the data (e.g., fix the data sources, such as broken sensors, or detect and remove outliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Increasing a model’s complexity will typically increase its variance and reduce its bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Conversely, reducing a model’s complexity increases its bias and reduces its variance. This is why it is called a trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A simple way to regularize a polynomial model is to reduce the number of polynomial degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para un modelo lineal, la regularización generalmente se logra restringiendo los pesos del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora veremos Ridge Regression, Lasso Regression y Elastic Net, que implementan tres formas diferentes de restringir los pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La regresión de cresta (también llamada regularización de Tikhonov) es una versión regularizada de la regresión lineal: se agrega un término de regularización igual a $ \\alpha \\sum_{i = 1}^{n} \\theta_{i}^{2} $ a la función de costo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto obliga al algoritmo de aprendizaje no solo a ajustar los datos, sino también a mantener los pesos del modelo lo más pequeños posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that the regularization term should only be added to the cost function during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once the model is\n",
    "trained, you want to use the unregularized performance measure to evaluate the\n",
    "model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es bastante común que la función de costo utilizada durante el entrenamiento sea diferente de la medida de desempeño utilizada para la prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Además de la regularización, otra razón por la que pueden ser diferentes es que una buena función de costo de capacitación debe tener derivados fáciles de optimizar, mientras que la medida de rendimiento utilizada para la prueba debe estar lo más cerca posible del objetivo final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por ejemplo, los clasificadores a menudo se entrenan usando una función de costo como la pérdida de registros (discutida en un momento) pero se evalúan usando precisión/recuperación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El hiperparámetro $\\alpha$ controla cuánto desea regularizar el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If $\\alpha = 0$, then Ridge Regression is just Linear Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If $\\alpha$ is very large, then all weights end up very close to zero and the result is a flat line going through the data’s mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 4-8 presents the Ridge Regression cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 4-8. Ridge Regression cost function  \n",
    "\n",
    "$$ J(\\theta) = MSE(\\theta) + α 12 \\sigma n i=1 θi 2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that the bias term θ is not regularized (the sum starts at i = 1, not 0). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we define w as the vector of feature weights ($θ$ to $θ$ ), then the regularization term is equal to $½(∥ w ∥ $) , where $∥ w ∥$ represents the $ℓ$ norm of the weight vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For Gradient Descent, just add αw to the MSE gradient vector (Equation 4-6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ADVERTENCIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Es importante escalar los datos (p. ej., usando un `StandardScaler`) antes de realizar la regresión de cresta, ya que es sensible a la escala de las entidades de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto es cierto para la mayoría de los modelos regularizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La Figura 4-17 muestra varios modelos de Ridge entrenados en algunos datos lineales usando diferentes valores de $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A la izquierda, se utilizan modelos Ridge sencillos, lo que lleva a predicciones lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A la derecha, los datos primero se expanden usando `PolynomialFeatures(degree=10)`, luego se escalan usando un `StandardScaler`, y finalmente los modelos Ridge se aplican a las características resultantes: esto es Polynomial Regression with Ridge regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Observe cómo el aumento de $\\alpha$ conduce a predicciones más planas (es decir, menos extremas, más razonables), lo que reduce la varianza del modelo pero aumenta su sesgo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "figura 4.17\n",
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/fig_4_17.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Al igual que con la regresión lineal, podemos realizar la regresión de cresta calculando una ecuación de forma cerrada o realizando un descenso de gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los pros y los contras son los mismos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La ecuación 4-9 muestra la solución de forma cerrada, donde $A$ es la matriz identidad $(n + 1) \\times (n + 1)$, excepto con $\\theta$ en la celda superior izquierda, correspondiente a el término de sesgo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 4-9. Ridge Regression closed-form solution\n",
    "\n",
    "$$ \\theta = \\left(X^{T}X + \\alpha A \\right)^{-1} X^{T} y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is how to perform Ridge Regression with Scikit-Learn using a closed-form solution (a variant of Equation 4-9 that uses a matrix factorization technique by André-Louis Cholesky):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And using Stochastic Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sgd_reg = SGDRegressor(penalty=\"l2\")\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The penalty hyperparameter sets the type of regularization term to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Specifying \"l2\" indicates that you want SGD to add a regularization term to the cost function equal to half the square of the ℓ norm of the weight vector: this is simply Ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La reducción absoluta mínima y la regresión del operador de selección (generalmente llamada regresión de lazo) es otra versión regularizada de la regresión lineal: al igual que la regresión de cresta, agrega un término de regularización a la función de costo, pero usa la norma ℓ del vector de peso en lugar de la mitad el cuadrado de la norma ℓ (ver Ecuación 4-10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 4-10. Lasso Regression cost function  \n",
    "\n",
    "$$ J(\\theta) = MSE(\\theta) + \\alpha \\sum_{i = 1}^{n} |θ_{i}| $$\n",
    "\n",
    "Figure 4-18 shows the same thing as Figure 4-17 but replaces Ridge models with Lasso models and uses smaller $\\alpha$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "figura 4.18  \n",
    "\n",
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/fig_4_18.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An important characteristic of Lasso Regression is that it tends to eliminate the weights of the least important features (i.e., set them to zero). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, the\n",
    "dashed line in the righthand plot in Figure 4-18 (with α = 10 ) looks quadratic,\n",
    "almost linear: all the weights for the high-degree polynomial features are equal\n",
    "to zero. In other words, Lasso Regression automatically performs feature\n",
    "selection and outputs a sparse model (i.e., with few nonzero feature weights).\n",
    "You can get a sense of why this is the case by looking at Figure 4-19: the axes\n",
    "represent two model parameters, and the background contours represent different\n",
    "loss functions. In the top-left plot, the contours represent the ℓ loss (|θ | + |θ |),\n",
    "which drops linearly as you get closer to any axis. For example, if you initialize\n",
    "the model parameters to θ = 2 and θ = 0.5, running Gradient Descent will\n",
    "decrement both parameters equally (as represented by the dashed yellow line);\n",
    "therefore θ will reach 0 first (since it was closer to 0 to begin with). After that,\n",
    "Gradient Descent will roll down the gutter until it reaches θ = 0 (with a bit of\n",
    "bouncing around, since the gradients of ℓ never get close to 0: they are either –1\n",
    "or 1 for each parameter). In the top-right plot, the contours represent Lasso’s cost\n",
    "function (i.e., an MSE cost function plus an ℓ loss). The small white circles\n",
    "show the path that Gradient Descent takes to optimize some model parameters\n",
    "that were initialized around θ = 0.25 and θ = –1: notice once again how the\n",
    "path quickly reaches θ = 0, then rolls down the gutter and ends up bouncing\n",
    "around the global optimum (represented by the red square). If we increased α,\n",
    "the global optimum would move left along the dashed yellow line, while if we decreased α, the global optimum would move right (in this example, the optimal\n",
    "parameters for the unregularized MSE are θ = 2 and θ = 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/fig_4_19.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The two bottom plots show the same thing but with an ℓ penalty instead. In the\n",
    "bottom-left plot, you can see that the ℓ loss decreases with the distance to the\n",
    "origin, so Gradient Descent just takes a straight path toward that point. In the\n",
    "bottom-right plot, the contours represent Ridge Regression’s cost function (i.e.,\n",
    "an MSE cost function plus an ℓ loss). There are two main differences with\n",
    "Lasso. First, the gradients get smaller as the parameters approach the global\n",
    "optimum, so Gradient Descent naturally slows down, which helps convergence\n",
    "(as there is no bouncing around). Second, the optimal parameters (represented by\n",
    "the red square) get closer and closer to the origin when you increase α, but they\n",
    "never get eliminated entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Referentes   \n",
    "\n",
    "* Cuadernos Jupyter de Geron en GitHub de la tercera edición de manos a la obra al machine learning con sklearn: https://github.com/ageron/handson-ml3\n",
    "\n",
    "* Compute least-squares solution to equation Ax = b: https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "es"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "es",
   "useGoogleTranslate": true
  },
  "rise": {
   "scroll": true,
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
