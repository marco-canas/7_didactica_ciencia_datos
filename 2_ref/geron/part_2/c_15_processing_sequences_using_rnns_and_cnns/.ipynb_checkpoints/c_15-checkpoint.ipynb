{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6bcf36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_15/c_15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca9c2c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 15. Secuencias de procesamiento usando RNN y CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4850437e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El bateador golpea la pelota."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17a88db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El jardinero inmediatamente comienza a correr, anticipando la trayectoria de la pelota."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ddbcb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lo rastrea, adapta sus movimientos y finalmente lo atrapa (bajo un estruendo de aplausos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe50fd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Predecir el futuro es algo que haces todo el tiempo, ya sea que estés terminando la oración de un amigo o anticipando el olor del café en el desayuno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b02be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En este capítulo analizaremos las redes neuronales recurrentes (RNN), una clase de redes que pueden predecir el futuro (bueno, hasta cierto punto, por supuesto)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8eb07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pueden analizar datos de series temporales, como los precios de las acciones, y decirle cuándo comprar o vender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9438ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En los sistemas de conducción autónomos, pueden anticipar las trayectorias de los automóviles y ayudar a evitar accidentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e167294",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En términos más generales, pueden funcionar en secuencias de longitudes arbitrarias, en lugar de entradas de tamaño fijo como todas las redes que hemos considerado hasta ahora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f837b67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por ejemplo, pueden tomar oraciones, documentos o muestras de audio como entrada, lo que los hace extremadamente útiles para aplicaciones de procesamiento de lenguaje natural, como la traducción automática o la conversión de voz a texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be0aa20",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En este capítulo, primero veremos los conceptos fundamentales que subyacen a las RNN y cómo entrenarlas usando retropropagación a través del tiempo, luego las usaremos para pronosticar una serie de tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fafd83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Después de eso, exploraremos las dos principales dificultades que enfrentan las RNN:\n",
    "\n",
    "* Gradientes inestables (discutidos en el Capítulo 11), que se pueden aliviar utilizando varias técnicas, incluida la eliminación recurrente y la normalización de capas recurrentes\n",
    "* Una memoria a corto plazo (muy) limitada, que se puede ampliar utilizando células LSTM y GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9148a8b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "RNNs are not the only types of neural networks capable of handling sequential data: for small sequences, a regular dense network can do the trick; and for very long sequences, such as audio samples or text, convolutional neural networks can actually work quite well too. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e6eda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will discuss both of these possibilities, and we will finish this chapter by implementing a WaveNet: this is a CNN architecture capable of handling sequences of tens of thousands of time steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34967dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In Chapter 16, we will continue to explore RNNs and see how to use them for natural language processing, along with more recent architectures based on attention mechanisms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c7ef6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9592da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neurons and Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df9d1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Up to now we have focused on feedforward neural networks, where the\n",
    "activations flow only in one direction, from the input layer to the output\n",
    "layer (a few exceptions are discussed in Appendix E). A recurrent neural\n",
    "network looks very much like a feedforward neural network, except it also\n",
    "has connections pointing backward. Let’s look at the simplest possible\n",
    "RNN, composed of one neuron receiving inputs, producing an output, and\n",
    "sending that output back to itself, as shown in Figure 15-1 (left). At each\n",
    "time step t (also called a frame), this recurrent neuron receives the inputs\n",
    "x as well as its own output from the previous time step, y . Since there\n",
    "is no previous output at the first time step, it is generally set to 0. We can\n",
    "represent this tiny network against the time axis, as shown in Figure 15-1\n",
    "(right). This is called unrolling the network through time (it’s the same\n",
    "recurrent neuron represented once per time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b068d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e002232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cce46c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdaeff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "rise": {
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
