{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn.pipeline.Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sklearn.pipeline.Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Pipelines and composite estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6.1. Pipelines and composite estimators](https://scikit-learn.org/stable/modules/compose.html#pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Los transformadores generalmente se combinan con clasificadores, regresores u otros estimadores para construir un estimador compuesto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "La herramienta más común es una tubería. Pipeline se usa a menudo en combinación con FeatureUnion, que concatena la salida de los transformadores en un espacio de características compuesto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "TransformedTargetRegressor se ocupa de transformar el objetivo (es decir, log-transform y). Por el contrario, las canalizaciones solo transforman los datos observados (X)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "### 6.1.1. Pipeline: encadenamiento de estimadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Pipeline se puede utilizar para encadenar varios estimadores en uno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Esto es útil ya que a menudo hay una secuencia fija de pasos en el procesamiento de los datos, por ejemplo, selección, normalización y clasificación de características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Pipeline tiene varios propósitos aquí:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "**Conveniencia y encapsulación**  \n",
    "Solo tiene que llamar a ajustar y predecir una vez en sus datos para ajustar una secuencia completa de estimadores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "**Selección de parámetros conjuntos**  \n",
    "Puede realizar búsquedas en cuadrículas sobre los parámetros de todos los estimadores en la tubería a la vez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "**Seguridad**  \n",
    "Las canalizaciones ayudan a evitar la filtración de estadísticas de sus datos de prueba al modelo entrenado en la validación cruzada, al garantizar que se utilicen las mismas muestras para entrenar los transformadores y predictores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Todos los estimadores en una tubería, excepto el último, deben ser transformadores (es decir, deben tener un método de transformación). El último estimador puede ser de cualquier tipo (transformador, clasificador, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "#### 6.1.1.1. Uso\n",
    "##### 6.1.1.1.1. Construcción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "El Pipeline se construye usando una lista de pares (clave, valor), donde la clave es una cadena que contiene el nombre que desea dar a este paso y el valor es un objeto estimador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n",
    "pipe = Pipeline(estimators)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "La función de utilidad make_pipeline es una forma abreviada de construir tuberías; toma un número variable de estimadores y devuelve una canalización, completando los nombres automáticamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('binarizer', Binarizer()), ('multinomialnb', MultinomialNB())])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.pipeline import make_pipeline\n",
    ">>> from sklearn.naive_bayes import MultinomialNB\n",
    ">>> from sklearn.preprocessing import Binarizer\n",
    ">>> make_pipeline(Binarizer(), MultinomialNB()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.1.1.2. Accessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Los estimadores de una canalización se almacenan como una lista en el atributo pasos, pero se puede acceder a ellos por índice o nombre indexando (con [idx]) la canalización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('reduce_dim', PCA())"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> pipe.steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> pipe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> pipe['reduce_dim']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "El atributo ``named_steps`` de Pipeline permite acceder a los pasos por nombre con la terminación de tabulación en entornos interactivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " pipe.named_steps.reduce_dim is pipe['reduce_dim']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "También se puede extraer una sub-canalización utilizando la notación de corte que se usa comúnmente para las secuencias de Python, como listas o cadenas (aunque solo se permite un paso de 1). Esto es conveniente para realizar solo algunas de las transformaciones (o su inversa):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('reduce_dim', PCA())])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('clf', SVC())])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "##### 6.1.1.1.3. Parámetros anidados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Se puede acceder a los parámetros de los estimadores en la tubería utilizando la sintaxis ``<estimator> __ <parameter>``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC(C=10))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.set_params(clf__C=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Esto es particularmente importante para realizar búsquedas en cuadrículas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = dict(reduce_dim__n_components=[2, 5, 10],\n",
    "                      clf__C=[0.1, 10, 100])\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Los pasos individuales también se pueden reemplazar como parámetros, y los pasos no finales se pueden ignorar configurándolos como 'passthrough':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "param_grid = dict(reduce_dim=['passthrough', PCA(5), PCA(10)],\n",
    "                      clf=[SVC(), LogisticRegression()],\n",
    "                      clf__C=[0.1, 10, 100])\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Los estimadores de la canalización se pueden recuperar por índice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "o por nombre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe['reduce_dim'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pipeline Anova SVM\n",
    "\n",
    "* Sample pipeline for text feature extraction and evaluation\n",
    "\n",
    "* Pipelining: chaining a PCA and a logistic regression\n",
    "\n",
    "* Explicit feature map approximation for RBF kernels\n",
    "\n",
    "* SVM-Anova: SVM with univariate feature selection\n",
    "\n",
    "* Selecting dimensionality reduction with Pipeline and GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See Also:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Composite estimators and parameter spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.1.2. Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling fit on the pipeline is the same as calling fit on each estimator in turn, transform the input and pass it on to the next step. The pipeline has all the methods that the last estimator in the pipeline has, i.e. if the last estimator is a classifier, the Pipeline can be used as a classifier. If the last estimator is a transformer, again, so is the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.1.3. Caching transformers: avoid repeated computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting transformers may be computationally expensive. With its memory parameter set, Pipeline will cache each transformer after calling fit. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical. A typical example is the case of a grid search in which the transformers can be fitted only once and reused for each configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter memory is needed in order to cache the transformers. memory can be either a string containing the directory where to cache the transformers or a joblib.Memory object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='C:\\\\Users\\\\Usuario\\\\AppData\\\\Local\\\\Temp\\\\tmpk0uc5w46',\n",
       "         steps=[('reduce_dim', PCA()), ('clf', SVC())])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n",
    "cachedir = mkdtemp()\n",
    "pipe = Pipeline(estimators, memory=cachedir)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Borre el directorio de caché cuando ya no lo necesite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning Side effect of caching transformers\n",
    "Using a Pipeline without cache enabled, it is possible to inspect the original instance such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.datasets import load_digits\n",
    ">>> X_digits, y_digits = load_digits(return_X_y=True)\n",
    ">>> pca1 = PCA()\n",
    ">>> svm1 = SVC()\n",
    ">>> pipe = Pipeline([('reduce_dim', pca1), ('clf', svm1)])\n",
    ">>> pipe.fit(X_digits, y_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.77484909e-19 -1.73094651e-02 -2.23428835e-01 ... -8.94184677e-02\n",
      "  -3.65977111e-02 -1.14684954e-02]\n",
      " [ 3.27805401e-18 -1.01064569e-02 -4.90849204e-02 ...  1.76697117e-01\n",
      "   1.94547053e-02 -6.69693895e-03]\n",
      " [-1.68358559e-18  1.83420720e-02  1.26475543e-01 ...  2.32084163e-01\n",
      "   1.67026563e-01  3.48043832e-02]\n",
      " ...\n",
      " [ 0.00000000e+00 -8.73056983e-16 -8.00882817e-17 ...  4.50992264e-17\n",
      "  -6.85099394e-17  1.37105203e-16]\n",
      " [ 0.00000000e+00 -1.43163189e-16  1.69094260e-16 ...  3.09312540e-17\n",
      "  -5.28224496e-17  4.51534285e-17]\n",
      " [ 1.00000000e+00 -1.68983002e-17  5.73338351e-18 ...  8.66631300e-18\n",
      "  -1.57615962e-17  4.07058917e-18]]\n"
     ]
    }
   ],
   "source": [
    ">>> # The pca instance can be inspected directly\n",
    ">>> print(pca1.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly. In following example, accessing the PCA instance pca2 will raise an AttributeError since pca2 will be an unfitted transformer. Instead, use the attribute named_steps to inspect estimators within the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='C:\\\\Users\\\\Usuario\\\\AppData\\\\Local\\\\Temp\\\\tmpfk8tnkin',\n",
       "         steps=[('reduce_dim', PCA()), ('clf', SVC())])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> cachedir = mkdtemp()\n",
    ">>> pca2 = PCA()\n",
    ">>> svm2 = SVC()\n",
    ">>> cached_pipe = Pipeline([('reduce_dim', pca2), ('clf', svm2)],\n",
    "                           memory=cachedir)\n",
    ">>> cached_pipe.fit(X_digits, y_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.77484909e-19 -1.73094651e-02 -2.23428835e-01 ... -8.94184677e-02\n",
      "  -3.65977111e-02 -1.14684954e-02]\n",
      " [ 3.27805401e-18 -1.01064569e-02 -4.90849204e-02 ...  1.76697117e-01\n",
      "   1.94547053e-02 -6.69693895e-03]\n",
      " [-1.68358559e-18  1.83420720e-02  1.26475543e-01 ...  2.32084163e-01\n",
      "   1.67026563e-01  3.48043832e-02]\n",
      " ...\n",
      " [ 0.00000000e+00 -8.73056983e-16 -8.00882817e-17 ...  4.50992264e-17\n",
      "  -6.85099394e-17  1.37105203e-16]\n",
      " [ 0.00000000e+00 -1.43163189e-16  1.69094260e-16 ...  3.09312540e-17\n",
      "  -5.28224496e-17  4.51534285e-17]\n",
      " [ 1.00000000e+00 -1.68983002e-17  5.73338351e-18 ...  8.66631300e-18\n",
      "  -1.57615962e-17  4.07058917e-18]]\n"
     ]
    }
   ],
   "source": [
    ">>> print(cached_pipe.named_steps['reduce_dim'].components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] El sistema no puede encontrar la ruta especificada: 'C:\\\\Users\\\\Usuario\\\\AppData\\\\Local\\\\Temp\\\\tmpfk8tnkin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-9c4cda5c7992>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Remove the cache directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcachedir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    514\u001b[0m             \u001b[1;31m# can't continue even if onerror hook returns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_rmtree_unsafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;31m# Allow introspection of whether or not the hardening against symlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscandir_it\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m         \u001b[0monerror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m         \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_rmtree_unsafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mscandir_it\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m             \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscandir_it\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] El sistema no puede encontrar la ruta especificada: 'C:\\\\Users\\\\Usuario\\\\AppData\\\\Local\\\\Temp\\\\tmpfk8tnkin'"
     ]
    }
   ],
   "source": [
    ">>> # Remove the cache directory\n",
    ">>> rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting dimensionality reduction with Pipeline and GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2. Transforming target in regression\n",
    "TransformedTargetRegressor transforms the targets y before fitting a regression model. The predictions are mapped back to the original space via an inverse transform. It takes as an argument the regressor that will be used for prediction, and the transformer that will be applied to the target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformedTargetRegressor(regressor=LinearRegression(),\n",
       "                           transformer=QuantileTransformer(output_distribution='normal'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X, y = X[:2000, :], y[:2000]  # select a subset of data\n",
    "transformer = QuantileTransformer(output_distribution='normal')\n",
    "regressor = LinearRegression()\n",
    "regr = TransformedTargetRegressor(regressor=regressor,\n",
    "                                      transformer=transformer)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> raw_target_regr = LinearRegression().fit(X_train, y_train)\n",
    ">>> print('R2 score: {0:.2f}'.format(raw_target_regr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple transformations, instead of a Transformer object, a pair of functions can be passed, defining the transformation and its inverse mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>\n",
    ">>> def func(x):\n",
    "...     return np.log(x)\n",
    ">>> def inverse_func(x):\n",
    "...     return np.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, the object is created as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>\n",
    ">>> regr = TransformedTargetRegressor(regressor=regressor,\n",
    "...                                   func=func,\n",
    "...                                   inverse_func=inverse_func)\n",
    ">>> regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the provided functions are checked at each fit to be the inverse of each other. However, it is possible to bypass this checking by setting check_inverse to False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>\n",
    ">>> def inverse_func(x):\n",
    "...     return x\n",
    ">>> regr = TransformedTargetRegressor(regressor=regressor,\n",
    "...                                   func=func,\n",
    "...                                   inverse_func=inverse_func,\n",
    "...                                   check_inverse=False)\n",
    ">>> regr.fit(X_train, y_train)\n",
    "TransformedTargetRegressor(...)\n",
    ">>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 score: -1.57\n",
    "Note The transformation can be triggered by setting either transformer or the pair of functions func and inverse_func. However, setting both options will raise an error.\n",
    "Examples:\n",
    "\n",
    "Effect of transforming the targets in regression model\n",
    "\n",
    "6.1.3. FeatureUnion: composite feature spaces\n",
    "FeatureUnion combines several transformer objects into a new transformer that combines their output. A FeatureUnion takes a list of transformer objects. During fitting, each of these is fit to the data independently. The transformers are applied in parallel, and the feature matrices they output are concatenated side-by-side into a larger matrix.\n",
    "\n",
    "When you want to apply different transformations to each field of the data, see the related class ColumnTransformer (see user guide).\n",
    "\n",
    "FeatureUnion serves the same purposes as Pipeline - convenience and joint parameter estimation and validation.\n",
    "\n",
    "FeatureUnion and Pipeline can be combined to create complex models.\n",
    "\n",
    "(A FeatureUnion has no way of checking whether two transformers might produce identical features. It only produces a union when the feature sets are disjoint, and making sure they are is the caller’s responsibility.)\n",
    "\n",
    "6.1.3.1. Usage\n",
    "A FeatureUnion is built using a list of (key, value) pairs, where the key is the name you want to give to a given transformation (an arbitrary string; it only serves as an identifier) and value is an estimator object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>\n",
    ">>> from sklearn.pipeline import FeatureUnion\n",
    ">>> from sklearn.decomposition import PCA\n",
    ">>> from sklearn.decomposition import KernelPCA\n",
    ">>> estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]\n",
    ">>> combined = FeatureUnion(estimators)\n",
    ">>> combined\n",
    "FeatureUnion(transformer_list=[('linear_pca', PCA()),\n",
    "                               ('kernel_pca', KernelPCA())])\n",
    "Like pipelines, feature unions have a shorthand constructor called make_union that does not require explicit naming of the components.\n",
    "\n",
    "Like Pipeline, individual steps may be replaced using set_params, and ignored by setting to 'drop':\n",
    "\n",
    ">>>\n",
    ">>> combined.set_params(kernel_pca='drop')\n",
    "FeatureUnion(transformer_list=[('linear_pca', PCA()),\n",
    "                               ('kernel_pca', 'drop')])\n",
    "Examples:\n",
    "\n",
    "Concatenating multiple feature extraction methods\n",
    "\n",
    "6.1.4. ColumnTransformer for heterogeneous data\n",
    "Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps. Often it is easiest to preprocess data before applying scikit-learn methods, for example using pandas. Processing your data before passing it to scikit-learn might be problematic for one of the following reasons:\n",
    "\n",
    "Incorporating statistics from test data into the preprocessors makes cross-validation scores unreliable (known as data leakage), for example in the case of scalers or imputing missing values.\n",
    "\n",
    "You may want to include the parameters of the preprocessors in a parameter search.\n",
    "\n",
    "The ColumnTransformer helps performing different transformations for different columns of the data, within a Pipeline that is safe from data leakage and that can be parametrized. ColumnTransformer works on arrays, sparse matrices, and pandas DataFrames.\n",
    "\n",
    "To each column, a different transformation can be applied, such as preprocessing or a specific feature extraction method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>\n",
    ">>> import pandas as pd\n",
    ">>> X = pd.DataFrame(\n",
    "...     {'city': ['London', 'London', 'Paris', 'Sallisaw'],\n",
    "...      'title': [\"His Last Bow\", \"How Watson Learned the Trick\",\n",
    "...                \"A Moveable Feast\", \"The Grapes of Wrath\"],\n",
    "...      'expert_rating': [5, 3, 4, 5],\n",
    "...      'user_rating': [4, 5, 4, 3]})\n",
    "For this data, we might want to encode the 'city' column as a categorical variable using OneHotEncoder but apply a CountVectorizer to the 'title' column. As we might use multiple feature extraction methods on the same column, we give each transformer a unique name, say 'city_category' and 'title_bow'. By default, the remaining rating columns are ignored (remainder='drop'):\n",
    "\n",
    ">>>\n",
    ">>> from sklearn.compose import ColumnTransformer\n",
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> from sklearn.preprocessing import OneHotEncoder\n",
    ">>> column_trans = ColumnTransformer(\n",
    "...     [('city_category', OneHotEncoder(dtype='int'),['city']),\n",
    "...      ('title_bow', CountVectorizer(), 'title')],\n",
    "...     remainder='drop')\n",
    "\n",
    ">>> column_trans.fit(X)\n",
    "ColumnTransformer(transformers=[('city_category', OneHotEncoder(dtype='int'),\n",
    "                                 ['city']),\n",
    "                                ('title_bow', CountVectorizer(), 'title')])\n",
    "\n",
    ">>> column_trans.get_feature_names()\n",
    "['city_category__x0_London', 'city_category__x0_Paris', 'city_category__x0_Sallisaw',\n",
    "'title_bow__bow', 'title_bow__feast', 'title_bow__grapes', 'title_bow__his',\n",
    "'title_bow__how', 'title_bow__last', 'title_bow__learned', 'title_bow__moveable',\n",
    "'title_bow__of', 'title_bow__the', 'title_bow__trick', 'title_bow__watson',\n",
    "'title_bow__wrath']\n",
    "\n",
    ">>> column_trans.transform(X).toarray()\n",
    "array([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0],\n",
    "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]]...)\n",
    "In the above example, the CountVectorizer expects a 1D array as input and therefore the columns were specified as a string ('title'). However, OneHotEncoder as most of other transformers expects 2D data, therefore in that case you need to specify the column as a list of strings (['city']).\n",
    "\n",
    "Apart from a scalar or a single item list, the column selection can be specified as a list of multiple items, an integer array, a slice, a boolean mask, or with a make_column_selector. The make_column_selector is used to select columns based on data type or column name:\n",
    "\n",
    ">>>\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> from sklearn.compose import make_column_selector\n",
    ">>> ct = ColumnTransformer([\n",
    "...       ('scale', StandardScaler(),\n",
    "...       make_column_selector(dtype_include=np.number)),\n",
    "...       ('onehot',\n",
    "...       OneHotEncoder(),\n",
    "...       make_column_selector(pattern='city', dtype_include=object))])\n",
    ">>> ct.fit_transform(X)\n",
    "array([[ 0.904...,  0.      ,  1. ,  0. ,  0. ],\n",
    "       [-1.507...,  1.414...,  1. ,  0. ,  0. ],\n",
    "       [-0.301...,  0.      ,  0. ,  1. ,  0. ],\n",
    "       [ 0.904..., -1.414...,  0. ,  0. ,  1. ]])\n",
    "Strings can reference columns if the input is a DataFrame, integers are always interpreted as the positional columns.\n",
    "\n",
    "We can keep the remaining rating columns by setting remainder='passthrough'. The values are appended to the end of the transformation:\n",
    "\n",
    ">>>\n",
    ">>> column_trans = ColumnTransformer(\n",
    "...     [('city_category', OneHotEncoder(dtype='int'),['city']),\n",
    "...      ('title_bow', CountVectorizer(), 'title')],\n",
    "...     remainder='passthrough')\n",
    "\n",
    ">>> column_trans.fit_transform(X)\n",
    "array([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 4],\n",
    "       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3, 5],\n",
    "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 4],\n",
    "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 5, 3]]...)\n",
    "The remainder parameter can be set to an estimator to transform the remaining rating columns. The transformed values are appended to the end of the transformation:\n",
    "\n",
    ">>>\n",
    ">>> from sklearn.preprocessing import MinMaxScaler\n",
    ">>> column_trans = ColumnTransformer(\n",
    "...     [('city_category', OneHotEncoder(), ['city']),\n",
    "...      ('title_bow', CountVectorizer(), 'title')],\n",
    "...     remainder=MinMaxScaler())\n",
    "\n",
    ">>> column_trans.fit_transform(X)[:, -2:]\n",
    "array([[1. , 0.5],\n",
    "       [0. , 1. ],\n",
    "       [0.5, 0.5],\n",
    "       [1. , 0. ]])\n",
    "The make_column_transformer function is available to more easily create a ColumnTransformer object. Specifically, the names will be given automatically. The equivalent for the above example would be:\n",
    "\n",
    ">>>\n",
    ">>> from sklearn.compose import make_column_transformer\n",
    ">>> column_trans = make_column_transformer(\n",
    "...     (OneHotEncoder(), ['city']),\n",
    "...     (CountVectorizer(), 'title'),\n",
    "...     remainder=MinMaxScaler())\n",
    ">>> column_trans\n",
    "ColumnTransformer(remainder=MinMaxScaler(),\n",
    "                  transformers=[('onehotencoder', OneHotEncoder(), ['city']),\n",
    "                                ('countvectorizer', CountVectorizer(),\n",
    "                                 'title')])\n",
    "6.1.5. Visualizing Composite Estimators\n",
    "Estimators can be displayed with a HTML representation when shown in a jupyter notebook. This can be useful to diagnose or visualize a Pipeline with many estimators. This visualization is activated by setting the display option in set_config:\n",
    "\n",
    ">>>\n",
    ">>> from sklearn import set_config\n",
    ">>> set_config(display='diagram')   \n",
    ">>> # diplays HTML representation in a jupyter context\n",
    ">>> column_trans  \n",
    "An example of the HTML output can be seen in the HTML representation of Pipeline section of Column Transformer with Mixed Types. As an alternative, the HTML can be written to a file using estimator_html_repr:\n",
    "\n",
    ">>>\n",
    ">>> from sklearn.utils import estimator_html_repr\n",
    ">>> with open('my_estimator.html', 'w') as f:  \n",
    "...     f.write(estimator_html_repr(clf))\n",
    "Examples:\n",
    "\n",
    "Column Transformer with Heterogeneous Data Sources\n",
    "\n",
    "Column Transformer with Mixed Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "es",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
