{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn.preprocessing.StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class ``sklearn.preprocessing.StandardScaler(*, copy=True, with_mean=True, with_std=True)``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Estandarizar características eliminando la media y escalando a la varianza de la unidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "La puntuación estándar de una muestra de $x$ se calcula como:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ z = \\frac{x-\\mu}{s} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "donde $\\mu$ es la media de las muestras de entrenamiento o cero si ``with_mean=False``, y $s$ es la desviación estándar de las muestras de entrenamiento o uno si ``with_std=False``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "El centrado y el escalado ocurren de forma independiente en cada característica al calcular las estadísticas relevantes en las muestras del conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "A continuación, se almacenan la media y la desviación estándar para utilizarlas en datos posteriores mediante la transformación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "La estandarización de un conjunto de datos es un requisito común para muchos estimadores de aprendizaje automático: pueden comportarse mal si las características individuales no se parecen más o menos a datos estándar distribuidos normalmente (por ejemplo, gaussiano con media $0$ y varianza unitaria)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Por ejemplo, muchos elementos utilizados en la función objetivo de un algoritmo de aprendizaje (como el núcleo RBF de Support Vector Machines o los regularizadores L1 y L2 de modelos lineales) asumen que todas las características están centradas alrededor de 0 y tienen varianza en el mismo orden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Si una característica tiene una varianza que es órdenes de magnitud mayor que otras, podría dominar la función objetivo y hacer que el estimador no pueda aprender de otras características correctamente como se esperaba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Este escalador también se puede aplicar a matrices CSR o CSC dispersas pasando ``with_mean=False`` para evitar romper la estructura de dispersión de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more in the User Guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters\n",
    "copybool, default=True\n",
    "If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with_meanbool, default=True\n",
    "If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with_stdbool, default=True\n",
    "If True, scale the data to unit variance (or equivalently, unit standard deviation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes\n",
    "scale_ndarray of shape (n_features,) or None\n",
    "Per feature relative scaling of the data to achieve zero mean and unit variance. Generally this is calculated using np.sqrt(var_). If a variance is zero, we can’t achieve unit variance, and the data is left as-is, giving a scaling factor of 1. scale_ is equal to None when with_std=False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New in version 0.17: scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean_ndarray of shape (n_features,) or None\n",
    "The mean value for each feature in the training set. Equal to None when with_mean=False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "var_ndarray of shape (n_features,) or None\n",
    "The variance for each feature in the training set. Used to compute scale_. Equal to None when with_std=False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_samples_seen_int or ndarray of shape (n_features,)\n",
    "The number of samples processed by the estimator for each feature. If there are no missing samples, the n_samples_seen will be an integer, otherwise it will be an array of dtype int. If sample_weights are used it will be a float (if no missing data) or an array of dtype float that sums the weights seen so far. Will be reset on new calls to fit, but increments across partial_fit calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scale\n",
    "Equivalent function without the estimator API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA\n",
    "Further removes the linear correlation across features with ‘whiten=True’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "NaNs are treated as missing values: disregarded in fit, and maintained in transform.\n",
    "\n",
    "We use a biased estimator for the standard deviation, equivalent to numpy.std(x, ddof=0). Note that the choice of ddof is unlikely to affect model performance.\n",
    "\n",
    "For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "data = [[0, 0], [0, 0], [1, 1], [1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0], [0, 0], [1, 1], [1, 1]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler()\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "print(scaler.fit(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "print(scaler.mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1.]\n",
      " [-1. -1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(scaler.transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(scaler.transform([[2, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0], [0, 0], [1, 1], [1, 1]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "es",
   "useGoogleTranslate": true
  },
  "rise": {
   "enable_chalkboard": true,
   "theta": "sky",
   "transition": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
