{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60c0a4a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_sv/chap_5_svm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f50600",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 5. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01790174",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Guido Van Rossum Creador de Python\n",
    "<img src = 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Guido_van_Rossum_OSCON_2006.jpg/300px-Guido_van_Rossum_OSCON_2006.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541097ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una máquina de soporte vectorial (SVM) es un modelo de aprendizaje automático potente y versátil, capaz de realizar \n",
    "* clasificación lineal o no lineal, \n",
    "* regresión e \n",
    "* incluso detección de valores atípicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4a4c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Es uno de los modelos más populares en Machine Learning, y \n",
    "* cualquier persona interesada en Machine Learning debería tenerlo en su caja de herramientas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf76a4bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las SVM son particularmente adecuadas para la clasificación de conjuntos de datos complejos de tamaño pequeño o mediano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64899e27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este capítulo explicará \n",
    "* los conceptos básicos de las SVM, \n",
    "* cómo usarlas y \n",
    "* cómo funcionan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4885bb1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792f3f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La idea fundamental detrás de las SVM se explica mejor con algunas imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f0c382",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La Figura 5-1 muestra parte del conjunto de datos del iris que se presentó al final del Capítulo 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c15bbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_1.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b33fe9e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las dos clases se pueden separar fácilmente con una línea recta (son linealmente separables). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1babb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El gráfico de la izquierda muestra los límites de decisión de tres posibles clasificadores lineales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d708526f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e898ea7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creación de función para obtener atributos y etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008aafb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_flowers_feature():\n",
    "    '''\n",
    "    iris.data contiene un total de 150 datos, incluidos tres tipos de iris\n",
    "    Aquí usamos un total de 100 datos, setosa y versicolor\n",
    "    Cada conjunto de datos de iris contiene cuatro características\n",
    "    Para facilitar el dibujo, seleccione solo dos de las características \n",
    "    (la longitud de los pétalos y la longitud de los sépalos)\n",
    "    '''\n",
    "    from sklearn.datasets import load_iris\n",
    "    X,y = load_iris(return_X_y = True)\n",
    "    setosa_o_versicolor = (y==0) | (y==1)\n",
    "    X = X[setosa_o_versicolor]\n",
    "    X = X[:,(2,3)]\n",
    "    y = y[setosa_o_versicolor]\n",
    "    return X,y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f26dc8d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X,y = get_flowers_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf3422",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, \\\n",
    "                                            random_state = 42, stratify = y)\n",
    "escalador = StandardScaler()\n",
    "X_train_escalado = escalador.fit_transform(X_train)\n",
    "svm_clf = SVC(kernel = 'linear') \n",
    "svm_clf.fit(X_train_escalado, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e31ec3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Descripción formal del clasificador de Máquina de soporte vectorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299bd6b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ h_{w}(x_{i}) = w^{T}x_{i} + b = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f46bb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ w_{0}x + w_{1}y + b = 0  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f12e4be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ y = - \\frac{ w_{0} }{ w_{1} }x - \\frac{b}{ w_{1} }  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4376e96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advertencia: \n",
    "\n",
    "El clasificador se entrenó con el **conjunto de entrenamiento escalado**, por lo tanto, la visualización debe ser para las **instancias de entrenamiento escaladas**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6252fa",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# trazado de la clase setosa\n",
    "ax.plot(X_train_escalado[:,0][y_train==0], X_train_escalado[:,1][y_train==0],\\\n",
    "        'yo', label = 'Setosa')\n",
    "ax.plot(X_train_escalado[:,0][y_train==1], X_train_escalado[:,1][y_train==1], 'bs',\\\n",
    "        label = 'Versicolor')\n",
    "ax.legend()\n",
    "\n",
    "# definición del rectángulo de visualización\n",
    "c,d = X_train_escalado[:,0].min()-0.3, X_train_escalado[:,0].max()+0.3\n",
    "e,f = X_train_escalado[:,1].min()-0.3, X_train_escalado[:,1].max()+0.3\n",
    "\n",
    "ax.axis([c,d,e,f])\n",
    "\n",
    "ax.grid(alpha = 0.4)\n",
    "\n",
    "b,w = svm_clf.intercept_[0], svm_clf.coef_[0]\n",
    "\n",
    "x0 = np.linspace(c,d, 2)\n",
    "decision_boundary = -w[0]/w[1]*x0 - b/w[1] \n",
    "ax.plot(x0, decision_boundary, 'k-')  # gráfica del hiperplano separador \n",
    "\n",
    "plt.xlabel('petal length')\n",
    "plt.ylabel('petal width')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293491d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El modelo cuyo **límite de decisión** está representado por la línea discontinua es tan malo que ni siquiera separa las clases correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa64669f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Los otros dos modelos funcionan perfectamente en este conjunto de entrenamiento, \n",
    "* pero sus límites de decisión se acercan tanto a las instancias que estos modelos probablemente no funcionarán tan bien en nuevas instancias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7486e518",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Por el contrario, la línea continua en el gráfico de la derecha representa el límite de decisión de un clasificador SVM; \n",
    "* esta línea no solo separa las dos clases, sino que también se mantiene lo más alejada posible de las instancias de entrenamiento más cercanas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbca33ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Puede pensar en un clasificador SVM como si se ajustara a la calle más ancha posible (representada por las líneas discontinuas paralelas) entre las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267241c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto se llama **clasificación de gran margen**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b83c04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tenga en cuenta que agregar más instancias de capacitación \"fuera de la calle\" no afectará en absoluto el límite de decisión: está completamente determinado (o \"respaldado\") por las instancias ubicadas en el borde de la calle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3968da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estos casos se denominan **vectores de soporte** (están encerrados en un círculo en la figura 5-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc272dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_2.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a4f8cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ADVERTENCIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a583d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las SVM son sensibles a las escalas de atributos, como puede ver en la Figura 5-2: en el gráfico de la izquierda, la escala vertical es mucho más grande que la escala horizontal, por lo que la calle más ancha posible está cerca de la horizontal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc18d4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Después de escalar atributos (p. ej., usando `StandardScaler` de Scikit-Learn), el límite de decisión en el gráfico de la derecha se ve mucho mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e76986",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Soft Margin Classification\n",
    "Clasificación de margen blando"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3916dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si imponemos estrictamente que todas las instancias deben estar fuera de la calle y del lado derecho, esto se denomina *clasificación de margen duro*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324f902",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hay dos problemas principales con la **clasificación de margen duro**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a6477",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Primero, solo funciona si los datos son **linealmente separables**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce511324",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En segundo lugar, es sensible a los **valores atípicos**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b73e35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* La Figura 5-3 muestra el conjunto de datos del iris con solo un valor atípico adicional: a la izquierda, es imposible encontrar un margen duro;   \n",
    "* a la derecha, el límite de decisión termina siendo muy diferente del que vimos en la Figura 5-1 sin el valor atípico, y probablemente tampoco se generalice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4146fdd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_3.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d98cb6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para evitar estos problemas, utilice un modelo más flexible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efabdde",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El objetivo es encontrar un buen equilibrio entre mantener la calle lo más grande posible y limitar las violaciones de los márgenes (es decir, instancias que terminan en el medio de la calle o incluso en el lado equivocado)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6446fb20",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto se llama ***clasificación de margen suave***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8e352",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Al crear un modelo SVM con Scikit-Learn, podemos especificar una serie de hiperparámetros. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a46e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`C` es uno de esos hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d89b3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si lo establecemos en un valor bajo, terminamos con el modelo a la izquierda de la Figura 5-4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2cdef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_4.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42824d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Con un valor alto, obtenemos el modelo de la derecha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97152f3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las violaciones de los márgenes son malas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32abef31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por lo general, es mejor tener algunos de ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ffd52d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sin embargo, en este caso, el modelo de la izquierda tiene muchas violaciones de márgenes, pero probablemente generalice mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb05fe04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sugerencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d2cd1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si su modelo SVM está sobreajustado, puede intentar regularizarlo reduciendo `C`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a09e92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El siguiente código de Scikit-Learn carga el conjunto de datos de iris, escala las características y luego entrena un modelo SVM lineal (usando la clase `LinearSVC` con `C=1` y la función de pérdida de bisagra, descrita en breve) para detectar flores de Iris virginica:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba04688c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris virginica\n",
    "svm_clf = Pipeline([\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "])\n",
    "svm_clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341fd646",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El modelo resultante se representa a la izquierda en la Figura 5-4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784a680",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Luego, como de costumbre, puede usar el modelo para hacer predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0de8e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a018a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NOTA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f1692",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A diferencia de los clasificadores de regresión logística, los clasificadores `SVM` no generan probabilidades para cada clase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424b749",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En lugar de usar la clase `LinearSVC`, podríamos usar la clase SVC con un núcleo lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733bb98",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Al crear el modelo SVC, escribiríamos `SVC(kernel=\"linear\", C=1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b797a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "O podríamos usar la clase SGDClassifier, con `SGDClassifier(loss=\"hinge\", alpha=1/(m*C))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77abe2bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "conda install -c conda-forge/label/broken tensorflow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd48092",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto aplica el Descenso de Gradiente Estocástico regular (vea el Capítulo 4) para entrenar un clasificador SVM lineal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df449dea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "No converge tan rápido como la clase `LinearSVC`, pero puede ser útil para manejar tareas de clasificación en línea o grandes conjuntos de datos que no caben en la memoria (entrenamiento fuera del núcleo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d1dca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sugerencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b144fb29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La clase `LinearSVC` regulariza el término de sesgo, por lo que primero debe centrar el conjunto de entrenamiento restando su media."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0396a56e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto es automático si escala los datos usando `StandardScaler`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6dac4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "También asegúrese de establecer el hiperparámetro  `loss` en `hinge`, ya que no es el valor predeterminado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd3a168",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finalmente, para un mejor rendimiento, debe establecer el hiperparámetro `dual` en `False`, a menos que haya más funciones que instancias de entrenamiento (hablaremos de la dualidad más adelante en este capítulo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f233e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonlinear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6c149",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aunque los clasificadores SVM lineales son eficientes y funcionan sorprendentemente bien en muchos casos, muchos conjuntos de datos ni siquiera están cerca de ser separables linealmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa157ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Un enfoque para manejar conjuntos de datos no lineales es agregar más atributos,  \n",
    "* como atributos polinomiales (como hizo en el Capítulo 4); \n",
    "* en algunos casos, esto puede dar como resultado un conjunto de datos linealmente separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765e6300",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Considere el diagrama de la izquierda en la Figura 5-5: representa un conjunto de datos simple con solo una característica, $x_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eccdb75",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_5.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b972b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este conjunto de datos no es linealmente separable, como puede ver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975c2bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pero si agrega una segunda característica $x_{2} = (x_{1})^{2}$ , el conjunto de datos 2D resultante es perfectamente separable linealmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d484e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para implementar esta idea usando Scikit-Learn, cree un Pipeline que contenga un transformador `PolynomialFeatures` (discutido en \"Regresión polinomial\"), seguido de un `StandardScaler` y un `LinearSVC`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d136ec09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Probemos esto en el conjunto de datos de las lunas: este es un conjunto de datos de juguete para la clasificación binaria en el que los puntos de datos tienen la forma de dos semicírculos intercalados (consulte la Figura 5-6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98077921",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_6.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cd27c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Puedes generar este conjunto de datos usando la función `make_moons()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c1d58f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "polynomial_svm_clf = Pipeline([\n",
    "                              (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "                              (\"scaler\", StandardScaler()),\n",
    "                              (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "                              ])\n",
    "polynomial_svm_clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d3f6ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66555c55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Agregar atributos polinomiales es simple de implementar y puede funcionar muy bien con todo tipo de algoritmos de aprendizaje automático (no solo SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d9de06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dicho esto, en un grado polinomial bajo, este método no puede manejar conjuntos de datos muy complejos, y con un grado polinomial alto crea una gran cantidad de características, lo que hace que el modelo sea demasiado lento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e0d16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Afortunadamente, al usar SVM, puede aplicar una técnica matemática casi milagrosa llamada truco del kernel (explicado en un momento)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebbc9dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El truco del kernel hace posible obtener el mismo resultado que si hubiera agregado muchos atributos polinomiales, incluso con polinomios de muy alto grado, sin tener que agregarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fc3a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por lo tanto, no hay una explosión combinatoria de la cantidad de atributos porque en realidad no agrega ningún atributo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ff6fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este truco lo implementa la clase `SVC`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d864a474",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Vamos a probarlo en el conjunto de datos de las lunas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aaf3e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "iris = load_iris()\n",
    "X, y = iris.data[:,(2,3)], iris.target\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.svm import SVC\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "])\n",
    "poly_kernel_svm_clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8c721",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este código entrena un clasificador `SVM` utilizando un kernel polinomial de tercer grado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f2a0c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Se representa a la izquierda en la Figura 5-7. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c798f0c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A la derecha hay otro clasificador `SVM` que utiliza un núcleo polinomial de décimo grado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f1fd3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_7.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc4d1f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Obviamente, si su modelo se ajusta en exceso, es posible que desee reducir el grado del polinomio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6105471",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por el contrario, si no se ajusta bien, puede intentar aumentarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae375c1b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El hiperparámetro `coef0` controla cuánto influyen en el modelo los polinomios de alto grado frente a los polinomios de bajo grado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5877bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sugerencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e79d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Un enfoque común para encontrar los valores correctos de hiperparámetros es usar la búsqueda en cuadrícula (consulte el Capítulo 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff51a459",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A menudo es más rápido hacer primero una búsqueda de cuadrícula muy gruesa y luego una búsqueda de cuadrícula más fina alrededor de los mejores valores encontrados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148ce6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tener una buena idea de lo que realmente hace cada hiperparámetro también puede ayudarlo a buscar en la parte correcta del espacio de hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d1d2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Similarity Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e173a30e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Otra técnica para abordar problemas no lineales es agregar atributos calculados usando una función de similitud, que mide cuánto se parece cada instancia a un punto de referencia particular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd595ce0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por ejemplo, tomemos el conjunto de datos 1D discutido anteriormente y agreguemos dos puntos de referencia en $x = –2$ y $x = 1$ (vea el diagrama de la izquierda en la Figura 5-8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff21276",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_8.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e32cd29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A continuación, definamos que la función de similitud es la función de base radial gaussiana (RBF) con $\\gamma = 0,3$ (consulte la ecuación 5-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf69758",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 5-1. Gaussian RBF\n",
    "\n",
    "$$ \\phi_{\\gamma}(x,\\mathcal{l}) = exp\\left( -\\gamma \\Vert x - \\mathcal{l} \\Vert^{2} \\right) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac349c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta es una función en forma de campana que varía de 0 (muy lejos del punto de referencia) a 1 (en el punto de referencia)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdf9b09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora estamos listos para calcular las nuevas características."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c2e09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por ejemplo, veamos la instancia $x = -1$: se encuentra a una distancia de 1 del primer punto de referencia y de 2 del segundo punto de referencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43552a82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por lo tanto, sus nuevos atributos son $x = exp(–0,3 × 1) \\approx 0,74$ y $x = exp(– 0,3 × 2) \\approx 0,30$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef1a170",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El gráfico de la derecha en la Figura 5-8 muestra el conjunto de datos transformado (eliminando los atributos originales)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071931d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Como puede ver, ahora es linealmente separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84dadc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Quizás se pregunte cómo seleccionar los puntos de referencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf5414d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El enfoque más simple es crear un punto de referencia en la ubicación de todas y cada una de las instancias del conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d107eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hacer eso crea muchas dimensiones y, por lo tanto, aumenta las posibilidades de que el conjunto de entrenamiento transformado sea linealmente separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc232a09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La desventaja es que un conjunto de entrenamiento con $m$ instancias y $n$ atributos se transforma en un conjunto de entrenamiento con $m$ instancias y $m$ atributos (suponiendo que descarte los atributos originales)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3091a5e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si su conjunto de entrenamiento es muy grande, terminará con una cantidad igualmente grande de atributos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e42500",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian RBF Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dcb813",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Al igual que el método de características polinómicas, el método de características de similitud puede ser útil con cualquier algoritmo de aprendizaje automático, pero puede ser computacionalmente costoso calcular todas las características adicionales, especialmente en grandes conjuntos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb09a9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una vez más, el truco del kernel hace su magia SVM, haciendo posible obtener un resultado similar como si hubiera agregado muchas características de similitud. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa64c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Probemos la clase SVC con el núcleo Gaussian RBF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b5e63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "])\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad69250",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este modelo se representa en la parte inferior izquierda de la Figura 5-9. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c603b6c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los otros gráficos muestran modelos entrenados con diferentes valores de hiperparámetros gamma ($\\gamma$) y `C`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34481427",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El aumento de gamma hace que la curva en forma de campana sea más estrecha (vea los gráficos de la derecha en la Figura 5-8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e2954",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Como resultado, el rango de influencia de cada instancia es más pequeño: el límite de decisión termina siendo más irregular, moviéndose alrededor de las instancias individuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb50f30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por el contrario, un valor de gamma pequeño hace que la curva en forma de campana sea más ancha: las instancias tienen un mayor rango de influencia y el límite de decisión termina siendo más suave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b3dc5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entonces $\\gamma$ actúa como un hiperparámetro de regularización: si su modelo se sobreajusta, debe reducirlo; si es insuficiente, debe aumentarlo (similar al hiperparámetro `C`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0367854",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Existen otros núcleos, pero se usan mucho menos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49156405",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Algunos núcleos están especializados para estructuras de datos específicas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3738dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los núcleos de cadena se utilizan a veces al clasificar documentos de texto o secuencias de ADN (por ejemplo, utilizando el núcleo de subsecuencia de cadena o los núcleos basados en la distancia de Levenshtein)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba052c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_9.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a7cf9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sugerencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75339f7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Con tantos núcleos para elegir, ¿cómo puede decidir cuál usar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187d188",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Como regla general, siempre debe probar primero el kernel lineal (recuerde que LinearSVC es mucho más rápido que SVC(kernel=\"linear\")), especialmente si el conjunto de entrenamiento es muy grande o si tiene muchas funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492c3920",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si el conjunto de entrenamiento no es demasiado grande, también debería probar el kernel Gaussian RBF; funciona bien en la mayoría de los casos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570fcee6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Luego, si tiene tiempo libre y poder de cómputo, puede experimentar con algunos otros núcleos, utilizando la validación cruzada y la búsqueda en cuadrícula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aedab7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Le gustaría experimentar así, especialmente si hay núcleos especializados para la estructura de datos de su conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac0c6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Complejidad computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a6480",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La clase LinearSVC se basa en la biblioteca liblinear, que implementa un algoritmo optimizado para SVM lineales. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10c622c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "No es compatible con el truco del kernel, pero escala casi linealmente con la cantidad de instancias de capacitación y la cantidad de funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c55ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Su complejidad de tiempo de entrenamiento es aproximadamente $O(m \\times n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4336c4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El algoritmo tarda más si necesita una precisión muy alta. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ee82c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto está controlado por el hiperparámetro de tolerancia $\\epsilon$ (llamado `tol` en Scikit-Learn). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db75f3d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En la mayoría de las tareas de clasificación, la tolerancia predeterminada está bien."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef63153",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La clase SVC se basa en la biblioteca libsvm, que implementa un algoritmo que admite el truco del núcleo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048954ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La complejidad del tiempo de entrenamiento suele estar entre $O(m \\times n)$ y $O(m \\times n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9477e44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Desafortunadamente, esto significa que se vuelve terriblemente lento cuando la cantidad de instancias de capacitación aumenta (por ejemplo, cientos de miles de instancias)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e1ab6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este algoritmo es perfecto para conjuntos de entrenamiento complejos de tamaño pequeño o mediano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a49506",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Se escala bien con la cantidad de funciones, especialmente con funciones escasas (es decir, cuando cada instancia tiene pocas funciones distintas de cero)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1c3740",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En este caso, el algoritmo se escala aproximadamente con el número promedio de entidades distintas de cero por instancia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4864dcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Table 5-1 compares Scikit-Learn’s SVM classification classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5311344b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/table_5_1.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2990bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVM Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986882f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Como se mencionó anteriormente, el algoritmo SVM es versátil: no solo admite la clasificación lineal y no lineal, sino que también admite la regresión lineal y no lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465d7954",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para usar SVM para la regresión en lugar de la clasificación, el truco es invertir el objetivo: en lugar de intentar encajar la calle más grande posible entre dos clases mientras limita las violaciones de margen, SVM Regression intenta encajar tantas instancias como sea posible en la calle mientras limita el margen. infracciones (es decir, instancias fuera de la calle)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae8aac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El ancho de la calle está controlado por un hiperparámetro, $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4499de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La Figura 5-10 muestra dos modelos de regresión SVM lineal entrenados en algunos datos lineales aleatorios, uno con un margen grande ($\\epsilon = 1,5$) y el otro con un margen pequeño ($\\epsilon = 0,5$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d94b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_10.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef25068",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Agregar más instancias de entrenamiento dentro del margen no afecta las predicciones del modelo; por lo tanto, se dice que el modelo es $\\epsilon$-insensible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c9c17e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Puede utilizar la clase `LinearSVR` de Scikit-Learn para realizar una regresión SVM lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4febc2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El siguiente código produce el modelo representado a la izquierda en la Figura 5-10 (primero se deben escalar y centrar los datos de entrenamiento):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9f33b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f751d00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para abordar tareas de regresión no lineal, puede usar un modelo `SVM` kernelizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6864916a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La figura 5-11 muestra la regresión SVM en un conjunto de entrenamiento cuadrático aleatorio, utilizando un núcleo polinomial de segundo grado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6915a38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hay poca regularización en el gráfico de la izquierda (es decir, un valor de `C` grande) y mucha más regularización en el gráfico de la derecha (es decir, un valor de `C` pequeño)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48f118a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_11.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f4224",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El siguiente código usa la clase SVR de Scikit-Learn (que admite el truco del kernel) para producir el modelo representado a la izquierda en la Figura 5-11:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098e83c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f09d41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La clase `SVR` es el equivalente de regresión de la clase `SVC`, y la clase `LinearSVR` es el equivalente de regresión de la clase `LinearSVC`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b32196",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La clase `LinearSVR` escala linealmente con el tamaño del conjunto de entrenamiento (al igual que la clase `LinearSVC`), mientras que la clase `SVR` se vuelve demasiado lenta cuando el conjunto de entrenamiento crece (al igual que la clase `SVC`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c01be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NOTE  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1867c837",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las SVM también se pueden usar para la detección de valores atípicos; consulte la documentación de Scikit-Learn para obtener más detalles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b461c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bajo el capó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e1ac6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta sección explica cómo las SVM hacen predicciones y cómo funcionan sus algoritmos de entrenamiento, comenzando con clasificadores SVM lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb970c26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si recién está comenzando con Machine Learning, puede omitirlo de manera segura e ir directamente a los ejercicios al final de este capítulo, y regresar más tarde cuando desee obtener una comprensión más profunda de SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad581bb5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Primero, una palabra acerca de las notaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb326d8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En el Capítulo 4 usamos la convención de poner todos los parámetros del modelo en un vector $\\mathbf{\\theta}$, incluido el término de sesgo $\\theta_{0}$ y los pesos de los atributos de entrada $\\theta_{1}$ a $ \\theta_{n}$, y agregando una entrada de sesgo $x = 1$ a todas las instancias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c02a4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En este capítulo usaremos una convención que es más conveniente (y más común) cuando se trata de SVM:  \n",
    "* el término de sesgo se llamará $b$ y \n",
    "* el vector de ponderaciones de atributos se llamará $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f9bba8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "No se agregará ninguna característica de sesgo a los vectores de características de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a7f5f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Function and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0027a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El modelo clasificador lineal `SVM` predice la clase de una nueva instancia $x$ simplemente calculando la función de decisión $w^{T}x + b = w_{1} x_{1} + \\cdots + w_{n} x_ {n} + b$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ac569",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si el resultado es positivo, la clase predicha $\\hat{y}$ es la clase positiva (1), y de lo contrario, es la clase negativa (0); ver la Ecuación 5-2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484c1db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 5-2. Linear SVM classifier prediction  \n",
    "\n",
    "$$ \\hat{y} = \\begin{cases} 0 & \\text{si}\\ w^{T}x + b < 0 \\\\ 1 & \\text{si}\\ w^{T}x + b \\geq 0 \\end{cases} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cadaa6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La Figura 5-12 muestra la función de decisión que corresponde al modelo de la izquierda en la Figura 5-4: es un plano 2D porque este conjunto de datos tiene dos características (ancho de pétalo y largo de pétalo). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd0abb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El límite de decisión es el conjunto de puntos donde la función de decisión es igual a 0: es la intersección de dos planos, que es una línea recta (representada por la línea continua gruesa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a739e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "figura 5.12  \n",
    "\n",
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_12.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9c4ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las líneas discontinuas representan los puntos donde la función de decisión es igual a 1 o –1: son paralelas ya la misma distancia del límite de decisión, y forman un margen a su alrededor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c427f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entrenar un clasificador SVM lineal significa encontrar los valores de $\\mathbf{w}$ y $b$ que hacen que este margen sea lo más amplio posible mientras se evitan violaciones de margen (margen duro) o se limitan (margen blando)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d0a1b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objetivo de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae423f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Considere la pendiente de la función de decisión: es igual a la norma del vector de peso, $\\Vert w \\Vert$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedc9b5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si dividimos esta pendiente por 2, los puntos donde la función de decisión es igual a $\\pm 1$ van a estar el doble de lejos de la frontera de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36bdd12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En otras palabras, dividir la pendiente por 2 multiplicará el margen por 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d50623",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto puede ser más fácil de visualizar en 2D, como se muestra en la Figura 5-13."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0cd3cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cuanto menor sea el vector de peso $w$, mayor será el margen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74585a70",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_13.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c1680",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entonces queremos minimizar $\\Vert w \\Vert$ para obtener un margen grande."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f9683f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si también queremos evitar violaciones de margen (margen duro), entonces necesitamos que la función de decisión sea mayor que 1 para todas las instancias de entrenamiento positivo y menor que -1 para instancias de entrenamiento negativo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd31b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si definimos $t^{(i)} = –1$ para instancias negativas (si $y^{(i)} = 0$) y $t^{(i)} = 1$ para instancias positivas (si $ y^{(i)} = 1$), entonces podemos expresar esta restricción como $t^{(i)} (w^{T} x^{(i)} + b) \\geq 1$ para todas las instancias ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e51ef69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por lo tanto, podemos expresar el objetivo del clasificador SVM lineal de margen duro como el problema de optimización con restricciones en la Ecuación 5-3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95c962d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ecuación 5-3. Objetivo clasificador SVM lineal de margen duro\n",
    "\n",
    "$$ \\text{minimice}_{w,b} \\ \\ \\ \\   \\frac{1}{2}w^{T}w $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b6981",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\text{subject to} \\ t^{(i)}\\left( w^{T}x^{(i)} + b \\right) \\geq 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea08db4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ddfe60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estamos minimizando $\\frac{1}{2}w^{T}w$, que es igual a $\\frac{1}{2}\\Vert w \\Vert^{2}$, en lugar de minimizar $\\Vert w \\Vert$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d293db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "De hecho, $\\frac{1}{2}\\Vert w \\Vert^{2}$ tiene una derivada simple y agradable (es solo $w$), mientras que $\\Vert w \\Vert$ no es diferenciable en $w = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54296c30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los algoritmos de optimización funcionan mucho mejor en funciones diferenciables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37c2af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para obtener el objetivo de margen suave, necesitamos introducir una variable de holgura $\\zeta \\geq 0$ para cada instancia: $\\zeta$ mide cuánto se le permite a la $i$ - ésima instancia violar el margen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4386ba2a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora tenemos dos objetivos en conflicto: \n",
    "\n",
    "* hacer que las variables de holgura sean lo más pequeñas posible para reducir las violaciones de los márgenes y \n",
    "* hacer que $\\frac{1}{2}w^{T}w$ sea lo más pequeño posible para aumentar el margen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a00dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is where the `C` hyperparameter comes in: it allows us to define the tradeoff between these two\n",
    "objectives. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d073ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This gives us the constrained optimization problem in Equation 5-4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc5b78d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 5-4. Soft margin linear SVM classifier objective  \n",
    "\n",
    "$$ \\text{minimize}_{} $$\n",
    "$$ \\text{subject to} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011dc5b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quadratic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d679fb1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The hard margin and soft margin problems are both convex quadratic optimization problems with linear constraints. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd82846d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Such problems are known as Quadratic Programming\n",
    "(QP) problems. Many off-the-shelf solvers are available to solve QP problems by using a\n",
    "variety of techniques that are outside the scope of this book.\n",
    "The general problem formulation is given by Equation 5-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345bb2b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9812da18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementación en código de la propuesta de Gerón para la aplicación de las Máquinas de soporte vectorial para clasificación binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab4551e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Chapter 5 – Support Vector Machines. Cuaderno de Geron en GitHub](https://github.com/ageron/handson-ml2/blob/master/05_support_vector_machines.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834adbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este cuaderno contiene todo el código de muestra y las soluciones a los ejercicios del capítulo 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278cb967",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb23f27c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Primero, importemos algunos módulos comunes, asegurémonos de que MatplotLib trace figuras en línea y prepare una función para guardar las figuras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334560ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "También verificamos que Python 3.5 o posterior esté instalado (aunque Python 2.x puede funcionar, está obsoleto, por lo que le recomendamos que use Python 3 en su lugar), así como Scikit-Learn $\\geq $ 0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d66dfe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa4e32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"svm\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7af5646",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243521f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las siguientes celdas de código generan las primeras cifras en el capítulo 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a73ec7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El primer ejemplo de código real viene después."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb2ce82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Código para generar la Figura 5–1. Clasificación de gran margen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9bf941",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "setosa_or_versicolor = (y == 0) | (y == 1)\n",
    "X = X[setosa_or_versicolor]\n",
    "y = y[setosa_or_versicolor]\n",
    "\n",
    "# SVM Classifier model\n",
    "svm_clf = SVC(kernel=\"linear\", C=float(\"inf\"))\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30bd51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Bad models\n",
    "x0 = np.linspace(0, 5.5, 200)\n",
    "pred_1 = 5*x0 - 20\n",
    "pred_2 = x0 - 1.8\n",
    "pred_3 = 0.1 * x0 + 0.5\n",
    "\n",
    "def plot_svc_decision_boundary(svm_clf, xmin, xmax):\n",
    "    w = svm_clf.coef_[0]\n",
    "    b = svm_clf.intercept_[0]\n",
    "\n",
    "    # At the decision boundary, w0*x0 + w1*x1 + b = 0\n",
    "    # => x1 = -w0/w1 * x0 - b/w1\n",
    "    x0 = np.linspace(xmin, xmax, 200)\n",
    "    decision_boundary = -w[0]/w[1] * x0 - b/w[1]\n",
    "\n",
    "    margin = 1/w[1]\n",
    "    gutter_up = decision_boundary + margin\n",
    "    gutter_down = decision_boundary - margin\n",
    "\n",
    "    svs = svm_clf.support_vectors_\n",
    "    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA')\n",
    "    plt.plot(x0, decision_boundary, \"k-\", linewidth=2)\n",
    "    plt.plot(x0, gutter_up, \"k--\", linewidth=2)\n",
    "    plt.plot(x0, gutter_down, \"k--\", linewidth=2)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(x0, pred_1, \"g--\", linewidth=2)\n",
    "plt.plot(x0, pred_2, \"m-\", linewidth=2)\n",
    "plt.plot(x0, pred_3, \"r-\", linewidth=2)\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris versicolor\")\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([0, 5.5, 0, 2])\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_svc_decision_boundary(svm_clf, 0, 5.5)\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.axis([0, 5.5, 0, 2])\n",
    "\n",
    "save_fig(\"large_margin_classification_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2173fa24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Code to generate Figure 5–2. Sensitivity to feature scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd60e38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Xs = np.array([[1, 50], [5, 20], [3, 80], [5, 60]]).astype(np.float64)\n",
    "ys = np.array([0, 0, 1, 1])\n",
    "svm_clf = SVC(kernel=\"linear\", C=100)\n",
    "svm_clf.fit(Xs, ys)\n",
    "\n",
    "plt.figure(figsize=(9,2.7))\n",
    "plt.subplot(121)\n",
    "plt.plot(Xs[:, 0][ys==1], Xs[:, 1][ys==1], \"bo\")\n",
    "plt.plot(Xs[:, 0][ys==0], Xs[:, 1][ys==0], \"ms\")\n",
    "plot_svc_decision_boundary(svm_clf, 0, 6)\n",
    "plt.xlabel(\"$x_0$\", fontsize=20)\n",
    "plt.ylabel(\"$x_1$    \", fontsize=20, rotation=0)\n",
    "plt.title(\"Unscaled\", fontsize=16)\n",
    "plt.axis([0, 6, 0, 90])\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(Xs)\n",
    "svm_clf.fit(X_scaled, ys)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(X_scaled[:, 0][ys==1], X_scaled[:, 1][ys==1], \"bo\")\n",
    "plt.plot(X_scaled[:, 0][ys==0], X_scaled[:, 1][ys==0], \"ms\")\n",
    "plot_svc_decision_boundary(svm_clf, -2, 2)\n",
    "plt.xlabel(\"$x'_0$\", fontsize=20)\n",
    "plt.ylabel(\"$x'_1$  \", fontsize=20, rotation=0)\n",
    "plt.title(\"Scaled\", fontsize=16)\n",
    "plt.axis([-2, 2, -2, 2])\n",
    "\n",
    "save_fig(\"sensitivity_to_feature_scales_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec084c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7845b389",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a2c583",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a0157f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6717a2b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [1.4.7. Mathematical formulation](https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c300c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una máquina de vectores de soporte construye un hiperplano o un conjunto de hiperplanos en un espacio dimensional alto o infinito, que puede usarse para clasificación, regresión u otras tareas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bafd8cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Intuitivamente, una buena separación se logra con el hiperplano que tiene la mayor distancia a los puntos de datos de entrenamiento más cercanos de cualquier clase (el llamado margen funcional), ya que, en general, cuanto mayor es el margen, menor es el error de generalización del clasificador."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189cba95",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La siguiente figura muestra la función de decisión para un problema linealmente separable, con tres muestras en los límites del margen, llamados \"vectores de soporte\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf25bfb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://scikit-learn.org/stable/_images/sphx_glr_plot_separating_hyperplane_001.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b335ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En general, cuando el problema no es linealmente separable, los vectores de soporte son las muestras dentro de los límites del margen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7dc960",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We recommend [13] and [14] as good references for the theory and practicalities of SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9610d92e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.4.7.1. SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b6d43e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dados los vectores de entrenamiento $x_{i} \\in \\mathbb{R}^{p}$, $i = 1, \\ldots, n$, en dos clases, y un vector $y \\in \\{1,-1\\}$, nuestro objetivo es encontrar $w \\in \\mathbb{R}^{P}$ y $b \\in \\mathbb{R}$ tales que la predicción dada por $sgn(w^{T}\\phi( x) + b)$ es correcto para la mayoría de las muestras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4740c990",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "SVC solves the following primal problem:\n",
    "\n",
    "$$ \\min_{w,b,\\zeta} \\ \\ \\frac{1}{2}w^{T}w + C \\sum_{i = 1}^{n} \\zeta_{i}  $$\n",
    "\n",
    "$$ \\text{Subject to}\\ \\  y_{i}() $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e3c105",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac92ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2bb27a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e41cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd659fad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Referencias  \n",
    "\n",
    "* El modelo de Máquina de soporte vectorial: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html  \n",
    "\n",
    "* Cuaderno de Geron sobre Maquinas de Soporte Vectorial: https://github.com/ageron/handson-ml2/blob/master/05_support_vector_machines.ipynb\n",
    "\n",
    "* Formulación matemática de la SVC: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
    "\n",
    "* Duval, R. (2004). Semiosis y pensamiento humano: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cadefe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "rise": {
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
