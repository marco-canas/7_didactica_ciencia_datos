{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60c0a4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_sv/chap_5_svm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f50600",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 5. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541097ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una máquina de soporte vectorial (SVM) es un modelo de aprendizaje automático potente y versátil, capaz de realizar clasificación lineal o no lineal, regresión e incluso detección de valores atípicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4a4c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es uno de los modelos más populares en Machine Learning, y cualquier persona interesada en Machine Learning debería tenerlo en su caja de herramientas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf76a4bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las SVM son particularmente adecuadas para la clasificación de conjuntos de datos complejos de tamaño pequeño o mediano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64899e27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este capítulo explicará los conceptos básicos de las SVM, cómo usarlas y cómo funcionan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4885bb1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792f3f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La idea fundamental detrás de las SVM se explica mejor con algunas imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f0c382",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La Figura 5-1 muestra parte del conjunto de datos del iris que se presentó al final del Capítulo 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b33fe9e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las dos clases se pueden separar fácilmente con una línea recta (son linealmente separables). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1babb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El gráfico de la izquierda muestra los límites de decisión de tres posibles clasificadores lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c15bbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_1.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293491d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El modelo cuyo límite de decisión está representado por la línea discontinua es tan malo que ni siquiera separa las clases correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa64669f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los otros dos modelos funcionan perfectamente en este conjunto de entrenamiento, pero sus límites de decisión se acercan tanto a las instancias que estos modelos probablemente no funcionarán tan bien en nuevas instancias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7486e518",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por el contrario, la línea continua en el gráfico de la derecha representa el límite de decisión de un clasificador SVM; esta línea no solo separa las dos clases, sino que también se mantiene lo más alejada posible de las instancias de entrenamiento más cercanas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbca33ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Puede pensar en un clasificador SVM como si se ajustara a la calle más ancha posible (representada por las líneas discontinuas paralelas) entre las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267241c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto se llama **clasificación de gran margen**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b83c04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tenga en cuenta que agregar más instancias de capacitación \"fuera de la calle\" no afectará en absoluto el límite de decisión: está completamente determinado (o \"respaldado\") por las instancias ubicadas en el borde de la calle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3968da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estos casos se denominan vectores de soporte (están encerrados en un círculo en la figura 5-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc272dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_sv/figure_5_2.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a4f8cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ADVERTENCIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a583d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las SVM son sensibles a las escalas de características, como puede ver en la Figura 5-2: en el gráfico de la izquierda, la escala vertical es mucho más grande que la escala horizontal, por lo que la calle más ancha posible está cerca de la horizontal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc18d4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Después de escalar características (p. ej., usando StandardScaler de Scikit-Learn), el límite de decisión en el gráfico de la derecha se ve mucho mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e76986",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Soft Margin Classification\n",
    "Clasificación de margen blando"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3916dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si imponemos estrictamente que todas las instancias deben estar fuera de la calle y del lado derecho, esto se denomina *clasificación de margen duro*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324f902",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hay dos problemas principales con la clasificación de margen duro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a6477",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Primero, solo funciona si los datos son linealmente separables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce511324",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En segundo lugar, es sensible a los valores atípicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b73e35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La Figura 5-3 muestra el conjunto de datos del iris con solo un valor atípico adicional: a la izquierda, es imposible encontrar un margen duro; a la derecha, el límite de decisión termina siendo muy diferente del que vimos en la Figura 5-1 sin el valor atípico, y probablemente tampoco se generalice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4146fdd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_sv/figure_5_3.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d98cb6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para evitar estos problemas, utilice un modelo más flexible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efabdde",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El objetivo es encontrar un buen equilibrio entre mantener la calle lo más grande posible y limitar las violaciones de los márgenes (es decir, instancias que terminan en el medio de la calle o incluso en el lado equivocado)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6446fb20",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto se llama *clasificación de margen suave*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8e352",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Al crear un modelo SVM con Scikit-Learn, podemos especificar una serie de hiperparámetros. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a46e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "C es uno de esos hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d89b3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si lo establecemos en un valor bajo, terminamos con el modelo a la izquierda de la Figura 5-4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42824d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Con un valor alto, obtenemos el modelo de la derecha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97152f3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las violaciones de los márgenes son malas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32abef31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por lo general, es mejor tener algunos de ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2cdef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_sv/figure_5_4.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ffd52d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sin embargo, en este caso, el modelo de la izquierda tiene muchas violaciones de márgenes, pero probablemente generalice mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb05fe04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sugerencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d2cd1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si su modelo SVM está sobreajustado, puede intentar regularizarlo reduciendo C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a09e92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El siguiente código de Scikit-Learn carga el conjunto de datos de iris, escala las características y luego entrena un modelo SVM lineal (usando la clase LinearSVC con C=1 y la función de pérdida de bisagra, descrita en breve) para detectar flores de Iris virginica:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba04688c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_svc', LinearSVC(C=1, loss='hinge'))])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris virginica\n",
    "svm_clf = Pipeline([\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "])\n",
    "svm_clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341fd646",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El modelo resultante se representa a la izquierda en la Figura 5-4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784a680",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Luego, como de costumbre, puede usar el modelo para hacer predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c0de8e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a018a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f1692",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A diferencia de los clasificadores de regresión logística, los clasificadores SVM no generan probabilidades para cada clase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424b749",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En lugar de usar la clase `LinearSVC`, podríamos usar la clase SVC con un núcleo lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733bb98",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Al crear el modelo SVC, escribiríamos `SVC(kernel=\"linear\", C=1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b797a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "O podríamos usar la clase SGDClassifier, con `SGDClassifier(loss=\"hinge\", alpha=1/(m*C))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96248889",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge/label/broken tensorflow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd48092",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto aplica el Descenso de Gradiente Estocástico regular (vea el Capítulo 4) para entrenar un clasificador SVM lineal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df449dea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "No converge tan rápido como la clase LinearSVC, pero puede ser útil para manejar tareas de clasificación en línea o grandes conjuntos de datos que no caben en la memoria (entrenamiento fuera del núcleo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d1dca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sugerencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b144fb29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La clase LinearSVC regulariza el término de sesgo, por lo que primero debe centrar el conjunto de entrenamiento restando su media."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0396a56e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto es automático si escala los datos usando StandardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6dac4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "También asegúrese de establecer el hiperparámetro de pérdida en \"bisagra\", ya que no es el valor predeterminado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd3a168",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finalmente, para un mejor rendimiento, debe establecer el hiperparámetro dual en Falso, a menos que haya más funciones que instancias de entrenamiento (hablaremos de la dualidad más adelante en este capítulo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f233e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonlinear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6c149",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aunque los clasificadores SVM lineales son eficientes y funcionan sorprendentemente bien en muchos casos, muchos conjuntos de datos ni siquiera están cerca de ser separables linealmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa157ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Un enfoque para manejar conjuntos de datos no lineales es agregar más atributos, como atributos polinomiales (como hizo en el Capítulo 4); en algunos casos, esto puede dar como resultado un conjunto de datos linealmente separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765e6300",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Considere el diagrama de la izquierda en la Figura 5-5: representa un conjunto de datos simple con solo una característica, $x_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eccdb75",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_5.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b972b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este conjunto de datos no es linealmente separable, como puede ver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975c2bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pero si agrega una segunda característica $x_{2} = (x_{1})^{2}$ , el conjunto de datos 2D resultante es perfectamente separable linealmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d484e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para implementar esta idea usando Scikit-Learn, cree un Pipeline que contenga un transformador `PolynomialFeatures` (discutido en \"Regresión polinomial\"), seguido de un `StandardScaler` y un `LinearSVC`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d136ec09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Probemos esto en el conjunto de datos de las lunas: este es un conjunto de datos de juguete para la clasificación binaria en el que los puntos de datos tienen la forma de dos semicírculos intercalados (consulte la Figura 5-6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98077921",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_6.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cd27c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Puedes generar este conjunto de datos usando la función `make_moons()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c1d58f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "polynomial_svm_clf = Pipeline([\n",
    "                              (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "                              (\"scaler\", StandardScaler()),\n",
    "                              (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "                              ])\n",
    "polynomial_svm_clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d3f6ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66555c55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Agregar funciones polinómicas es simple de implementar y puede funcionar muy bien con todo tipo de algoritmos de aprendizaje automático (no solo SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d9de06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dicho esto, en un grado polinomial bajo, este método no puede manejar conjuntos de datos muy complejos, y con un grado polinomial alto crea una gran cantidad de características, lo que hace que el modelo sea demasiado lento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e0d16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Afortunadamente, al usar SVM, puede aplicar una técnica matemática casi milagrosa llamada truco del kernel (explicado en un momento)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebbc9dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El truco del núcleo hace posible obtener el mismo resultado que si hubiera agregado muchas características polinómicas, incluso con polinomios de muy alto grado, sin tener que agregarlas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fc3a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por lo tanto, no hay una explosión combinatoria de la cantidad de funciones porque en realidad no agrega ninguna característica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ff6fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este truco lo implementa la clase SVC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d864a474",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Vamos a probarlo en el conjunto de datos de las lunas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8aaf3e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "])\n",
    "poly_kernel_svm_clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a75464",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This code trains an SVM classifier using a third-degree polynomial kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548bce4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is represented on the left in Figure 5-7. On the right is another SVM classifier using a 10th-degree polynomial kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d63258",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Obviously, if your model is overfitting, you might want to reduce the polynomial degree. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d3617b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Conversely, if it is underfitting, you can try increasing it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae375c1b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El hiperparámetro coef0 controla cuánto influyen en el modelo los polinomios de alto grado frente a los polinomios de bajo grado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f1fd3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_7.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5877bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e79d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A common approach to finding the right hyperparameter values is to use grid search (see Chapter 2). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff51a459",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is often faster to first do a very coarse grid search, then a finer grid search around the best values found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148ce6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Having a good sense of what each hyperparameter actually does can also help you search in the right part of the hyperparameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d1d2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Similarity Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e173a30e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another technique to tackle nonlinear problems is to add features computed using a similarity function, which measures how much each instance resembles a particular landmark. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd595ce0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, let’s take the 1D dataset discussed earlier and add two landmarks to it at $x = –2$ and $x = 1$ (see the left plot in Figure 5-8). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e32cd29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, let’s define the similarity function to be the Gaussian Radial Basis Function (RBF) with $\\gamma  = 0.3$  (see Equation 5-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf69758",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 5-1. Gaussian RBF\n",
    "\n",
    "$$ \\phi_{\\gamma}(x,\\mathcal{l}) =  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac349c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at the landmark). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdf9b09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we are ready to compute the new features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c2e09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, let’s look at the instance $x = –1$: it is located at a distance of 1 from the first landmark and 2 from the second landmark. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43552a82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore its new features are x = exp(–0.3 × 1 ) ≈ 0.74 and x = exp(– 0.3 × 2 ) ≈ 0.30. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef1a170",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The plot on the right in Figure 5-8 shows the transformed dataset (dropping the original features). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071931d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As you can see, it is now linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff21276",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_sv/figure_5_8.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84dadc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You may wonder how to select the landmarks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf5414d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The simplest approach is to create a landmark at the location of each and every instance in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d107eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Doing that creates many dimensions and thus increases the chances that the transformed training set will be linearly separable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc232a09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The downside is that a training set with m instances and n features gets transformed into a training set with m instances and m features (assuming you drop the original features). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3091a5e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If your training set is very large, you end up with an equally large number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e42500",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian RBF Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dcb813",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Just like the polynomial features method, the similarity features method can be useful with any Machine Learning algorithm, but it may be computationally expensive to compute all the additional features, especially on large training sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb09a9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once again the kernel trick does its SVM magic, making it possible to obtain a similar result as if you had added many similarity features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa64c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s try the SVC class with the Gaussian RBF kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c3b5e63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=0.001, gamma=5))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "])\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad69250",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This model is represented at the bottom left in Figure 5-9. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c603b6c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The other plots show models trained with different values of hyperparameters gamma (γ) and C. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34481427",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Increasing gamma makes the bell-shaped curve narrower (see the righthand plots in Figure 5-8). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e2954",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As a result, each instance’s range of influence is smaller: the decision boundary ends up being more\n",
    "irregular, wiggling around individual instances. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb50f30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Conversely, a small gamma value makes the bell-shaped curve wider: instances have a larger range of influence, and the decision boundary ends up smoother. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b3dc5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So γ acts like a regularization hyperparameter: if your model is overfitting, you should reduce it; if it is underfitting, you should increase it (similar to the C hyperparameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0367854",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Other kernels exist but are used much more rarely. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49156405",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some kernels are specialized for specific data structures. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3738dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "String kernels are sometimes used when classifying text documents or DNA sequences (e.g., using the string subsequence kernel or kernels based on the Levenshtein distance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba052c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figura_5_9.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a7cf9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75339f7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Con tantos núcleos para elegir, ¿cómo puede decidir cuál usar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187d188",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Como regla general, siempre debe probar primero el kernel lineal (recuerde que LinearSVC es mucho más rápido que SVC(kernel=\"linear\")), especialmente si el conjunto de entrenamiento es muy grande o si tiene muchas funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492c3920",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si el conjunto de entrenamiento no es demasiado grande, también debería probar el kernel Gaussian RBF; funciona bien en la mayoría de los casos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570fcee6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Luego, si tiene tiempo libre y poder de cómputo, puede experimentar con algunos otros núcleos, utilizando la validación cruzada y la búsqueda en cuadrícula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aedab7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Le gustaría experimentar así, especialmente si hay núcleos especializados para la estructura de datos de su conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac0c6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Complejidad computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a6480",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La clase LinearSVC se basa en la biblioteca liblinear, que implementa un algoritmo optimizado para SVM lineales. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10c622c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "No es compatible con el truco del kernel, pero escala casi linealmente con la cantidad de instancias de capacitación y la cantidad de funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c55ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Its training time complexity is roughly $O(m \\times n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4336c4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The algorithm takes longer if you require very high precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ee82c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is controlled by the tolerance hyperparameter ϵ (called tol in Scikit-Learn). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db75f3d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In most classification tasks, the default tolerance is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef63153",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The SVC class is based on the libsvm library, which implements an algorithm that supports\n",
    "the kernel trick. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048954ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The training time complexity is usually between $O(m × n)$ and $O(m × n)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9477e44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unfortunately, this means that it gets dreadfully slow when the number of training instances gets large (e.g., hundreds of thousands of instances). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e1ab6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This algorithm is perfect for complex small or medium-sized training sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a49506",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It scales well with the number of features, especially with sparse features (i.e., when each instance has few nonzero features). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1c3740",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this case, the algorithm scales roughly with the average number of nonzero features per\n",
    "instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4864dcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Table 5-1 compares Scikit-Learn’s SVM classification classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5311344b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/table_5_1.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2990bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVM Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986882f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As mentioned earlier, the SVM algorithm is versatile: not only does it support linear and nonlinear classification, but it also supports linear and nonlinear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465d7954",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To use SVMs for regression instead of classification, the trick is to reverse the objective: instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM\n",
    "Regression tries to fit as many instances as possible on the street while limiting margin\n",
    "violations (i.e., instances off the street). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae8aac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The width of the street is controlled by a hyperparameter, $ϵ$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4499de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Figure 5-10 shows two linear SVM Regression models trained on some random linear data, one with a large margin ($ϵ = 1.5$) and the other with a small margin ($ϵ = 0.5$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d94b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_svm/figure_5_10.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef25068",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Adding more training instances within the margin does not affect the model’s predictions; thus, the model is said to be ϵ-insensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c9c17e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can use Scikit-Learn’s LinearSVR class to perform linear SVM Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4febc2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following code produces the model represented on the left in Figure 5-10 (the training data\n",
    "should be scaled and centered first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe781212",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "rise": {
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
