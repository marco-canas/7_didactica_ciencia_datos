{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_4/c_4_training_models_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 4  \n",
    "## Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Capítulo 4\n",
    "## ¿Qué significa entrenar un modelo? \n",
    "Aurelien Gerón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Página 167"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hasta ahora, hemos tratado los modelos de Machine Learning y sus algoritmos de entrenamiento en su mayoría como cajas negras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si realizó algunos de los ejercicios de los capítulos anteriores, es posible que se haya sorprendido de todo lo que puede hacer sin saber nada sobre lo que hay debajo del capó:  \n",
    "\n",
    "* optimizó un sistema de regresión,   \n",
    "* mejoró un clasificador de imágenes de dígitos e incluso \n",
    "* construyó un clasificador de spam desde cero,   \n",
    "\n",
    "todo esto sin saber cómo funcionan realmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "De hecho, en muchas situaciones no es necesario conocer los detalles de implementación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sin embargo, tener una buena comprensión de cómo funcionan las cosas puede ayudarlo a:\n",
    "\n",
    "* encontrar rápidamente el modelo apropiado, \n",
    "* el algoritmo de entrenamiento adecuado para usar y \n",
    "* un buen conjunto de hiperparámetros para su tarea. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comprender qué hay debajo del capó también lo ayudará a depurar problemas y realizar análisis de errores de manera más eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por último, la mayoría de los temas discutidos en este capítulo serán esenciales para comprender, construir y entrenar redes neuronales (discutidos en la Parte II de este libro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Habrá bastantes ecuaciones matemáticas en este capítulo, usando nociones básicas de álgebra lineal y cálculo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para comprender estas ecuaciones, necesitarás saber qué son los vectores y las matrices; cómo transponerlos, multiplicarlos e invertirlos; y qué son las derivadas parciales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si no está familiarizado con estos conceptos, consulte los tutoriales introductorios de cálculo y álgebra lineal disponibles como cuadernos de Jupyter en el material complementario en línea.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para aquellos que son realmente alérgicos a las matemáticas, aún deben leer este capítulo y simplemente omitir las ecuaciones; con suerte, el texto será suficiente para ayudarlo a comprender la mayoría de los conceptos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En el Capítulo 1 analizamos un modelo de regresión simple de satisfacción con la vida:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* un modelo lineal hace una predicción simplemente calculando una suma ponderada de las \n",
    "  características de entrada, \n",
    "* más una constante llamada término de sesgo (también llamado término de intersección)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 4-1. Linear Regression model prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\hat{y} = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\cdots + \\theta_{n}x_{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En esta ecuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\hat{y}$ es el valor predicho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$n$ es el número de características o atributos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$x_{i}$ es el valor de la $i$ - ésima característica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* $\\theta_{j}$ es el $j$ - ésimo parámetro del modelo \n",
    "* (incluido el término de sesgo $\\theta_{0}$ y los pesos de característica $\\theta_{1},\\theta_{2},\\ldots, \\theta_{n}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto se puede escribir de manera mucho más concisa usando una forma vectorizada, como se muestra en la siguiente ecuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ecuación 4-2. Predicción del modelo de regresión lineal (forma vectorizada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\hat{y} = h_{\\mathbf{\\theta}}(\\mathbf{x}) = \\mathbf{\\theta} \\cdot \\mathbf{x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En esta ecuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* $\\mathbf{\\theta}$ es el vector de parámetros del modelo, que contiene el término de sesgo $\\theta_{0}$ y el de los pesos de las características $\\theta_{1}$ a $\\theta_{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* $\\mathbf{x}$ es el *vector de características* o de instancias, que contiene $x_{0}$ a $x_{n}$, con $x_{0}$ siempre igual a $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "* $\\mathbf{\\theta \\cdot x}$ es el producto escalar de los vectores $\\mathbf{\\theta}$ y $\\mathbf{x}$, que por supuesto es igual a $\\theta_{0}x_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\cdots + \\theta_{n}x_{n}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "* $h_{\\mathbf{\\theta}}$ es la función de hipótesis, utilizando los parámetros del modelo $\\mathbf{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NOTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si $\\mathbf{\\theta}$ y $\\mathbf{x}$ son vectores de columna, entonces la predicción es \n",
    "\n",
    "$$ \\hat{y} = \\mathbf{\\theta}^{T}\\mathbf{x}, $$\n",
    "\n",
    "donde $\\mathbf{\\theta}^{T}$ es la transposición de $\\mathbf{\\theta}$ (un vector de fila en lugar de un vector de columna) y   \n",
    "$\\mathbf{\\theta}^{T}x$ es la multiplicación de las matrices $\\theta^{T}$ y $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* este es el modelo de regresión lineal,   \n",
    "* pero ¿cómo lo entrenamos?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bueno, recuerde que entrenar un modelo significa establecer sus parámetros para que el modelo se ajuste mejor al conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para este propósito, primero necesitamos una medida de qué tan bien (o mal) el modelo se ajusta a los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La medida de rendimiento más común de un modelo de regresión es el error cuadrático medio (RMSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ RMSE(\\theta) = \\sqrt{\\frac{1}{m}\\sum_{j = 0}^{n} (\\theta^{T} x^{j} - y^{j})^{2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por lo tanto, para entrenar un modelo de regresión lineal, necesitamos encontrar el valor de $\\theta$ que minimice el RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* En la práctica, es más sencillo minimizar el error cuadrático medio (MSE) que el RMSE, \n",
    "* y conduce al mismo resultado (pues el valor que minimiza a la función, minimiza su raíz cuadrada). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Se calcula el MSE de una hipótesis de regresión lineal $h$ en un conjunto de entrenamiento $\\mathbf{X}$ usando la ecuación 4-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ecuación 4-3. Función de costo MSE para un modelo de regresión lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ MSE(X,h_{\\theta}) = \\frac{1}{m} \\sum_{i = 1}^{n} (\\mathbf{\\theta}^{T}\\mathbf{x}^{i} - y^{i} )^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La única diferencia es que escribimos $h_{\\mathbf{\\theta}}$ en lugar de solo $h$ para dejar en claro que el modelo está parametrizado por el vector $\\mathbf{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para simplificar las notaciones, simplemente escribiremos $MSE(\\mathbf{\\theta})$ en lugar de $MSE(\\mathbf{X}, h_{\\mathbf{\\theta}})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## La ecuación normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para encontrar el valor de $\\mathbf{\\theta}$ que minimiza la función de costo, existe una solución de forma cerrada, en otras palabras, una ecuación matemática que da el resultado directamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto se llama **ecuación normal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\hat{\\mathbf{\\theta}} = (\\mathbf{X}^{T}\\mathbf{X})^{-1} \\mathbf{X}^{T}y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En esta ecuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* $\\hat{\\mathbf{\\theta}}$ es el valor de $\\mathbf{\\theta}$ que minimiza la función de costo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* $\\mathbf{y}$ es el vector de valores objetivo que contiene $y^{(1)}$ a $y^{(m)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Generemos algunos datos de apariencia lineal para probar esta ecuación (Figura 4-1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Fijamos un estado aleatorio para reproducibilidad\n",
    "np.random.seed(48)\n",
    "X = 2 * np.random.rand(100, 1) \n",
    "# Crea una matriz de orden 100 x 1 con valores aleatorios en el intervalo [0,2) \n",
    "y = 4 + 3 * X + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5)) \n",
    "plt.style.use('dark_background') \n",
    "plt.scatter(X,y, color = 'green')    \n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Conjunto de datos lineales generados aleatoriamente')\n",
    "plt.savefig('dataset_lineal_aleatorio.jpg')\n",
    "\n",
    "plt.grid() \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora calculemos $\\hat{\\mathbf{\\theta}}$ usando la Ecuación Normal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Usaremos la función ``inv()`` del módulo de álgebra lineal de NumPy `` (np.linalg) `` para calcular la inversa de una matriz, y el método ``.dot()`` para la multiplicación de matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "# np.c_ concatena matrices \n",
    "X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La función que usamos para generar los datos es $y = 4 + 3x + \\text{Ruido Gausiano}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Veamos qué encontró la ecuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Habríamos esperado $\\theta_{0} = 4$ y $\\theta_{1}=3$ en lugar de $\\theta_{0} = 4.01$ y $\\theta_{1} = 2.924$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lo suficientemente cerca, pero el ruido hizo imposible recuperar los parámetros exactos de la función original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora podemos hacer predicciones usando $\\hat{\\mathbf{\\theta}}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Grafiquemos las predicciones de este modelo (Figura 4-2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.scatter(X, y, color = 'blue') \n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.savefig('que_significa_entrenar_un_modelo.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Realizar una regresión lineal con Scikit-Learn es simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La clase ``LinearRegression`` se basa en la función ``scipy.linalg.lstsq()`` (el nombre significa \"mínimos cuadrados\"), a la que puede llamar directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Esta función calcula $\\hat{\\theta} = X^{+}y$ donde $X^{+}$ es la pseudoinversa de $X$ (específicamente, la inversa de Moore-Penrose)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Puede usar `np.linalg.pinv()` para calcular la pseudoinversa directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.linalg.pinv(X_b).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La pseudoinversa en sí misma se calcula usando una técnica de factorización de matrices estándar llamada *Descomposición de valores singulares* (SVD) que puede descomponer la matriz del conjunto de entrenamiento $\\mathbf{X}$ en la multiplicación de tres matrices $U\\Sigma V^{T}$ (ver ``numpy.linalg.svd()``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La pseudoinversa se calcula como $\\mathbf{X}^{+} = \\mathbf{V\\Sigma}^{+}\\mathbf{U}^{T}$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para calcular la matriz $\\Sigma^{+}$, el algoritmo toma $\\Sigma$ y establece en cero todos los valores menores que un pequeño valor de umbral, luego reemplaza todos los valores distintos de cero con su inversa y finalmente transpone el resultado matriz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este enfoque es más eficiente que calcular la Ecuación Normal, además maneja muy bien los casos extremos: de hecho, la Ecuación Normal puede no funcionar si la matriz $\\mathbf{X}^{T}\\mathbf{X}$ no es invertible (es decir, , singular), como si $m < n$ o si algunas características son redundantes, pero la pseudoinversa siempre está definida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La ecuación normal calcula la inversa de $X^{T}X$, que es una matriz $(n + 1) \\times (n + 1)$ (donde $n$ es el número de características)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La complejidad computacional de invertir una matriz de este tipo suele ser de $O(n^{2.4})$ a $O(n^{3})$, según la implementación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En otras palabras, si duplica la cantidad de características, multiplica el tiempo de cálculo por aproximadamente $2^{2.4} = 5.3$ a $2^{3} = 8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El enfoque SVD utilizado por la clase LinearRegression de Scikit-Learn es aproximadamente $O(n^{2})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si duplica la cantidad de características, multiplica el tiempo de cálculo por aproximadamente 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WARNING  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Both the Normal Equation and the SVD approach get very slow when the number of features\n",
    "grows large (e.g., 100,000). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En el lado positivo, ambos son lineales con respecto a la cantidad de instancias en el conjunto de entrenamiento (son $O(m)$), por lo que manejan grandes conjuntos de entrenamiento de manera eficiente, siempre que quepan en la memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Also, once you have trained your Linear Regression model (using the Normal Equation or any other algorithm), predictions are very fast: the computational complexity is linear with regard to both the number of instances you want to make predictions on and the number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In other words, making predictions on twice as many instances (or twice as many features) will take\n",
    "roughly twice as much time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we will look at a very different way to train a Linear Regression model, which is better suited for cases where there are a large number of features or too many training instances to fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gradient Descent es un algoritmo de optimización genérico capaz de encontrar soluciones óptimas a una amplia gama de problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La idea general de Gradient Descent es ajustar los parámetros iterativamente para minimizar una función de costo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose you are lost in the mountains in a dense fog, and you can only feel the slope of the ground below your feet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A good strategy to get to the bottom of the valley quickly is to go downhill in the direction of the steepest slope.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto es exactamente lo que hace Gradient Descent: mide el gradiente local de la función de error con respecto al vector de parámetros $\\theta$, y va en la dirección del gradiente descendente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once the gradient is zero, you have reached a minimum!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Concretely, you start by filling θ with random values (this is called random\n",
    "initialization).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm converges to a minimum (see Figure 4-3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='figure_4_3.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Page 174 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An important parameter in Gradient Descent is the size of the steps, determined by the learning rate hyperparameter.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time (see Figure 4-4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='figure_4_4.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On the other hand, if the learning rate is too high, you might jump across the\n",
    "valley and end up on the other side, possibly even higher up than you were\n",
    "before. This might make the algorithm diverge, with larger and larger values,\n",
    "failing to find a good solution (see Figure 4-5). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='figure_4_5.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por último, no todas las funciones de costos se ven como buenos cuencos normales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Puede haber agujeros, crestas, mesetas y todo tipo de terrenos irregulares, haciendo dificil la convergencia al mínimo global. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La figura 4-6 muestra los dos desafíos principales del Gradiente descendente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si la inicialización aleatoria inicia el algoritmo de la izquierda, entonces\n",
    "convergen a un mínimo local, que no es tan bueno como el mínimo global. Si se\n",
    "comienza a la derecha, luego tomará mucho tiempo cruzar la meseta. Y si\n",
    "si se detiene demasiado pronto, nunca alcanzará el mínimo global."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='figure_4_6.jpg'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fortunately, the MSE cost function for a Linear Regression model happens to be a convex function, which means that if you pick any two points on the curve, the line segment joining them never crosses the curve.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This implies that there are no local minima, just one global minimum. It is also a continuous function with a slope that never changes abruptly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These two facts have a great consequence:  \n",
    "Gradient Descent is guaranteed to approach arbitrarily close the global minimum  \n",
    "(if you wait long enough and if the learning rate is not too high)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In fact, the cost function has the shape of a bowl, but it can be an elongated bowl if the features have very different scales.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Figure 4-7 shows Gradient Descent on a training set where features 1 and 2 have the same scale (on the left), and on a training set where feature 1 has much smaller values than feature 2 (on the right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='figure_4_7.jpg'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Como puedes ver, a la izquierda el algoritmo de Descenso de Gradiente va directo hacia el mínimo, alcanzándolo rápidamente, mientras que a la derecha primero va en una dirección casi ortogonal a la dirección del mínimo global, y termina con una marcha larga. por un valle casi plano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It will eventually reach the minimum, but it will take a long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Al usar Gradient Descent, debe asegurarse de que todas las funciones tengan una escala similar (p. ej., usando la clase StandardScaler de Scikit-Learn) o, de lo contrario, tardará mucho más en converger.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este diagrama también ilustra el hecho de que entrenar un modelo significa buscar una combinación de parámetros del modelo que minimice una función de costo (sobre el conjunto de entrenamiento)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es una búsqueda en el espacio de parámetros del modelo: cuantos más parámetros tiene un modelo, más dimensiones tiene este espacio y más difícil es la búsqueda: buscar una aguja en un pajar de 300 dimensiones es mucho más complicado que en 3 dimensiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Afortunadamente, dado que la función de costo es convexa en el caso de la regresión lineal, la aguja simplemente está en el fondo del recipiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradiente descendente incremental "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para implementar Gradient Descent, debe calcular el gradiente de la función de costo con respecto a cada parámetro del modelo $\\theta_{j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En otras palabras, debe calcular cuánto cambiará la función de costo si cambia $\\theta_{j}$ solo un poco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A esto se le llama derivada parcial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es como preguntar \"¿Cuál es la pendiente de la montaña bajo mis pies si miro hacia el este?\" y luego hacer la misma pregunta mirando al norte (y así sucesivamente para todas las demás dimensiones, si puedes imaginar un universo con más de tres dimensiones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La ecuación 4-5 calcula la derivada parcial de la función de costo con respecto al parámetro $\\theta_{j}$, anotó $\\frac{\\partial}{\\partial \\theta_{j}} MSE(\\mathbf{\\theta})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ecuación 4-5. Derivadas parciales de la función de costo  \n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta_{j}} MSE(\\theta) = \\frac{2}{m} \\sum_{i = 1}^{m} (\\theta^{T}x^{(i)} - y^{(i)})x_{j}^{(i)}     $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En lugar de calcular estas derivadas parciales individualmente, puede usar la Ecuación 4-6 para calcularlas todas de una sola vez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The gradient vector, noted $\\nabla MSE(\\theta)$, contains all the partial derivatives of the cost function (one for each model parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 4-6. Gradient vector of the cost function  \n",
    "\n",
    "$$ \\nabla_{theta}MSE(\\theta) = \\begin{pmatrix} \\frac{\\partial}{\\partial \\theta_{0}}MSE(\\theta) \\\\ \\frac{\\partial}{\\partial \\theta_{1}}MSE(\\theta) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial \\theta_{n}}MSE(\\theta)   \\end{pmatrix} = \\frac{2}{m} X^{T}(X\\theta - y) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tenga en cuenta que esta fórmula implica cálculos sobre el conjunto de entrenamiento completo `X`, en cada paso de descenso de gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es por eso que el algoritmo se llama Batch Gradient Descent: utiliza el lote completo de datos de entrenamiento en cada paso (en realidad, Full Gradient Descent probablemente sería un mejor nombre)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As a result it is terribly slow on very large training sets (but we will see much faster\n",
    "Gradient Descent algorithms shortly). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, Gradient Descent scales well with the number of features; training a Linear Regression model when there are hundreds of thousands of features is much faster using Gradient Descent than using the Normal Equation or SVD decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once you have the gradient vector, which points uphill, just go in the opposite\n",
    "direction to go downhill. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This means subtracting $\\nabla MSE(\\theta)$ from $\\thate$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is where the learning rate $\\eta$ comes into play: multiply the gradient vector by $\\eta$ to\n",
    "determine the size of the downhill step (Equation 4-7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 4-7. Gradient Descent step\n",
    "\n",
    "$$ \\theta^{\\text{}} = \\theta - \\eta \\nabla_{\\theta} MSE(\\theta) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s look at a quick implementation of this algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "eta = 0.1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That wasn’t too hard! Let’s look at the resulting theta:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hey, that’s exactly what the Normal Equation found! Gradient Descent worked perfectly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But what if you had used a different learning rate eta? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Figure 4-8 shows the first 10 steps of Gradient Descent using three different learning rates (the dashed line represents the starting point)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "figura 4-8\n",
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On the left, the learning rate is too low: the algorithm will eventually reach the\n",
    "solution, but it will take a long time. In the middle, the learning rate looks pretty\n",
    "good: in just a few iterations, it has already converged to the solution. On the right, the learning rate is too high: the algorithm diverges, jumping all over the\n",
    "place and actually getting further and further away from the solution at every\n",
    "step.\n",
    "To find a good learning rate, you can use grid search (see Chapter 2). However,\n",
    "you may want to limit the number of iterations so that grid search can eliminate\n",
    "models that take too long to converge.\n",
    "You may wonder how to set the number of iterations. If it is too low, you will\n",
    "still be far away from the optimal solution when the algorithm stops; but if it is\n",
    "too high, you will waste time while the model parameters do not change\n",
    "anymore. A simple solution is to set a very large number of iterations but to\n",
    "interrupt the algorithm when the gradient vector becomes tiny—that is, when its\n",
    "norm becomes smaller than a tiny number ϵ (called the tolerance)—because this\n",
    "happens when Gradient Descent has (almost) reached the minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CONVERGENCE RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the cost function is convex and its slope does not change abruptly (as\n",
    "is the case for the MSE cost function), Batch Gradient Descent with a fixed\n",
    "learning rate will eventually converge to the optimal solution, but you may\n",
    "have to wait a while: it can take O(1/ϵ) iterations to reach the optimum\n",
    "within a range of ϵ, depending on the shape of the cost function. If you\n",
    "divide the tolerance by 10 to have a more precise solution, then the\n",
    "algorithm may have to run about 10 times longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The main problem with Batch Gradient Descent is the fact that it uses the whole\n",
    "training set to compute the gradients at every step, which makes it very slow\n",
    "when the training set is large. At the opposite extreme, Stochastic Gradient\n",
    "Descent picks a random instance in the training set at every step and computes\n",
    "the gradients based only on that single instance. Obviously, working on a single\n",
    "instance at a time makes the algorithm much faster because it has very little data\n",
    "to manipulate at every iteration. It also makes it possible to train on huge\n",
    "training sets, since only one instance needs to be in memory at each iteration\n",
    "(Stochastic GD can be implemented as an out-of-core algorithm; see Chapter 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, due to its stochastic (i.e., random) nature, this algorithm is\n",
    "much less regular than Batch Gradient Descent: instead of gently decreasing\n",
    "until it reaches the minimum, the cost function will bounce up and down,\n",
    "decreasing only on average. Over time it will end up very close to the minimum,\n",
    "but once it gets there it will continue to bounce around, never settling down (see\n",
    "Figure 4-9). So once the algorithm stops, the final parameter values are good,\n",
    "but not optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "es"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "es",
   "useGoogleTranslate": true
  },
  "rise": {
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
