{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60c0a4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/introducci-n-al-Machine-Learning/blob/main/classes/class_1/class_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f50600",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 5. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541097ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una máquina de soporte vectorial (SVM) es un modelo de aprendizaje automático potente y versátil, capaz de realizar clasificación lineal o no lineal, regresión e incluso detección de valores atípicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4a4c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es uno de los modelos más populares en Machine Learning, y cualquier persona interesada en Machine Learning debería tenerlo en su caja de herramientas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf76a4bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las SVM son particularmente adecuadas para la clasificación de conjuntos de datos complejos de tamaño pequeño o mediano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64899e27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este capítulo explicará los conceptos básicos de las SVM, cómo usarlas y cómo funcionan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4885bb1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792f3f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La idea fundamental detrás de las SVM se explica mejor con algunas imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f0c382",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La Figura 5-1 muestra parte del conjunto de datos del iris que se presentó al final del Capítulo 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822b7b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las dos clases se pueden separar fácilmente con una línea recta (son linealmente separables). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1babb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El gráfico de la izquierda muestra los límites de decisión de tres posibles clasificadores lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c15bbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_sv/figure_5_1.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293491d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El modelo cuyo límite de decisión está representado por la línea discontinua es tan malo que ni siquiera separa las clases correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa64669f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los otros dos modelos funcionan perfectamente en este conjunto de entrenamiento, pero sus límites de decisión se acercan tanto a las instancias que estos modelos probablemente no funcionarán tan bien en nuevas instancias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7486e518",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por el contrario, la línea continua en el gráfico de la derecha representa el límite de decisión de un clasificador SVM; esta línea no solo separa las dos clases, sino que también se mantiene lo más alejada posible de las instancias de entrenamiento más cercanas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705aab34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Puede pensar en un clasificador SVM como si se ajustara a la calle más ancha posible (representada por las líneas discontinuas paralelas) entre las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267241c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto se llama **clasificación de gran margen**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b83c04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tenga en cuenta que agregar más instancias de capacitación \"fuera de la calle\" no afectará en absoluto el límite de decisión: está completamente determinado (o \"respaldado\") por las instancias ubicadas en el borde de la calle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3968da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estos casos se denominan vectores de soporte (están encerrados en un círculo en la figura 5-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc272dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_sv/figure_5_2.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a4f8cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ADVERTENCIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a583d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las SVM son sensibles a las escalas de características, como puede ver en la Figura 5-2: en el gráfico de la izquierda, la escala vertical es mucho más grande que la escala horizontal, por lo que la calle más ancha posible está cerca de la horizontal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc18d4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Después de escalar características (p. ej., usando StandardScaler de Scikit-Learn), el límite de decisión en el gráfico de la derecha se ve mucho mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e76986",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Soft Margin Classification\n",
    "Clasificación de margen blando"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3916dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si imponemos estrictamente que todas las instancias deben estar fuera de la calle y del lado derecho, esto se denomina *clasificación de margen duro*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324f902",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hay dos problemas principales con la clasificación de margen duro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0425e9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Primero, solo funciona si los datos son linealmente separables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce511324",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En segundo lugar, es sensible a los valores atípicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b73e35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La Figura 5-3 muestra el conjunto de datos del iris con solo un valor atípico adicional: a la izquierda, es imposible encontrar un margen duro; a la derecha, el límite de decisión termina siendo muy diferente del que vimos en la Figura 5-1 sin el valor atípico, y probablemente tampoco se generalice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4146fdd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_sv/figure_5_3.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d98cb6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para evitar estos problemas, utilice un modelo más flexible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efabdde",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El objetivo es encontrar un buen equilibrio entre mantener la calle lo más grande posible y limitar las violaciones de los márgenes (es decir, instancias que terminan en el medio de la calle o incluso en el lado equivocado)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6446fb20",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto se llama *clasificación de margen suave*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8e352",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Al crear un modelo SVM con Scikit-Learn, podemos especificar una serie de hiperparámetros. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a46e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "C es uno de esos hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d89b3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si lo establecemos en un valor bajo, terminamos con el modelo a la izquierda de la Figura 5-4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42824d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Con un valor alto, obtenemos el modelo de la derecha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8196e3d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las violaciones de los márgenes son malas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32abef31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por lo general, es mejor tener algunos de ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2cdef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_sv/figure_5_4.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ffd52d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sin embargo, en este caso, el modelo de la izquierda tiene muchas violaciones de márgenes, pero probablemente generalice mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb05fe04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sugerencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d2cd1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si su modelo SVM está sobreajustado, puede intentar regularizarlo reduciendo C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a09e92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El siguiente código de Scikit-Learn carga el conjunto de datos de iris, escala las características y luego entrena un modelo SVM lineal (usando la clase LinearSVC con C=1 y la función de pérdida de bisagra, descrita en breve) para detectar flores de Iris virginica:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba04688c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_svc', LinearSVC(C=1, loss='hinge'))])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris virginica\n",
    "svm_clf = Pipeline([\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "])\n",
    "svm_clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341fd646",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El modelo resultante se representa a la izquierda en la Figura 5-4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784a680",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Luego, como de costumbre, puede usar el modelo para hacer predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c0de8e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a018a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f1692",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A diferencia de los clasificadores de regresión logística, los clasificadores SVM no generan probabilidades para cada clase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424b749",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En lugar de usar la clase `LinearSVC`, podríamos usar la clase SVC con un núcleo lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733bb98",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Al crear el modelo SVC, escribiríamos `SVC(kernel=\"linear\", C=1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b797a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or we could use the SGDClassifier class, with `SGDClassifier(loss=\"hinge\", alpha=1/(m*C))`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd48092",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This applies regular Stochastic Gradient Descent (see Chapter 4) to train a linear SVM classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df449dea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It does not converge as fast as the LinearSVC class, but it can be useful to handle online classification tasks or huge datasets that do not fit in memory (out-of-core training)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d1dca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b144fb29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The LinearSVC class regularizes the bias term, so you should center the training set first by subtracting its mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0396a56e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is automatic if you scale the data using the StandardScaler. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6dac4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Also make sure you set the loss hyperparameter to \"hinge\", as it is not the default value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd3a168",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, for better performance, you should set the dual hyperparameter to False, unless there are more features than training instances (we will discuss duality later in the chapter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f233e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonlinear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6c149",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Although linear SVM classifiers are efficient and work surprisingly well in many cases, many datasets are not even close to being linearly separable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa157ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One approach to handling nonlinear datasets is to add more features, such as polynomial features (as you did in Chapter 4); in some cases this can result in a linearly separable dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765e6300",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider the left plot in Figure 5-5: it represents a simple dataset with just one feature, $x_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b972b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This dataset is not linearly separable, as you can see. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975c2bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But if you add a second feature $x_{2} = (x_{1})^{2}$ , the resulting 2D dataset is perfectly linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeff6e97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_sv/figure_5_5.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524bd8b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para implementar esta idea usando Scikit-Learn, cree un Pipeline que contenga un transformador PolynomialFeatures (discutido en \"Regresión polinomial\"), seguido de un StandardScaler y un `LinearSVC`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8344f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s test this on the moons dataset: this is a toy dataset for binary classification in which the data points are shaped as two interleaving half circles (see Figure 5-6). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05de6edb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can generate this dataset using the `make_moons()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b61858",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "polynomial_svm_clf = Pipeline([\n",
    "(\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "])\n",
    "polynomial_svm_clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109b3ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_sv/figure_5_6.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7649e24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021be62c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Agregar funciones polinómicas es simple de implementar y puede funcionar muy bien con todo tipo de algoritmos de aprendizaje automático (no solo SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207e2797",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That said, at a low polynomial degree, this method cannot deal with very complex datasets, and with a high polynomial degree it creates a huge number of features, making the model too slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b3795",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fortunately, when using SVMs you can apply an almost miraculous mathematical technique called the kernel trick (explained in a moment). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38391031",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The kernel trick makes it possible to get the same result as if you had added many polynomial features, even with very high-degree polynomials, without actually having to add them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaedf1b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So there is no combinatorial explosion of the number of features because you don’t actually add any\n",
    "features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd81ed5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This trick is implemented by the SVC class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743b08d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s test it on the moons dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76fc54c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "])\n",
    "poly_kernel_svm_clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce88f36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This code trains an SVM classifier using a third-degree polynomial kernel. It is represented\n",
    "on the left in Figure 5-7. On the right is another SVM classifier using a 10th-degree polynomial kernel. Obviously, if your model is overfitting, you might want to reduce the\n",
    "polynomial degree. Conversely, if it is underfitting, you can try increasing it. The\n",
    "hyperparameter coef0 controls how much the model is influenced by high-degree\n",
    "polynomials versus low-degree polynomials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef82a39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cae999d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946394a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e96c9a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ad0107",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9807dc1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125a99e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
