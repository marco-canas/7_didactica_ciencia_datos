{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a680f5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8539469",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 5. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95acab73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una máquina de soporte vectorial (SVM) es un modelo de aprendizaje automático potente y versátil, capaz de realizar clasificación lineal o no lineal, regresión e incluso detección de valores atípicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aad6d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es uno de los modelos más populares en Machine Learning, y cualquier persona interesada en Machine Learning debería tenerlo en su caja de herramientas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf4c88a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las SVM son particularmente adecuadas para la clasificación de conjuntos de datos complejos de tamaño pequeño o mediano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aa05cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este capítulo explicará los conceptos básicos de las SVM, cómo usarlas y cómo funcionan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b234b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4730ffc9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The fundamental idea behind SVMs is best explained with some pictures. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece7e4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Figure 5-1 shows part of the iris dataset that was introduced at the end of Chapter 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43c3cef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The two classes can clearly be separated easily with a straight line (they are linearly separable). The left plot shows the decision boundaries of three possible linear classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78439efb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The model whose decision boundary is represented by the dashed line is so bad that it does not even separate the classes properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7ede43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The other two models work perfectly on this training set, but their decision boundaries come so close to the instances that these models will probably not perform as well on new instances. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e508a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In contrast, the solid line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the two classes but also stays as far away from the closest training instances as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada59af0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can think of an SVM classifier as fitting the widest possible street (represented by the parallel\n",
    "dashed lines) between the classes. This is called large margin classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93714302",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_sv/figure_5_1.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e348d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully determined (or “supported”) by the instances located on the edge of the street. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223fedf9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These instances are called the support vectors (they are circled in Figure 5-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2640345",
   "metadata": {},
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/chap_5_sv/figure_5_2.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c008a13a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ac8d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "SVMs are sensitive to the feature scales, as you can see in Figure 5-2: in the left plot, the vertical scale is much larger than the horizontal scale, so the widest possible street is close to horizontal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff1b585",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After feature scaling (e.g., using Scikit-Learn’s StandardScaler), the decision boundary in the right plot looks much better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc422aa2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Soft Margin Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a42c22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we strictly impose that all instances must be off the street and on the right side, this is\n",
    "called hard margin classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2ba9dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are two main issues with hard margin classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a5e8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, it only works if the data is linearly separable. Second, it is sensitive to outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def5a7ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Figure 5-3 shows the iris dataset with just one additional outlier: on the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the one we saw in Figure 5-1 without the outlier, and it will probably not generalize as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff6302d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e539ba58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To avoid these issues, use a more flexible model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b392b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The objective is to find a good balance between keeping the street as large as possible and limiting the margin violations (i.e., instances that end up in the middle of the street or even on the wrong side). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9662da53",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is called soft margin classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b206c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When creating an SVM model using Scikit-Learn, we can specify a number of\n",
    "hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8715ba3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "C is one of those hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0653d5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we set it to a low value, then we end up with the model on the left of Figure 5-4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f93ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With a high value, we get the model on the right. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddefd1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Margin violations are bad. It’s usually better to have few of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca891d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, in this case the model on the left has a lot of margin violations but will probably generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f91d35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5844ded0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde5460b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If your SVM model is overfitting, you can try regularizing it by reducing C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8048b2d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following Scikit-Learn code loads the iris dataset, scales the features, and then trains a linear SVM model (using the LinearSVC class with C=1 and the hinge loss function, described shortly) to detect Iris virginica flowers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881567e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_svc', LinearSVC(C=1, loss='hinge'))])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris virginica\n",
    "svm_clf = Pipeline([\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "])\n",
    "svm_clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a64703",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92909e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe1065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
