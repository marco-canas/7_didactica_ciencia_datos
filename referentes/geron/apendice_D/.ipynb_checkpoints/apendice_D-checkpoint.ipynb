{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b24bb7e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/didactica_ciencia_datos/blob/main/referentes/chollet/c1/c1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e2d093",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apéndice D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b12243",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Appendix D. Autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66d3fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este apéndice explica cómo funciona la característica de autodiferenciación (autodiff) de TensorFlow y cómo se compara con otras soluciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ee59a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suponga que define una función $f(x, y) = x^{2}y + y + 2$, y necesita sus derivadas parciales $\\partial f/\\partial x$ y $\\partial f/\\partial y$ , normalmente para realizar el Descenso de gradiente (o algún otro algoritmo de optimización)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6ae0af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sus opciones principales son: \n",
    "\n",
    "* la diferenciación manual, \n",
    "* la aproximación de diferencias finitas, \n",
    "* la diferencia automática en modo directo y \n",
    "* la diferencia automática en modo inverso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a1d79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "TensorFlow implements reverse-mode autodiff, but to understand it, it’s useful to look at the other options first. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b94d5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So let’s go through each of them, starting with manual differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e26add7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Manual Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1305250b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El primer enfoque para calcular derivadas es tomar un lápiz y una hoja de papel y usar su conocimiento de cálculo para derivar la ecuación apropiada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57b4059",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For the function $f(x, y)$ just defined, it is not too hard; you just need to use five rules:  \n",
    "\n",
    "* The derivative of a constant is 0.\n",
    "* The derivative of $\\lambda x$ is $λ$ (where $\\lambda$  is a constant).\n",
    "* The derivative of $x^{\\lambda}$ is $\\lambda x^{\\lambda - 1}$, so the derivative of $x^{2}$ is $2x$.\n",
    "* The derivative of a sum of functions is the sum of these functions’ derivatives.\n",
    "* The derivative of $\\lambda$ times a function is $\\lambda$ times its derivative.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59953139",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From these rules, you can derive Equation D-1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca91d5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation D-1. Partial derivatives of $f(x, y)$\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial x} & = \\frac{\\partial(x^{2}y)}{\\partial x} + \\frac{\\partial y}{ \\partial x} + \\frac{\\partial 2}{\\partial x} = y\\frac{\\partial x^{2}}{\\partial x} + 0 + 0 = 2xy  \\\\\n",
    "\\frac{\\partial f}{\\partial y} & = \n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1586c61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este enfoque puede volverse muy tedioso para funciones más complejas y corre el riesgo de cometer errores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b1a4a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fortunately, there are other options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0437f5e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s look at finite difference approximation now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7421abf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finite Difference Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7b90b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recuerda que la derivada $h′(x_{0})$ de una función $h(x)$ en un punto $x_{0}$ es la pendiente de la función en ese punto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08074aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More precisely, the derivative is defined as the limit of the slope of a straight line going through this point $x$ and another\n",
    "point $x$ on the function, as $x$ gets infinitely close to $x$ (see Equation D-2).\n",
    "\n",
    "\\begin{align*}\n",
    "h'(x_{0}) & = \\lim_{x \\to x_{0}} \\frac{h(x) - h(x_{0})}{x - x_{0}} \\\\\n",
    "& = \\lim_{\\epsilon \\to 0} \\frac{h(x_{0} + \\epsilon) - h(x_{0})}{\\epsilon}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01716df8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, if we wanted to calculate the partial derivative of $f(x, y)$ with regard to $x$ at $x = 3$ and $y = 4$, we could compute $f(3 + ε, 4) – f(3, 4)$ and divide the result by $ε$, using a very small value for $ε$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066799ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This type of numerical approximation of the derivative is called a finite difference approximation, and this specific equation is called Newton’s difference quotient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7d7971",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That’s exactly what the following code does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3981fcce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return x**2*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91e5c68d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def derivative(f, x, y, x_eps, y_eps):\n",
    "    return (f(x + x_eps, y + y_eps) - f(x, y)) / (x_eps + y_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94deeaae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_dx = derivative(f, 3, 4, 0.00001, 0)\n",
    "df_dy = derivative(f, 3, 4, 0, 0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251bf0b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unfortunately, the result is imprecise (and it gets worse for more complicated functions). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14d677",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The correct results are respectively 24 and 10, but instead we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0457fb5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.000039999805264\n"
     ]
    }
   ],
   "source": [
    "print(df_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7b2b3cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.000000000331966\n"
     ]
    }
   ],
   "source": [
    "print(df_dy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a7064",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that to compute both partial derivatives, we have to call f() at least three times (we called it four times in the preceding code, but it could be\n",
    "optimized). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9c2046",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If there were 1,000 parameters, we would need to call f() at\n",
    "least 1,001 times. When you are dealing with large neural networks, this\n",
    "makes finite difference approximation way too inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b467f25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, this method is so simple to implement that it is a great tool to check that the other methods are implemented correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0344a8d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, if it\n",
    "disagrees with your manually derived function, then your function probably\n",
    "contains a mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c930b27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So far, we have considered two ways to compute gradients: using manual differentiation and using finite difference approximation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1b933",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unfortunately, both were fatally flawed to train a large-scale neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43149c79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So let’s turn to autodiff, starting with forward mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd3dc53",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forward-Mode Autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c18e526",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Figure D-1 shows how forward-mode autodiff works on an even simpler function, g(x, y) = 5 + xy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e54338c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The graph for that function is represented on the left. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e242d5c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After forward-mode autodiff, we get the graph on the right, which represents the partial derivative $∂g∂x = 0 + (0 × x + y × 1) = y$ (we could similarly obtain the partial derivative with regard to y)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccbf6e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "figura 1_D\n",
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6e55a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8225ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7cc9a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c84ef9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad14a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aefe2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb095c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a87860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46088f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "rise": {
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
