{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbdb8f9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_11/c_11.ipynb#scrollTo=3496adae\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b2ab5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Capítulo 11 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496adae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 11. Entrenamiento de redes neuronales profundas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c3c6dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En el Capítulo 10 presentamos las redes neuronales artificiales y entrenamos nuestras primeras redes neuronales profundas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e434a99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pero eran redes poco profundas, con solo unas pocas capas ocultas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416b6f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¿Qué sucede si necesita abordar un problema complejo, como detectar cientos de tipos de objetos en imágenes de alta resolución?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341ec631",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es posible que deba entrenar un DNN mucho más profundo, tal vez con 10 capas o muchas más, cada una con cientos de neuronas, unidas por cientos de miles de conexiones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2cc28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entrenar un DNN profundo no es un paseo por el parque. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300894b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estos son algunos de los problemas con los que podría encontrarse:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f48663",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es posible que se enfrente al complicado problema de los gradientes que se desvanecen o al problema relacionado con los gradientes explosivos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc4d4fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto es cuando los gradientes se vuelven más y más pequeños, o más y más grandes, cuando fluyen hacia atrás a través de la DNN durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04538455",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ambos problemas hacen que las capas inferiores sean muy difíciles de entrenar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a9cacc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es posible que no tenga suficientes datos de entrenamiento para una red tan grande o que sea demasiado costoso etiquetarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a892343e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El entrenamiento puede ser extremadamente lento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d514f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Un modelo con millones de parámetros correría el grave riesgo de sobreajustar el conjunto de entrenamiento, especialmente si no hay suficientes instancias de entrenamiento o si son demasiado ruidosas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce1580",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En este capítulo repasaremos cada uno de estos problemas y presentaremos técnicas para resolverlos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c14237b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comenzaremos explorando los problemas de gradientes que se desvanecen y explotan y algunas de sus soluciones más populares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8186f96b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A continuación, veremos el aprendizaje por transferencia y el entrenamiento previo no supervisado, que pueden ayudarlo a abordar tareas complejas incluso cuando tiene pocos datos etiquetados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f65d47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Luego discutiremos varios optimizadores que pueden acelerar enormemente el entrenamiento de modelos grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d1e3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finalmente, veremos algunas técnicas de regularización populares para redes neuronales grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b6603",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Con estas herramientas podrás entrenar redes muy profundas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b9f81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¡Bienvenido al Aprendizaje Profundo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8822fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Los problemas de gradientes de desaparición/explosión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102345de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Como discutimos en el Capítulo 10, el algoritmo de retropropagación funciona yendo de la capa de salida a la capa de entrada, propagando el gradiente de error a lo largo del camino."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046795c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una vez que el algoritmo ha calculado el gradiente de la función de costo con respecto a cada parámetro en la red, usa estos gradientes para actualizar cada parámetro con un paso de descenso de gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb19a2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfbfa8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As a result, the Gradient Descent update leaves the lower layers’ connection weights virtually unchanged, and training never converges to a good solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b58a85d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We call this the vanishing gradients problem. In some cases, the opposite can happen: the gradients can grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c390dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is the exploding gradients problem, which surfaces in recurrent neural networks (see Chapter 15). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7eb415",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce15367",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This unfortunate behavior was empirically observed long ago, and it was one of the reasons deep neural networks were mostly abandoned in the early 2000s. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63965725",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It wasn’t clear what caused the gradients to be so unstable when training a DNN, but some light was shed in a 2010 paper by Xavier Glorot and Yoshua Bengio. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c52f41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The authors found a few suspects, including the combination of the popular logistic sigmoid activation function and the\n",
    "weight initialization technique that was most popular at the time (i.e., a normal distribution with a mean of 0 and a standard deviation of 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517547b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In short, they showed that with this activation function and this initialization scheme, the variance of the outputs of each layer is much greater than the variance of its inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054c5e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top\n",
    "layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4dfc6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This saturation is actually made worse by the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent function has a mean of 0 and behaves slightly better than the logistic function in deep networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebcd2a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Looking at the logistic activation function (see Figure 11-1), you can see that when inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56bfda8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thus, when backpropagation kicks in it has virtually no gradient to propagate back through the network; and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e924f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_11/figure_11_1.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9356f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Glorot and He Initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d658c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In their paper, Glorot and Bengio propose a way to significantly alleviate the unstable gradients problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba3d9e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "They point out that we need the signal to flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e7ba95",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We don’t want the signal to die out, nor do we want it to explode and saturate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d15b3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For the signal to flow properly, the authors argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction (please check out the paper if you are interested in the mathematical details). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f678416b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons (these numbers are called the fan-in and fan-out of the layer), but Glorot and Bengio proposed a good compromise that has proven to work very well in practice: the connection weights of each layer must be initialized randomly as described in Equation 11-1, where \n",
    "$fanavg = (fanin + fanout)/2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b140143",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This initialization strategy is called Xavier initialization or Glorot initialization, after the paper’s first author.\n",
    "Equation 11-1. Glorot initialization (when using the logistic activation function)\n",
    "\n",
    "$$ \\text{Normal distribucion with mean 0 and variance } \\ \\sigma^{2} = \\frac{1}{fac_{avg}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0183ee9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15392fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcfea5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f739219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde40ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb76ce20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12819641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42cd550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537fe329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f192d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6645d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11e15a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "rise": {
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
