{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6bcf36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_1/c_2/c_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca9c2c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 15. Secuencias de procesamiento usando RNN y CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cebdbe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El bateador golpea la pelota."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a57f95",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El jardinero inmediatamente comienza a correr, anticipando la trayectoria de la pelota."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b6c18f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lo rastrea, adapta sus movimientos y finalmente lo atrapa (bajo un estruendo de aplausos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f78a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Predecir el futuro es algo que haces todo el tiempo, ya sea que estés terminando la oración de un amigo o anticipando el olor del café en el desayuno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12012690",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this chapter we will discuss recurrent neural networks (RNNs), a class of nets that can predict the future (well, up to a point, of course). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b080ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "They can analyze time series data such as stock prices, and tell you when to buy or sell. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d58365c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In autonomous driving systems, they can anticipate car trajectories and help avoid accidents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044cda62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More generally, they can work on sequences of arbitrary lengths, rather than on fixed-sized inputs like all the nets we have considered so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df626500",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, they can take sentences, documents, or audio samples as input, making them extremely useful for natural language processing applications such as automatic translation or speech-to-text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af009db4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this chapter we will first look at the fundamental concepts underlying RNNs and how to train them using backpropagation through time, then we will use them to forecast a time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fafd83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After that we’ll explore the two main difficulties that RNNs face:  \n",
    "\n",
    "* Unstable gradients (discussed in Chapter 11), which can be alleviated using various techniques, including recurrent dropout    and recurrent layer normalization\n",
    "* A (very) limited short-term memory, which can be extended using LSTM and GRU cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1e6f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "RNNs are not the only types of neural networks capable of handling\n",
    "sequential data: for small sequences, a regular dense network can do the\n",
    "trick; and for very long sequences, such as audio samples or text,\n",
    "convolutional neural networks can actually work quite well too. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b0d11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will discuss both of these possibilities, and we will finish this chapter by implementing a WaveNet: this is a CNN architecture capable of handling sequences of tens of thousands of time steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bcd080",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In Chapter 16, we will continue to explore RNNs and see how to use them for natural language processing, along with more recent architectures based on attention mechanisms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55441d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31084485",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neurons and Layers\n",
    "Up to now we have focused on feedforward neural networks, where the\n",
    "activations flow only in one direction, from the input layer to the output\n",
    "layer (a few exceptions are discussed in Appendix E). A recurrent neural\n",
    "network looks very much like a feedforward neural network, except it also\n",
    "has connections pointing backward. Let’s look at the simplest possible\n",
    "RNN, composed of one neuron receiving inputs, producing an output, and\n",
    "sending that output back to itself, as shown in Figure 15-1 (left). At each\n",
    "time step t (also called a frame), this recurrent neuron receives the inputs\n",
    "x as well as its own output from the previous time step, y . Since there\n",
    "is no previous output at the first time step, it is generally set to 0. We can\n",
    "represent this tiny network against the time axis, as shown in Figure 15-1\n",
    "(right). This is called unrolling the network through time (it’s the same\n",
    "recurrent neuron represented once per time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b068d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e002232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cce46c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdaeff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "rise": {
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
