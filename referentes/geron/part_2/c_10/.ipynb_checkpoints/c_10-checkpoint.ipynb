{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6381fb7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_10/c_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647bc3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Capítulo 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbdc194",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducción a las Redes Artificiales con Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc8194",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los pájaros nos inspiraron a volar, las plantas de bardana inspiraron el velcro y la naturaleza ha inspirado innumerables inventos más."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da998e57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://i.blogs.es/1a9d2d/aves-colibri/1366_2000.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f39b2f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Parece lógico, entonces, observar la arquitectura del cerebro en busca de inspiración sobre cómo construir una máquina inteligente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40450b12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://www.analyticslane.com/storage/2018/05/redneuronal.png.webp'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bfd0bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta es la lógica que desencadenó las redes neuronales artificiales (ANN): una ANN es un modelo de aprendizaje automático inspirado en las redes de neuronas biológicas que se encuentran en nuestros cerebros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694fd04b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sin embargo, aunque los aviones se inspiraron en las aves, no es necesario que aleteen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45f0da9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "De manera similar, las ANN se han vuelto gradualmente bastante diferentes de sus primos biológicos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b25e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Algunos investigadores incluso argumentan que deberíamos abandonar la analogía biológica por completo (por ejemplo, diciendo \"unidades\" en lugar de \"neuronas\"), para no restringir nuestra creatividad a sistemas biológicamente plausibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916f43d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las ANN están en el centro mismo del Aprendizaje Profundo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f966c1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Son versátiles, potentes y escalables, lo que los hace ideales para abordar tareas de aprendizaje automático grandes y muy complejas, como clasificar miles de millones de imágenes (por ejemplo, imágenes de Google), potenciar los servicios de reconocimiento de voz (por ejemplo, Siri de Apple), recomendar los mejores videos para ver a cientos de millones de usuarios todos los días (por ejemplo, YouTube), o aprender a vencer al campeón mundial en el juego de Go (AlphaGo de DeepMind)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7125be0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La primera parte de este capítulo presenta las redes neuronales artificiales, comenzando con un recorrido rápido por las primeras arquitecturas ANN y llegando a los perceptrones multicapa (MLP), que se usan mucho en la actualidad (se explorarán otras arquitecturas en los próximos capítulos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a8deb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En la segunda parte, veremos cómo implementar redes neuronales utilizando la popular API de Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989adde7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta es una API de alto nivel simple y bellamente diseñada para construir, entrenar, evaluar y ejecutar redes neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9857ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pero no se deje engañar por su simplicidad: es lo suficientemente expresivo y flexible como para permitirle construir una amplia variedad de arquitecturas de redes neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c429206",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "De hecho, probablemente sea suficiente para la mayoría de sus casos de uso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a2594",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Y si alguna vez necesita flexibilidad adicional, siempre puede escribir componentes Keras personalizados utilizando su API de nivel inferior, como veremos en el Capítulo 12."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8ecad3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pero primero, ¡retrocedamos en el tiempo para ver cómo surgieron las redes neuronales artificiales!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7f873",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# De las neuronas biológicas a las artificiales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ac33b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Sorprendentemente, las ANN existen desde hace bastante tiempo:   \n",
    "* fueron introducidas por primera vez en 1943 por el neurofisiólogo Warren McCulloch y el matemático Walter Pitts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b25b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En su artículo histórico \"Un cálculo lógico de ideas inmanentes en la actividad nerviosa\", McCulloch y Pitts presentaron un modelo computacional simplificado de cómo las neuronas biológicas podrían trabajar juntas en cerebros de animales para realizar cálculos complejos utilizando la lógica proposicional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9172e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta fue la primera arquitectura de red neuronal artificial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dd68ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Desde entonces se han inventado muchas otras arquitecturas, como veremos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423c86b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los primeros éxitos de las ANN llevaron a la creencia generalizada de que pronto estaríamos conversando con máquinas verdaderamente inteligentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0fc049",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cuando quedó claro en la década de 1960 que esta promesa no se cumpliría (al menos durante bastante tiempo), la financiación se fue a otra parte y las ANN entraron en un largo invierno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f00d1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A principios de la década de 1980, se inventaron nuevas arquitecturas y se desarrollaron mejores técnicas de entrenamiento, lo que provocó un resurgimiento del interés por el conexionismo (el estudio de las redes neuronales)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb6ab3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pero el progreso fue lento, y en la década de 1990 se inventaron otras poderosas técnicas de aprendizaje automático, como las máquinas de soporte vectorial (consulte el Capítulo 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b58b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Estas técnicas parecían ofrecer mejores resultados y bases teóricas más sólidas que las ANN,   \n",
    "* por lo que una vez más se suspendió el estudio de las redes neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ec9dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora estamos presenciando otra ola de interés en las ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb6b6e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¿Se extinguirá esta ola como lo hicieron las anteriores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144e48d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bueno, aquí hay algunas buenas razones para creer que esta vez es diferente y que el renovado interés en las RNA tendrá un impacto mucho más profundo en nuestras vidas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f629b6ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Ahora hay una gran cantidad de datos disponibles para entrenar redes neuronales, \n",
    "* y las ANN con frecuencia superan a otras técnicas de ML en problemas muy grandes y complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9958f85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* El tremendo aumento en el poder de cómputo desde la década de 1990 ahora hace posible entrenar grandes redes neuronales en una cantidad de tiempo razonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0fbc9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Esto se debe en parte a la ley de Moore (la cantidad de componentes en los circuitos integrados se ha duplicado aproximadamente cada 2 años durante los últimos 50 años),  \n",
    "* pero también gracias a la industria del juego, que ha estimulado la producción de potentes tarjetas GPU por millones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55dfec1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Además, las plataformas en la nube han hecho que este poder sea accesible para todos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd252b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Se han mejorado los algoritmos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a3de1b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para ser justos, son solo ligeramente diferentes de los que se usaron en la década de 1990, pero estos ajustes relativamente pequeños han tenido un gran impacto positivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b06609",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Algunas limitaciones teóricas de las ANN han resultado ser benignas en la práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c736462",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por ejemplo, muchas personas pensaron que los algoritmos de entrenamiento ANN estaban condenados porque era probable que se quedaran atascados en los óptimos locales, pero resulta que esto es bastante raro en la práctica (y cuando es el caso, por lo general son bastante parecidos a los óptimos globales)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c19ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las ANN parecen haber entrado en un círculo virtuoso de financiación y progreso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af808279",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los productos sorprendentes basados en ANN aparecen regularmente en los titulares de las noticias, lo que atrae cada vez más atención y financiamiento hacia ellos, lo que resulta en más y más progresos y productos aún más sorprendentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1629102",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neuronas biológicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44430361",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Antes de hablar de las neuronas artificiales, echemos un vistazo rápido a una neurona biológica (representada en la figura 10-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8502fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_10/figure_10_1.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3cbc1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es una célula de aspecto inusual que se encuentra principalmente en el cerebro de los animales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33239ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Está compuesto por  \n",
    "\n",
    "* un cuerpo celular que contiene el núcleo y la mayoría de los componentes complejos de la célula, \n",
    "\n",
    "* muchas extensiones ramificadas llamadas dendritas,   \n",
    "\n",
    "* más una extensión muy larga llamada axón."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b4bee7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La longitud del axón puede ser solo unas pocas veces más larga que el cuerpo celular, o hasta decenas de miles de veces más larga."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80718fd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Cerca de su extremo, el axón se divide en muchas ramas llamadas telodendrias, y\n",
    "* en la punta de estas ramas hay estructuras minúsculas llamadas terminales sinápticas (o simplemente sinapsis), que están conectadas a las dendritas o cuerpos celulares de otras neuronas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b72f41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las neuronas biológicas producen impulsos eléctricos cortos llamados potenciales de acción (AP, o simplemente señales) que viajan a lo largo de los axones y hacen que las sinapsis liberen señales químicas llamadas neurotransmisores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18730164",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cuando una neurona recibe una cantidad suficiente de estos neurotransmisores en unos pocos milisegundos, dispara sus propios impulsos eléctricos (en realidad, depende de los eurotransmisores, ya que algunos de ellos inhiben el disparo de la neurona)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa315749",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Por lo tanto, las neuronas biológicas individuales parecen comportarse de una manera bastante simple, \n",
    "* pero están organizadas en una vasta red de miles de millones, con cada neurona típicamente conectada a miles de otras neuronas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6942101c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una red de neuronas bastante simples puede realizar cálculos muy complejos, al igual que un hormiguero complejo puede surgir de los esfuerzos combinados de hormigas simples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6847f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La arquitectura de las redes neuronales biológicas (BNN, por sus siglas en inglés) sigue siendo objeto de investigación activa, pero se han mapeado algunas partes del cerebro y parece que las neuronas a menudo se organizan en capas consecutivas, especialmente en la corteza cerebral (es decir, la capa externa de su cerebro), como se muestra en la figura 10-2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b07d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_10/figure_10_2.jpg?raw=true'> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db42db83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cálculos lógicos con neuronas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f571b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* McCulloch y Pitts propusieron un modelo muy simple de neurona biológica, que más tarde se conoció como neurona artificial: \n",
    "* tiene una o más entradas binarias (on/off) y una salida binaria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423966cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La neurona artificial activa su salida cuando más de un cierto número de sus entradas están activas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9f1809",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En su artículo, demostraron que, incluso con un modelo tan simplificado, es posible construir una red de neuronas artificiales que calcule cualquier proposición lógica que desee."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44367f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para ver cómo funciona una red de este tipo, construyamos algunas ANN que realicen varios cálculos lógicos (consulte la figura 10-3), suponiendo que una neurona se activa cuando al menos dos de sus entradas están activas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa93e12b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_10/figure_10_3.jpg?raw=true'> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f3ed4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Veamos qué hacen estas redes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f667262",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* La primera red de la izquierda es la función de identidad: \n",
    "* si se activa la neurona A, también se activa la neurona C (ya que recibe dos señales de entrada de la neurona A);  \n",
    "* pero si la neurona A está apagada, entonces la neurona C también lo está."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00425fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* La segunda red realiza un AND lógico: \n",
    "* la neurona C se activa solo cuando las neuronas A y B están activadas \n",
    "* (una sola señal de entrada no es suficiente para activar la neurona C)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972015f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* La tercera red realiza un OR lógico: \n",
    "* la neurona C se activa si se activa la neurona A o la neurona B (o ambas)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cddc43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Finalmente, si suponemos que una conexión de entrada puede inhibir la actividad de la neurona (que es el caso de las neuronas biológicas), entonces la cuarta red calcula una proposición lógica un poco más compleja: la neurona C se activa solo si la neurona A está activa y la neurona B está apagada. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0c555",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si la neurona A está activa todo el tiempo, obtienes un NO lógico:   \n",
    "* la neurona C está activa cuando la neurona B está apagada, y viceversa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3307101",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Puede imaginar cómo se pueden combinar estas redes para calcular expresiones lógicas complejas (vea los ejercicios al final del capítulo para ver un ejemplo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a80e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866f7c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El Perceptron es una de las arquitecturas ANN más simples,   \n",
    "* inventada en 1957 por Frank Rosenblatt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003176c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Se basa en una neurona artificial ligeramente diferente (consulte la figura 10-4)   \n",
    "* denominada unidad lógica de umbral (TLU) o, a veces, \n",
    "* unidad de umbral lineal (LTU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb05a3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Las entradas y salidas son números (en lugar de valores binarios de encendido/apagado), \n",
    "* y cada conexión de entrada está asociada con un peso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a031ffe9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La TLU calcula una suma ponderada de sus entradas \n",
    "\n",
    "$$ (z = w_{1}x_{1} + w_{2}x_{2} + \\cdots + w_{n}x_{n} = x^{T}w), $$ \n",
    "\n",
    "* luego aplica una función de paso a esa suma y genera el resultado: $h_{w}(x) = step(z)$, donde $z = x^{T}w$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c9c684",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_10/figure_10_4.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8f166",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La función escalonada más común utilizada en los perceptrones es la función escalonada de Heaviside (consulte la ecuación 10-1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea44f6a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A veces se usa la **función signo** en su lugar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5487b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 10-1. Common step functions used in Perceptrons (assuming threshold = 0)\n",
    "\n",
    "\n",
    "$$ \\text{heaviside}(z) = \\begin{cases} 0 & \\text{si}\\ z < 0 \\\\ 1 & \\text{si}\\ z \\geq 0 \\end{cases} $$\n",
    "\n",
    "$$ sgn(z) = \\begin{cases} -1 & \\text{si}\\ z < 0 \\\\ 0 & \\text{si}\\ z = 0 \\\\ 1 & \\text{si}\\ z > 0 \\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b528671",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Se puede utilizar una única TLU para la clasificación binaria lineal simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca05f38c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Calcula una combinación lineal de las entradas y, \n",
    "* si el resultado supera un umbral, genera la clase positiva. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15574db2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "De lo contrario, genera la clase negativa   \n",
    "* (al igual que una regresión logística o un clasificador SVM lineal)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f316365",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Podría, por ejemplo, usar una sola TLU para clasificar las flores de iris en función de la longitud y el ancho de los pétalos (también agregando una función de sesgo adicional $x_{0} = 1$, tal como lo hicimos en capítulos anteriores)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b7603",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entrenar una TLU en este caso significa encontrar los valores correctos para $w_{0}, w_{1}$ y $w_{2}$ (el algoritmo de entrenamiento se analiza en breve)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a6fbb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Un Perceptron se compone simplemente de una sola capa de TLU, con cada TLU conectada a todas las entradas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62432e4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cuando todas las neuronas de una capa están conectadas a cada neurona de la capa anterior (es decir, sus neuronas de entrada), la capa se denomina capa totalmente conectada o capa densa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb88f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las entradas del Perceptron se alimentan a neuronas de paso especiales llamadas neuronas de entrada: emiten cualquier entrada que reciban."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e72611",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Todas las neuronas de entrada forman la capa de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ecdd67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Además, generalmente se agrega una función de sesgo adicional $(x_{0} = 1)$:   \n",
    "* generalmente se representa usando un tipo especial de neurona llamada neurona de sesgo, que genera 1 todo el tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd70642",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En la figura 10-5 se representa un perceptrón con dos entradas y tres salidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed156dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_10/figure_10_5.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b97e5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este Perceptron puede clasificar instancias simultáneamente en tres clases binarias diferentes, \n",
    "* lo que lo convierte en un clasificador de salida múltiple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a66075f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gracias a la magia del álgebra lineal, la Ecuación 10-2 hace posible calcular eficientemente las salidas de una capa de neuronas artificiales para varias instancias a la vez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c5e31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ecuación 10-2. Cálculo de las salidas de una capa completamente conectada\n",
    "\n",
    "$$ h_{w,b}(X) = \\phi(XW+ b)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9359dfb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En esta ecuación:\n",
    "\n",
    "* Como siempre, $X$ representa la matriz de características de entrada. Tiene una fila por instancia y una columna por atributo.\n",
    "\n",
    "* La matriz de pesos $W$ contiene todos los pesos de conexión excepto los de la neurona de polarización. Tiene una fila por neurona de entrada y una columna por neurona artificial en la capa.\n",
    "\n",
    "* El vector de sesgo $b$ contiene todos los pesos de conexión entre la neurona de sesgo y las neuronas artificiales. Tiene un término de sesgo por neurona artificial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43afaada",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* La función $\\varphi$ se llama función de activación: cuando las neuronas artificiales son TLU, es una función escalonada (pero discutiremos otras funciones de activación en breve)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b6d58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entonces, ¿cómo se forma un Perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe73d60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El algoritmo de entrenamiento de Perceptron propuesto por Rosenblatt se inspiró en gran medida en la regla de Hebb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c2176",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En su libro de 1949 La Organización del Comportamiento (Wiley), Donald Hebb sugirió que cuando una neurona biológica desencadena a menudo otra neurona, la conexión entre estas dos neuronas se vuelve más fuerte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b15dd59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Siegrid Löwel luego resumió la idea de Hebb en la frase pegadiza, \"Células que disparan juntas, se conectan juntas\"; es decir, el peso de la conexión entre dos neuronas tiende a aumentar cuando disparan simultáneamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7fda75",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta regla más tarde se conoció como la regla de Hebb (o aprendizaje hebbiano)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c4a7f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los perceptrones se entrenan mediante una variante de esta regla que tiene en cuenta el error que comete la red cuando realiza una predicción; la regla de aprendizaje de Perceptron refuerza las conexiones que ayudan a reducir el error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d2998b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Más específicamente, el Perceptron recibe una instancia de entrenamiento a la vez, y para cada instancia hace sus predicciones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f5c16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por cada neurona de salida que produjo una predicción incorrecta, refuerza los pesos de conexión de las entradas que habrían contribuido a la predicción correcta. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9159370",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La regla se muestra en la Ecuación 10-3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2295a445",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ecuación 10-3. Regla de aprendizaje de perceptrón (actualización de peso)\n",
    "\n",
    "$$ w_{i,j}^{(\\text{next step})} = w_{i,j} + \\eta(y_{j} - \\hat{y}_{j})x_{i} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4797c69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En esta ecuación:\n",
    "\n",
    "* $w_{i,j}$ es el peso de conexión entre la $i$ ésima neurona de entrada y la $j$ ésima neurona de salida.\n",
    "* $x_{i}$ es el valor de entrada $i$ de la instancia de entrenamiento actual.\n",
    "* $\\hat{y}_{j}$ es la salida de la neurona de salida $j$ para la instancia de entrenamiento actual.\n",
    "* $y_{j}$ es la salida objetivo de la neurona de salida $j$ para la instancia de entrenamiento actual.\n",
    "* $\\eta$ es la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c36ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El límite de decisión de cada neurona de salida es lineal, por lo que los perceptrones son incapaces de aprender patrones complejos (al igual que los clasificadores de regresión logística)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343e1fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sin embargo, si las instancias de entrenamiento son linealmente separables, Rosenblatt demostró que este algoritmo convergería en una solución."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4a8ea0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto se llama el teorema de convergencia del perceptrón."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f2f238",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scikit-Learn proporciona una clase Perceptron que implementa una red de una sola TLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ae0bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Se puede usar más o menos como cabría esperar, por ejemplo, en el conjunto de datos del iris (presentado en el Capítulo 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84100ff3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int64) # Iris setosa?\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deb6f8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es posible que haya notado que el algoritmo de aprendizaje de Perceptron se parece mucho al descenso de gradiente estocástico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3312376",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "De hecho, la clase Perceptron de Scikit-Learn es equivalente a usar un SGDClassifier con los siguientes hiperparámetros: `loss=\"perceptron\", learning_rate=\"constant\", eta0=1` (la tasa de aprendizaje) y `penalty=None` (no regularización)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee442445",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tenga en cuenta que, a diferencia de los clasificadores de regresión logística, los perceptrones no generan una probabilidad de clase; más bien, hacen predicciones basadas en un umbral estricto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82954680",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta es una de las razones para preferir la regresión logística a los perceptrones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24ec0c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En su monografía Perceptrons de 1969, Marvin Minsky y Seymour Papert destacaron una serie de debilidades graves de los perceptrones, en particular, el hecho de que son incapaces de resolver algunos problemas triviales (por ejemplo, el problema de clasificación Exclusive OR (XOR); consulte el lado izquierdo de la Figura 10-6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721f7d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is true of any other linear classification model (such as Logistic Regression classifiers), but researchers had expected much more from Perceptrons, and some were so disappointed that they dropped neural networks altogether in favor of higher-level problems such as logic, problem solving, and search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6362ac11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It turns out that some of the limitations of Perceptrons can be eliminated by stacking multiple Perceptrons. The resulting ANN is called a Multilayer Perceptron (MLP). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd32ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An MLP can solve the XOR problem, as you can verify by computing the output of the MLP represented on the right side of Figure 10-6: with inputs (0, 0) or (1, 1), the network outputs 0, and with\n",
    "inputs (0, 1) or (1, 0) it outputs 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f8a5de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "All connections have a weight equal to 1, except the four connections where the weight is shown. Try verifying that this network indeed solves the XOR problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a7dc1d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Multilayer Perceptron and Backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c9745",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An MLP is composed of one (passthrough) input layer, one or more layers of TLUs, called hidden layers, and one final layer of TLUs called the output layer (see Figure 10-7). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2949dc8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The layers close to the input layer are usually called the lower layers, and the ones close to the outputs are usually called the upper layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4113d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Every layer except the output layer includes a bias neuron and is fully connected to the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d82b879",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73aef44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aad2d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b6acf7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9895079",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbeedee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "rise": {
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
