{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6381fb7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/edo/blob/main/clases/class_6/class_6_ODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647bc3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Capítulo 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbdc194",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducción a las Redes Artificiales con Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc8194",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los pájaros nos inspiraron a volar, las plantas de bardana inspiraron el velcro y la naturaleza ha inspirado innumerables inventos más."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f39b2f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Parece lógico, entonces, observar la arquitectura del cerebro en busca de inspiración sobre cómo construir una máquina inteligente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bfd0bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta es la lógica que desencadenó las redes neuronales artificiales (ANN): una ANN es un modelo de aprendizaje automático inspirado en las redes de neuronas biológicas que se encuentran en nuestros cerebros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694fd04b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sin embargo, aunque los aviones se inspiraron en las aves, no es necesario que aleteen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45f0da9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "De manera similar, las ANN se han vuelto gradualmente bastante diferentes de sus primos biológicos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b25e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Algunos investigadores incluso argumentan que deberíamos abandonar la analogía biológica por completo (por ejemplo, diciendo \"unidades\" en lugar de \"neuronas\"), para no restringir nuestra creatividad a sistemas biológicamente plausibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916f43d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las ANN están en el centro mismo del Aprendizaje Profundo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f966c1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Son versátiles, potentes y escalables, lo que los hace ideales para abordar tareas de aprendizaje automático grandes y muy complejas, como clasificar miles de millones de imágenes (por ejemplo, imágenes de Google), potenciar los servicios de reconocimiento de voz (por ejemplo, Siri de Apple), recomendar los mejores videos para ver a cientos de millones de usuarios todos los días (por ejemplo, YouTube), o aprender a vencer al campeón mundial en el juego de Go (AlphaGo de DeepMind)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7125be0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La primera parte de este capítulo presenta las redes neuronales artificiales, comenzando con un recorrido rápido por las primeras arquitecturas ANN y llegando a los perceptrones multicapa (MLP), que se usan mucho en la actualidad (se explorarán otras arquitecturas en los próximos capítulos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a8deb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En la segunda parte, veremos cómo implementar redes neuronales utilizando la popular API de Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989adde7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta es una API de alto nivel simple y bellamente diseñada para construir, entrenar, evaluar y ejecutar redes neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9857ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pero no se deje engañar por su simplicidad: es lo suficientemente expresivo y flexible como para permitirle construir una amplia variedad de arquitecturas de redes neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c429206",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "De hecho, probablemente sea suficiente para la mayoría de sus casos de uso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a2594",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Y si alguna vez necesita flexibilidad adicional, siempre puede escribir componentes Keras personalizados utilizando su API de nivel inferior, como veremos en el Capítulo 12."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8ecad3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pero primero, ¡retrocedamos en el tiempo para ver cómo surgieron las redes neuronales artificiales!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7f873",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# De las neuronas biológicas a las artificiales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ac33b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sorprendentemente, las ANN existen desde hace bastante tiempo: fueron introducidas por primera vez en 1943 por el neurofisiólogo Warren McCulloch y el matemático Walter Pitts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b25b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En su artículo histórico \"Un cálculo lógico de ideas inmanentes en la actividad nerviosa\", McCulloch y Pitts presentaron un modelo computacional simplificado de cómo las neuronas biológicas podrían trabajar juntas en cerebros de animales para realizar cálculos complejos utilizando la lógica proposicional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9172e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta fue la primera arquitectura de red neuronal artificial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dd68ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Desde entonces se han inventado muchas otras arquitecturas, como veremos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423c86b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los primeros éxitos de las ANN llevaron a la creencia generalizada de que pronto estaríamos conversando con máquinas verdaderamente inteligentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0fc049",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cuando quedó claro en la década de 1960 que esta promesa no se cumpliría (al menos durante bastante tiempo), la financiación se fue a otra parte y las ANN entraron en un largo invierno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f00d1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A principios de la década de 1980, se inventaron nuevas arquitecturas y se desarrollaron mejores técnicas de entrenamiento, lo que provocó un resurgimiento del interés por el conexionismo (el estudio de las redes neuronales)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb6ab3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pero el progreso fue lento, y en la década de 1990 se inventaron otras poderosas técnicas de aprendizaje automático, como las máquinas de soporte vectorial (consulte el Capítulo 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b58b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estas técnicas parecían ofrecer mejores resultados y bases teóricas más sólidas que las ANN, por lo que una vez más se suspendió el estudio de las redes neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ec9dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora estamos presenciando otra ola de interés en las ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb6b6e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¿Se extinguirá esta ola como lo hicieron las anteriores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144e48d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bueno, aquí hay algunas buenas razones para creer que esta vez es diferente y que el renovado interés en las RNA tendrá un impacto mucho más profundo en nuestras vidas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f629b6ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Ahora hay una gran cantidad de datos disponibles para entrenar redes neuronales, y las ANN con frecuencia superan a otras técnicas de ML en problemas muy grandes y complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9958f85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* El tremendo aumento en el poder de cómputo desde la década de 1990 ahora hace posible entrenar grandes redes neuronales en una cantidad de tiempo razonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0fbc9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Esto se debe en parte a la ley de Moore (la cantidad de componentes en los circuitos integrados se ha duplicado aproximadamente cada 2 años durante los últimos 50 años), pero también gracias a la industria del juego, que ha estimulado la producción de potentes tarjetas GPU por millones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55dfec1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Además, las plataformas en la nube han hecho que este poder sea accesible para todos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd252b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Se han mejorado los algoritmos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a3de1b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para ser justos, son solo ligeramente diferentes de los que se usaron en la década de 1990, pero estos ajustes relativamente pequeños han tenido un gran impacto positivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b06609",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Algunas limitaciones teóricas de las ANN han resultado ser benignas en la práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c736462",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por ejemplo, muchas personas pensaron que los algoritmos de entrenamiento ANN estaban condenados porque era probable que se quedaran atascados en los óptimos locales, pero resulta que esto es bastante raro en la práctica (y cuando es el caso, por lo general son bastante parecidos a los globales). óptimo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c19ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las ANN parecen haber entrado en un círculo virtuoso de financiación y progreso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af808279",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los productos sorprendentes basados en ANN aparecen regularmente en los titulares de las noticias, lo que atrae cada vez más atención y financiamiento hacia ellos, lo que resulta en más y más progresos y productos aún más sorprendentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1629102",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neuronas biológicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44430361",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Antes de hablar de las neuronas artificiales, echemos un vistazo rápido a una neurona biológica (representada en la figura 10-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3cbc1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es una célula de aspecto inusual que se encuentra principalmente en el cerebro de los animales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33239ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Está compuesto por un cuerpo celular que contiene el núcleo y la mayoría de los componentes complejos de la célula, muchas extensiones ramificadas llamadas dendritas, más una extensión muy larga llamada axón."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b4bee7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La longitud del axón puede ser solo unas pocas veces más larga que el cuerpo celular, o hasta decenas de miles de veces más larga."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80718fd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cerca de su extremo, el axón se divide en muchas ramas llamadas telodendrias, y en la punta de estas ramas hay estructuras minúsculas llamadas terminales sinápticas (o simplemente sinapsis), que están conectadas a las dendritas o cuerpos celulares de otras neuronas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8502fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_10/figure_10_1.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b72f41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las neuronas biológicas producen impulsos eléctricos cortos llamados potenciales de acción (AP, o simplemente señales) que viajan a lo largo de los axones y hacen que las sinapsis liberen señales químicas llamadas neurotransmisores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18730164",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cuando una neurona recibe una cantidad suficiente de estos neurotransmisores en unos pocos milisegundos, dispara sus propios impulsos eléctricos (en realidad, depende de los eurotransmisores, ya que algunos de ellos inhiben el disparo de la neurona)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa315749",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por lo tanto, las neuronas biológicas individuales parecen comportarse de una manera bastante simple, pero están organizadas en una vasta red de miles de millones, con cada neurona típicamente conectada a miles de otras neuronas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6942101c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una red de neuronas bastante simples puede realizar cálculos muy complejos, al igual que un hormiguero complejo puede surgir de los esfuerzos combinados de hormigas simples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6847f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La arquitectura de las redes neuronales biológicas (BNN, por sus siglas en inglés) sigue siendo objeto de investigación activa, pero se han mapeado algunas partes del cerebro y parece que las neuronas a menudo se organizan en capas consecutivas, especialmente en la corteza cerebral (es decir, la capa externa de su cerebro), como se muestra en la figura 10-2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b07d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_10/figure_10_2.jpg?raw=true'> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db42db83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cálculos lógicos con neuronas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f571b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "McCulloch y Pitts propusieron un modelo muy simple de neurona biológica, que más tarde se conoció como neurona artificial: tiene una o más entradas binarias (on/off) y una salida binaria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423966cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La neurona artificial activa su salida cuando más de un cierto número de sus entradas están activas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9f1809",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En su artículo, demostraron que, incluso con un modelo tan simplificado, es posible construir una red de neuronas artificiales que calcule cualquier proposición lógica que desee."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44367f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para ver cómo funciona una red de este tipo, construyamos algunas ANN que realicen varios cálculos lógicos (consulte la figura 10-3), suponiendo que una neurona se activa cuando al menos dos de sus entradas están activas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa93e12b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_10/figure_10_3.jpg?raw=true'> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f3ed4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Veamos qué hacen estas redes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f667262",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* La primera red de la izquierda es la función de identidad: si se activa la neurona A, también se activa la neurona C (ya que recibe dos señales de entrada de la neurona A); pero si la neurona A está apagada, entonces la neurona C también lo está."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00425fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* La segunda red realiza un AND lógico: la neurona C se activa solo cuando las neuronas A y B están activadas (una sola señal de entrada no es suficiente para activar la neurona C)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972015f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* La tercera red realiza un OR lógico: la neurona C se activa si se activa la neurona A o la neurona B (o ambas)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cddc43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Finalmente, si suponemos que una conexión de entrada puede inhibir la actividad de la neurona (que es el caso de las neuronas biológicas), entonces la cuarta red calcula una proposición lógica un poco más compleja: la neurona C se activa solo si la neurona A está activa y la neurona B está activa. apagado. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0c555",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si la neurona A está activa todo el tiempo, obtienes un NO lógico: la neurona C está activa cuando la neurona B está apagada, y viceversa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3307101",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Puede imaginar cómo se pueden combinar estas redes para calcular expresiones lógicas complejas (vea los ejercicios al final del capítulo para ver un ejemplo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a80e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866f7c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El Perceptron es una de las arquitecturas ANN más simples, inventada en 1957 por Frank Rosenblatt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003176c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Se basa en una neurona artificial ligeramente diferente (consulte la figura 10-4) denominada unidad lógica de umbral (TLU) o, a veces, unidad de umbral lineal (LTU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb05a3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las entradas y salidas son números (en lugar de valores binarios de encendido/apagado), y cada conexión de entrada está asociada con un peso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a031ffe9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La TLU calcula una suma ponderada de sus entradas $(z = w_{1}x_{1} + w_{2}x_{2} + \\cdots + w_{n}x_{n} = x^{T}w) $, luego aplica una función de paso a esa suma y genera el resultado: $h_{w}(x) = step(z)$, donde $z = x^{T}w$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c9c684",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_10/figure_10_4.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8f166",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La función escalonada más común utilizada en los perceptrones es la función escalonada de Heaviside (consulte la ecuación 10-1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea44f6a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A veces se usa la función de signo en su lugar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5487b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 10-1. Common step functions used in Perceptrons (assuming threshold = 0)\n",
    "\n",
    "\n",
    "$$ \\text{heaviside}(z) = \\begin{cases} 0 & \\text{si}\\ z < 0 \\\\ 1 & \\text{si}\\ z \\geq 0 \\end{cases} $$\n",
    "\n",
    "$$ sgn(z) = \\begin{cases} -1 & \\text{si}\\ z < 0 \\\\ 0 & \\text{si}\\ z = 0 \\\\ 1 & \\text{si}\\ z > 0 \\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b528671",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Se puede utilizar una única TLU para la clasificación binaria lineal simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf819f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Calcula una combinación lineal de las entradas y, si el resultado supera un umbral, genera la clase positiva. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15574db2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "De lo contrario, genera la clase negativa (al igual que una regresión logística o un clasificador SVM lineal)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f316365",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Podría, por ejemplo, usar una sola TLU para clasificar las flores de iris en función de la longitud y el ancho de los pétalos (también agregando una función de sesgo adicional $x_{0} = 1$, tal como lo hicimos en capítulos anteriores)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b7603",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entrenar una TLU en este caso significa encontrar los valores correctos para $w_{0}, w_{1}$ y $w_{2}$ (el algoritmo de entrenamiento se analiza en breve)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a6fbb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Un Perceptron se compone simplemente de una sola capa de TLU, con cada TLU conectada a todas las entradas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62432e4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cuando todas las neuronas de una capa están conectadas a cada neurona de la capa anterior (es decir, sus neuronas de entrada), la capa se denomina capa totalmente conectada o capa densa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb88f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las entradas del Perceptron se alimentan a neuronas de paso especiales llamadas neuronas de entrada: emiten cualquier entrada que reciban."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e72611",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Todas las neuronas de entrada forman la capa de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ecdd67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Además, generalmente se agrega una función de sesgo adicional $(x_{0} = 1)$: generalmente se representa usando un tipo especial de neurona llamada neurona de sesgo, que genera 1 todo el tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd70642",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En la figura 10-5 se representa un perceptrón con dos entradas y tres salidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac924f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b97e5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este Perceptron puede clasificar instancias simultáneamente en tres clases binarias diferentes, lo que lo convierte en un clasificador de salida múltiple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf1e555",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thanks to the magic of linear algebra, Equation 10-2 makes it possible to efficiently compute the outputs of a layer of artificial neurons for several instances at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e797b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 10-2. Computing the outputs of a fully connected layer\n",
    "\n",
    "$$ h_{w,b}(X) = \\phi(XW+ b)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c84b19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this equation:  \n",
    "\n",
    "* As always, X represents the matrix of input features. It has one row per instance and one column per feature.\n",
    "* The weight matrix W contains all the connection weights except for the ones from the bias neuron. It has one row per input neuron and one column per artificial neuron in the layer.\n",
    "* The bias vector b contains all the connection weights between the bias neuron and the artificial neurons. It has one bias term per artificial neuron.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e6a511",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The function ϕ is called the activation function: when the artificial neurons are TLUs, it is a step function (but we will discuss other activation functions shortly)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1049cb76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, how is a Perceptron trained? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ed73c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Perceptron training algorithm proposed by Rosenblatt was largely inspired by Hebb’s rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc015b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In his 1949 book The Organization of Behavior (Wiley), Donald Hebb suggested that when a biological neuron triggers another neuron often, the connection between these two neurons grows stronger. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba229d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Siegrid Löwel later summarized Hebb’s idea in the catchy phrase, “Cells that fire together, wire together”; that is, the connection weight between two neurons tends to increase when they fire simultaneously. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0ca91",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This rule later became known as Hebb’s rule (or Hebbian learning). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a10b3c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Perceptrons are trained using a variant of this rule that takes into account the error made by the network when it makes a prediction; the Perceptron learning rule reinforces connections that help\n",
    "reduce the error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dabbf07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More specifically, the Perceptron is fed one training instance at a time, and for each instance it makes its predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c19387",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3012e9e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The rule is shown in Equation 10-3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9e2ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 10-3. Perceptron learning rule (weight update)\n",
    "\n",
    "$$ w_{i,j}^{(\\text{next step})} = w_{i,j} + \\eta(y_{j} - \\hat{y}:{j})x_{i} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced8cd6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this equation:   \n",
    "\n",
    "* $w_{i,j}$ is the connection weight between the $i^{th}$ input neuron and the $j^{th}$ output neuron.\n",
    "* $x_{i}$ is the $i$ input value of the current training instance.\n",
    "* $\\hat{y}^{j}$  is the output of the $j$ output neuron for the current training instance.\n",
    "* $y_{j}$ is the target output of the $j$ output neuron for the current training instance.\n",
    "* $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b4797",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The decision boundary of each output neuron is linear, so Perceptrons are incapable of learning complex patterns (just like Logistic Regression classifiers). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f936ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, if the training instances are linearly separable, Rosenblatt demonstrated that this algorithm would converge to a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf1cc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is called the Perceptron convergence theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa30615",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scikit-Learn provides a Perceptron class that implements a single-TLU network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90273fbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It can be used pretty much as you would expect—for example, on the iris dataset (introduced in Chapter 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582fcf8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int) # Iris setosa?\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5f92cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd8c8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6588ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "rise": {
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
