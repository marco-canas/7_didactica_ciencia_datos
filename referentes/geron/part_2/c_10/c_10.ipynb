{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6381fb7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9647bc3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Capítulo 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbdc194",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Artificial Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc8194",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los pájaros nos inspiraron a volar, las plantas de bardana inspiraron el velcro y la naturaleza ha inspirado innumerables inventos más."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f39b2f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Parece lógico, entonces, observar la arquitectura del cerebro en busca de inspiración sobre cómo construir una máquina inteligente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bfd0bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta es la lógica que desencadenó las redes neuronales artificiales (ANN): una ANN es un modelo de aprendizaje automático inspirado en las redes de neuronas biológicas que se encuentran en nuestros cerebros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694fd04b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sin embargo, aunque los aviones se inspiraron en las aves, no es necesario que aleteen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45f0da9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "De manera similar, las ANN se han vuelto gradualmente bastante diferentes de sus primos biológicos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b25e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Algunos investigadores incluso argumentan que deberíamos abandonar la analogía biológica por completo (por ejemplo, diciendo \"unidades\" en lugar de \"neuronas\"), para no restringir nuestra creatividad a sistemas biológicamente plausibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916f43d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las ANN están en el centro mismo del Aprendizaje Profundo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f966c1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Son versátiles, potentes y escalables, lo que los hace ideales para abordar tareas de aprendizaje automático grandes y muy complejas, como clasificar miles de millones de imágenes (por ejemplo, imágenes de Google), potenciar los servicios de reconocimiento de voz (por ejemplo, Siri de Apple), recomendar los mejores videos para ver a cientos de millones de usuarios todos los días (por ejemplo, YouTube), o aprender a vencer al campeón mundial en el juego de Go (AlphaGo de DeepMind)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7125be0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La primera parte de este capítulo presenta las redes neuronales artificiales, comenzando con un recorrido rápido por las primeras arquitecturas ANN y llegando a los perceptrones multicapa (MLP), que se usan mucho en la actualidad (se explorarán otras arquitecturas en los próximos capítulos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a8deb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En la segunda parte, veremos cómo implementar redes neuronales utilizando la popular API de Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989adde7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta es una API de alto nivel simple y bellamente diseñada para construir, entrenar, evaluar y ejecutar redes neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9857ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pero no se deje engañar por su simplicidad: es lo suficientemente expresivo y flexible como para permitirle construir una amplia variedad de arquitecturas de redes neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c429206",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "De hecho, probablemente sea suficiente para la mayoría de sus casos de uso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a2594",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Y si alguna vez necesita flexibilidad adicional, siempre puede escribir componentes Keras personalizados utilizando su API de nivel inferior, como veremos en el Capítulo 12."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8ecad3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pero primero, ¡retrocedamos en el tiempo para ver cómo surgieron las redes neuronales artificiales!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7f873",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# De las neuronas biológicas a las artificiales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ac33b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sorprendentemente, las ANN existen desde hace bastante tiempo: fueron introducidas por primera vez en 1943 por el neurofisiólogo Warren McCulloch y el matemático Walter Pitts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b25b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En su artículo histórico \"Un cálculo lógico de ideas inmanentes en la actividad nerviosa\", McCulloch y Pitts presentaron un modelo computacional simplificado de cómo las neuronas biológicas podrían trabajar juntas en cerebros de animales para realizar cálculos complejos utilizando la lógica proposicional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9172e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esta fue la primera arquitectura de red neuronal artificial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dd68ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Desde entonces se han inventado muchas otras arquitecturas, como veremos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423c86b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los primeros éxitos de las ANN llevaron a la creencia generalizada de que pronto estaríamos conversando con máquinas verdaderamente inteligentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0fc049",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cuando quedó claro en la década de 1960 que esta promesa no se cumpliría (al menos durante bastante tiempo), la financiación se fue a otra parte y las ANN entraron en un largo invierno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f00d1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A principios de la década de 1980, se inventaron nuevas arquitecturas y se desarrollaron mejores técnicas de entrenamiento, lo que provocó un resurgimiento del interés por el conexionismo (el estudio de las redes neuronales)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb6ab3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pero el progreso fue lento, y en la década de 1990 se inventaron otras poderosas técnicas de aprendizaje automático, como las máquinas de soporte vectorial (consulte el Capítulo 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b58b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estas técnicas parecían ofrecer mejores resultados y bases teóricas más sólidas que las ANN, por lo que una vez más se suspendió el estudio de las redes neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ec9dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are now witnessing yet another wave of interest in ANNs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb6b6e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Will this wave die out like the previous ones did? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144e48d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Well, here are a few good reasons to believe that this time is different and that the renewed interest in ANNs will have a much more profound impact on our lives:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f629b6ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* There is now a huge quantity of data available to train neural networks, and ANNs frequently  \n",
    "  outperform other ML techniques on very large and complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9958f85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The tremendous increase in computing power since the 1990s now makes it possible to train large neural networks in a reasonable amount of time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0fbc9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is in part due to Moore’s law (the number of components in integrated circuits has doubled about every 2 years over the last 50 years), but also thanks to the gaming industry, which has stimulated the production of powerful GPU cards by the millions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55dfec1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Moreover, cloud platforms have made this power accessible to everyone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd252b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The training algorithms have been improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a3de1b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To be fair they are only slightly different from the ones used in the 1990s, but these relatively small tweaks have had a huge positive impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b06609",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some theoretical limitations of ANNs have turned out to be benign in practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c736462",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, many people thought that ANN training algorithms were doomed because they were likely to get stuck in local optima, but it turns out that this is rather rare in practice (and when it is the case, they are usually fairly close to the global optimum)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c19ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "ANNs seem to have entered a virtuous circle of funding and progress. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af808279",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Amazing products based on ANNs regularly make the headline news, which pulls more and more attention and funding toward them, resulting in more and more progress and even more amazing products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1629102",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Biological Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44430361",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before we discuss artificial neurons, let’s take a quick look at a biological neuron (represented in Figure 10-1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3cbc1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is an unusual-looking cell mostly found in animal brains. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33239ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It’s composed of a cell body containing the nucleus and most of the cell’s complex components, many branching extensions called dendrites, plus one very long extension called the axon. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b4bee7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The axon’s length may be just a few times longer than the cell body, or up to tens of thousands of times longer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80718fd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Near its extremity the axon splits off into many branches called telodendria, and at the tip of these branches are minuscule structures called synaptic terminals (or simply synapses), which are connected to the dendrites or cell bodies of other neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b72f41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Biological neurons produce short electrical impulses called action potentials (APs, or just signals) which travel along the axons and make the synapses release chemical signals called neurotransmitters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18730164",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When a neuron receives a sufficient amount of these neurotransmitters within a few milliseconds, it\n",
    "fires its own electrical impulses (actually, it depends on the neurotransmitters, as some of them inhibit the neuron from firing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8502fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_10/figure_10_1.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa315749",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thus, individual biological neurons seem to behave in a rather simple way, but they are organized in a vast network of billions, with each neuron typically connected to thousands of other neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6942101c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Highly complex computations can be performed by a network of fairly simple neurons, much like a complex anthill can emerge from the combined efforts of simple ants. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6847f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The architecture of biological neural networks (BNNs) is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive\n",
    "layers, especially in the cerebral cortex (i.e., the outer layer of your brain), as shown in Figure 10-2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b07d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_10/figure_10_2.jpg?raw=true'> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db42db83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logical Computations with Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f571b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "McCulloch and Pitts proposed a very simple model of the biological neuron, which later became known as an artificial neuron: it has one or more binary (on/off) inputs and one binary output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423966cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The artificial neuron activates its output when more than a certain number of its inputs are active."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9f1809",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In their paper, they showed that even with such a simplified model it is possible to build a network of artificial neurons that computes any logical proposition you want. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44367f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To see how such a network works, let’s build a few ANNs that perform various logical computations (see Figure 10-3), assuming that a neuron is activated when at least two of its inputs are active."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa93e12b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_10/figure_10_3.jpg?raw=true'> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f3ed4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s see what these networks do:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f667262",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The first network on the left is the identity function: if neuron A is activated, then neuron C gets activated as well (since it receives two input signals from neuron A); but if neuron A is off, then\n",
    "neuron C is off as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00425fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The second network performs a logical AND: neuron C is activated only when both neurons A and B are activated (a single input signal is not enough to activate neuron C)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972015f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The third network performs a logical OR: neuron C gets activated if either neuron A or neuron B is activated (or both)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cddc43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Finally, if we suppose that an input connection can inhibit the neuron’s activity (which is the case with biological neurons), then the fourth network computes a slightly more complex logical proposition: neuron C is activated only if neuron A is active and neuron B is off. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0c555",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If neuron A is active all the time, then you get a logical NOT: neuron C is active when neuron B is off, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3307101",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can imagine how these networks can be combined to compute complex logical expressions (see the exercises at the end of the chapter for an example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a80e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866f7c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Perceptron is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003176c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is based on a slightly different artificial neuron (see Figure 10-4) called a threshold logic unit (TLU), or sometimes a linear threshold unit (LTU). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb05a3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The inputs and output are numbers (instead of binary on/off values), and each input connection is associated with a weight. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a031ffe9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The TLU computes a weighted sum of its inputs $(z = w_{1}x_{1} + w_{2}x_{2} + \\cdots + w_{n}x_{n} = x^{T}w)$, then applies a step function to that sum and outputs the result: $h_{w}(x) = step(z)$, where $z = x^{T}w$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c9c684",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_10/figure_10_4.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8f166",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The most common step function used in Perceptrons is the Heaviside step function (see Equation 10-1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea44f6a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sometimes the sign function is used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5487b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 10-1. Common step functions used in Perceptrons (assuming threshold = 0)\n",
    "\n",
    "\n",
    "$$ \\text{heaviside}(z) = \\begin{cases} 0 & \\text{si}\\ z < 0 \\\\ 1 & \\text{si}\\ z \\geq 0 \\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b97e5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6192c964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "rise": {
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
