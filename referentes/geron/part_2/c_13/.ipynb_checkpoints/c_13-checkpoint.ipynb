{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d5a60e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/didactica_ciencia_datos/blob/main/referentes/geron/part_2/c_13/c_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b1db9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Capítulo 13. Carga y preprocesamiento de datos con TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1893a408",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hasta ahora, solo hemos utilizado conjuntos de datos que caben en la memoria, pero los sistemas de aprendizaje profundo a menudo se entrenan en conjuntos de datos muy grandes que no caben en la RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbea151",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ingerir un gran conjunto de datos y preprocesarlo de manera eficiente puede ser complicado de implementar con otras bibliotecas de aprendizaje profundo,  \n",
    "\n",
    "* pero TensorFlow lo hace fácil gracias a la API de datos: simplemente crea un objeto de conjunto de datos y le indica dónde obtener los datos y cómo transformarlos. ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34446f1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "TensorFlow se ocupa de todos los detalles de la implementación, como subprocesos múltiples, colas, procesamiento por lotes y captación previa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b730ca11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¡Además, la API de datos funciona a la perfección con `tf.keras`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c8b9c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "De forma estándar, la API de datos puede leer \n",
    "\n",
    "* archivos de texto (como archivos CSV), \n",
    "* archivos binarios con registros de tamaño fijo y \n",
    "* archivos binarios que usan el formato TFRecord de TensorFlow, que admite registros de diferentes tamaños."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b70117",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "TFRecord es un formato binario flexible y eficiente basado en Protocol Buffers (un formato binario de código abierto)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390a0b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La API de datos también tiene soporte para leer desde bases de datos SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863d548",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Además, muchas extensiones de código abierto están disponibles para leer todo tipo de fuentes de datos, como el servicio BigQuery de Google."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd0b40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Leer grandes conjuntos de datos de manera eficiente no es la única dificultad: los datos también deben ser preprocesados, generalmente normalizados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce04f27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Además, no siempre se compone estrictamente de campos numéricos convenientes: puede haber características de texto, características categóricas, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b65255e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estos deben codificarse, por ejemplo, mediante codificación one-hot, codificación de bolsa de palabras o incrustaciones (como veremos, una incrustación es un vector denso entrenable que representa una categoría o token). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf48d85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una opción para manejar todo este preprocesamiento es escribir sus propias capas de preprocesamiento personalizadas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d72289d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Otra es utilizar las capas de preprocesamiento estándar proporcionadas por Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98660a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En este capítulo, cubriremos la API de datos, el formato TFRecord y cómo crear capas de preprocesamiento personalizadas y usar las estándar de Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c973b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "También echaremos un vistazo rápido a algunos proyectos relacionados del ecosistema TensorFlow:\n",
    "Transformación TF (`tf.Transform`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e6bcb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hace posible escribir una única función de preprocesamiento que se puede ejecutar en modo por lotes en su conjunto de entrenamiento completo, antes del entrenamiento (para acelerarlo), y luego exportarlo a una función TF e incorporarlo a su modelo entrenado para que una vez que se implemente en producción, puede encargarse de preprocesar nuevas instancias sobre la marcha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0062609a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TF Datasets (TFDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4da6f51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Provides a convenient function to download many common datasets of all kinds, including large ones like ImageNet, as well as convenient dataset objects to manipulate them using the Data API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd9f39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So let’s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec84b26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Data API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f58ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Toda la API de datos gira en torno al concepto de un conjunto de datos: como puede sospechar, esto representa una secuencia de elementos de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e0570",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por lo general, usará conjuntos de datos que leen datos del disco gradualmente, pero para simplificar, creemos un conjunto de datos completamente en RAM usando `tf.data.Dataset.from_tensor_slices()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd91066",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b4cce4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(10)     # any data tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf751ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La función `from_tensor_slices()` toma un tensor y crea un `tf.data.Dataset` cuyos elementos son todos los segmentos de `X` (a lo largo de la primera dimensión), por lo que este conjunto de datos contiene 10 elementos: tensores $0, 1, 2, \\ldots, 9$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24dda6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this case we would have obtained the same dataset if we had used `tf.data.Dataset.range(10)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0b93e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can simply iterate over a dataset’s items like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "333392f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ca5260",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chaining Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc8f0ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once you have a dataset, you can apply all sorts of transformations to it by calling its transformation methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8228b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each method returns a new dataset, so you can chain transformations like this (this chain is illustrated in Figure 13-1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1742b84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d028ff53",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this example, we first call the `repeat()` method on the original dataset, and it returns a new dataset that will repeat the items of the original dataset three times. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290d97b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Of course, this will not copy all the data in memory three times! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139f3485",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(If you call this method with no arguments, the new dataset will repeat the source dataset forever, so the code that iterates over the dataset will have to decide when to stop.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b814d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then we call the `batch()` method on this new dataset, and again this creates a new dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f926d2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This one will group the items of the previous dataset in batches of seven items. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb00fe7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, we iterate over the items of this final dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b6434",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As you can see, the `batch()` method had to output a final batch of size two instead of seven, but you can call it with `drop_remainder=True` if you want it to drop this final batch so that all batches have the exact same size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc02f512",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7392f9a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The dataset methods do not modify datasets, they create new ones, so make sure to keep a reference to these new datasets (e.g., with dataset = ...), or else nothing will happen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8dd198",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can also transform the items by calling the `map()` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55fa611",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, this creates a new dataset with all items doubled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c68568f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5fa086",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This function is the one you will call to apply any preprocessing you want to your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb338a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sometimes this will include computations that can be quite intensive, such as reshaping or rotating an image, so you will usually want to spawn multiple threads to speed things up: it’s as simple as setting the `num_parallel_calls` argument. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62d0db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that the function you pass to the `map()` method must be convertible to a TF Function (see Chapter 12)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3e734",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While the `map()` method applies a transformation to each item, the `apply()` method applies a transformation to the dataset as a whole. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeac522",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, the following code applies the `unbatch()` function to the dataset (this function is currently experimental, but it will most likely move to the core API in a future release). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed35677",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each item in the new dataset will be a single-integer tensor instead of a batch of seven integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9282e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.apply(tf.data.experimental.unbatch()) # Items: 0,2,4,..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9622666",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is also possible to simply filter the dataset using the filter() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf995d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654d5ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You will often want to look at just a few items from a dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb922ab4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can use the take() method for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1843435",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd39c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "tf.Tensor(0, shape=(), dtype=int64)\n",
    "tf.Tensor(2, shape=(), dtype=int64)\n",
    "tf.Tensor(4, shape=(), dtype=int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ba56f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shuffling the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc08afa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As you know, Gradient Descent works best when the instances in the training set are independent and identically distributed (see Chapter 4). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1005e0c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A simple way to ensure this is to shuffle the instances, using the `shuffle()` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4ce0ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It will create a new dataset that will start by filling up a buffer with the first items of the source dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc44f27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then, whenever it is asked for an item, it will pull one out randomly from the buffer and replace it with a fresh one from the source dataset, until it has iterated entirely through the source dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c42fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At this point it continues to pull out items randomly from the buffer until it is empty. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1855c03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You must specify the buffer size, and it is important to make it large enough, or else shuffling will not be very effective. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72adf443",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Just don’t exceed the amount of RAM you have, and even if you have plenty of it, there’s no need to go beyond the dataset’s size. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904a6ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can provide a random seed if you want the same random order every time you run your program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a595bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, the following code creates and displays a dataset containing the integers 0 to 9, repeated 3 times, shuffled using a buffer of size 5 and a random seed of 42, and batched with a batch size of 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aae2b032",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d3443d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
    "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
    "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
    "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
    "tf.Tensor([3 6], shape=(2,), dtype=int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46c120",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b45608",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you call repeat() on a shuffled dataset, by default it will generate a new order at every iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797ea92e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is generally a good idea, but if you prefer to reuse the same order at each iteration (e.g., for tests or debugging), you can set reshuffle_each_iteration=False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a406a2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For a large dataset that does not fit in memory, this simple shuffling-buffer approach may not be sufficient, since the buffer will be small compared to the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c89cd69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One solution is to shuffle the source data itself (for example, on Linux you can shuffle text files using the shuf command). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3b3fac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This will definitely improve shuffling a lot! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849df330",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Even if the source data is shuffled, you will usually want to shuffle it some more, or else the same order will be repeated at each epoch, and the model may end up being biased (e.g., due to some spurious patterns present by chance in the source data’s order). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093b586f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To shuffle the instances some more, a common approach is to split the source data into multiple files, then read them in a random order during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4b3ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, instances located in the same file will still end up close to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62be979",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To avoid this you can pick multiple files randomly and read them simultaneously, interleaving their records. Then on top of that you can add a shuffling buffer using the `shuffle()` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6e53b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If all this sounds like a lot of work, don’t worry: the Data API makes all this possible in just a few lines of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d36edb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s see how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21052690",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interleaving lines from multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab259da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, let’s suppose that you’ve loaded the California housing dataset, shuffled it (unless it was already shuffled), and split it into a training set, a validation set, and a test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bbe227",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then you split each set into many CSV files that each look like this (each row contains eight input features plus the target median house value):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317dfd8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MedInc,HouseAge,AveRooms,AveBedrms,Popul,AveOccup,Lat,Long,MedianHouseValue\n",
    "3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442\n",
    "5.3275,5.0,6.4900,0.9910,3464.0,3.4433,33.69,-117.39,1.687\n",
    "3.1,29.0,7.5423,1.5915,1328.0,2.2508,38.44,-122.98,1.621\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743b04c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s also suppose train_filepaths contains the list of training file paths (and you also have valid_filepaths and test_filepaths):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f037788e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_filepaths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_filepaths\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_filepaths' is not defined"
     ]
    }
   ],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e7b074",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "['datasets/housing/my_train_00.csv', 'datasets/housing/my_train_01.csv',...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6bdac2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Alternatively, you could use file patterns; for example, `train_filepaths = \"datasets/housing/my_train_*.csv\"`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea49246",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let’s create a dataset containing only these file paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a67bd6d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_filepaths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m filepath_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mlist_files(\u001b[43mtrain_filepaths\u001b[49m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_filepaths' is not defined"
     ]
    }
   ],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a82cc61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By default, the list_files() function returns a dataset that shuffles the file paths. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b136b35d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In general this is a good thing, but you can set `shuffle=False` if you do not want that for some reason."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896bdae6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, you can call the `interleave()` method to read from five files at a time and interleave their lines (skipping the first line of each file, which is the header row, using the `skip()` method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9752d57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filepath_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m n_readers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mfilepath_dataset\u001b[49m\u001b[38;5;241m.\u001b[39minterleave(\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m filepath: tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTextLineDataset(filepath)\u001b[38;5;241m.\u001b[39mskip(\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m      4\u001b[0m     cycle_length\u001b[38;5;241m=\u001b[39mn_readers)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filepath_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bf0201",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The interleave() method will create a dataset that will pull five file paths from the `filepath_dataset`, and for each one it will call the function you gave it (a lambda in this example) to create a new dataset (in this case a TextLineDataset). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a91655",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To be clear, at this stage there will be seven datasets in all: the filepath dataset, the interleave dataset, and the five TextLineDatasets created internally by the interleave dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ff8ca0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When we iterate over the interleave dataset, it will cycle through these five TextLineDatasets, reading one line at a time from each until all datasets are out of items. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0bb08d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then it will get the next five file paths from the filepath_dataset and interleave them the same way, and so on until it runs out of file paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b1848",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc86f33d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For interleaving to work best, it is preferable to have files of identical length; otherwise the ends of the longest files will not be interleaved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59af95a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By default, interleave() does not use parallelism; it just reads one line at a time from each file, sequentially. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06244df3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you want it to actually read files in parallel, you can set the num_parallel_calls argument to the number of threads you want (note that the map() method also has this argument). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88057101",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can even set it to tf.data.experimental.AUTOTUNE to make TensorFlow choose the right number of threads dynamically based on the available CPU (however, this is an experimental feature for now). Let’s look at what the dataset contains now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d86d9fb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(line\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17e05d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'\n",
    "b'4.1812,52.0,5.7013,0.9965,692.0,2.4027,33.73,-118.31,3.215'\n",
    "b'3.6875,44.0,4.5244,0.9930,457.0,3.1958,34.04,-118.15,1.625'\n",
    "b'3.3456,37.0,4.5140,0.9084,458.0,3.2253,36.67,-121.7,2.526'\n",
    "b'3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d146d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These are the first rows (ignoring the header row) of five CSV files, chosen randomly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfee63a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Looks good! But as you can see, these are just byte strings; we need to parse them and scale the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572abe8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc854cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s implement a small function that will perform this preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "601a0345",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_mean, X_std \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;66;03m# mean and scale of each feature in the training set\u001b[39;00m\n\u001b[0;32m      2\u001b[0m n_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(line):\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "X_mean, X_std = [...] # mean and scale of each feature in the training set\n",
    "n_inputs = 8\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ad4ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Let’s walk through this code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45955f67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, the code assumes that we have precomputed the mean and standard deviation of each feature in the training set. X_mean and\n",
    "X_std are just 1D tensors (or NumPy arrays) containing eight floats, one per input feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799ca545",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The preprocess() function takes one CSV line and starts by parsing it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac20b73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For this it uses the tf.io.decode_csv() function, which takes two arguments: the first is the line to parse, and the second is an array containing the default value for each column in the CSV file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a10885b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This array tells TensorFlow not only the default value for each column, but also the number of columns and their types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a81cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this example, we tell it that all feature columns are floats and that missing values should default to 0, but we provide an empty array of type `tf.float32` as the default value for the last column (the target): the array tells TensorFlow that this column contains floats, but that there is no default value, so it will raise an exception if it encounters a missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0c6f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `decode_csv()` function returns a list of scalar tensors (one per column), but we need to return 1D tensor arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fda8eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So we call `tf.stack()` on all tensors except for the last one (the target): this will stack these tensors into a 1D array. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6519b2f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We then do the same for the target value (this makes it a 1D tensor array with a single value, rather than a scalar tensor)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba42da91",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, we scale the input features by subtracting the feature means and then dividing by the feature standard deviations, and we return a tuple containing the scaled features and the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c5b4cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s test this preprocessing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abad55e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346c6224",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(<tf.Tensor: id=6227, shape=(8,), dtype=float32, numpy=\n",
    "array([ 0.16579159, 1.216324 , -0.05204564, -0.39215982, -0.5277444 ,\n",
    "-0.2633488 , 0.8543046 , -1.3072058 ], dtype=float32)>,\n",
    "<tf.Tensor: [...], numpy=array([2.782], dtype=float32)>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9473b923",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Looks good! We can now apply the function to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01f99f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Putting Everything Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70676b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To make the code reusable, let’s put together everything we have discussed so far into a small helper function: it will create and return a dataset that will efficiently load California housing data from multiple CSV files, preprocess it, shuffle it, optionally repeat it, and batch it (see Figure 13-2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60c311e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655213b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Everything should make sense in this code, except the very last line (prefetch(1)), which is important for performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b31765",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Figura 13.2\n",
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e963fc71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Captación previa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca5d40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By calling prefetch(1) at the end, we are creating a dataset that will do its best to always be one batch ahead. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbcddef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In other words, while our training algorithm is working on one batch, the dataset will already be working in parallel on getting the next batch ready (e.g., reading the data from disk and preprocessing it). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa20618",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This can improve performance dramatically, as is illustrated in Figure 13-3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25adda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we also ensure that loading and preprocessing are multithreaded (by setting num_parallel_calls when calling interleave() and map()), we can exploit multiple cores on the CPU and hopefully make preparing one batch of data shorter than running a training step on the GPU: this way the GPU will be almost 100% utilized (except for the data transfer time from the CPU to the GPU ), and training will run much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d261d51c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "figure 13.3\n",
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed624ee8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc3c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a99cdef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc331365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47431096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d571d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ed750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "rise": {
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
