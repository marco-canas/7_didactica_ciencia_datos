{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de4b638f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/taca/blob/main/ref/charu/7_ch/7_1_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marco-canas/taca/blob/main/ref/charu/7_ch/7_1_introduction.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d52fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.1 Eigenvectors and diagonalizable matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca8923",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\"Las matemáticas son el arte de dar el mismo nombre a diferentes cosas\".  \n",
    "- Henri Poincaré"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499e2ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Video de apoyo]() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a2d88",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4e935e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cualquier matriz cuadrada $A$ de tamaño $d \\times d$ puede considerarse un operador lineal, que mapea el vector columna $d$-dimensional $\\overline{x}$ en un vector $d$-dimensional $A\\overline{x}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8594ea7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una transformación lineal $A\\overline{x}$ es una combinación de operaciones como rotaciones, reflejos y escalamiento de un vector $\\overline{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22dff33",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una matriz diagonalizable es un tipo especial de operador lineal que solo corresponde a un escalado simultáneo a lo largo de $d$ dimensiones diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c682c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estas $d$ direcciones diferentes se denominan vectores propios y los factores de escala $d$ se denominan valores propios. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185edbeb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Todas estas matrices se pueden descomponer usando una matriz $V$ de orden $d \\times d$ invertible y una matriz diagonal $\\Delta$ de orden $d \\times d$:  \n",
    "$$ A = V \\Delta V^{T}. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daede81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las columnas de $V$ contienen $d$ autovectores y las entradas diagonales de $\\Delta$ contienen los autovalores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0eed57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para cualquier $x \\in R^{d}$, se puede interpretar geométricamente $Ax$ usando la descomposición en términos de una secuencia de tres transformaciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd3587",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(i) La multiplicación de $x$ con $V^{−1}$ calcula el coordenadas de $x$ en un sistema de base (posiblemente no ortogonal) correspondiente a las columnas (vectores propios) de $V$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec18fe1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(ii) la multiplicación de $V^{−1}x$ con $\\Delta$ para crear $\\Delta V^{−1}x$ dilata estas coordenadas con factores de escala en $\\Delta$ en las direcciones del vector propio, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757de25f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(iii) la multiplicación final con $V$ para crear $V\\Delta V^{−1}x$ transforma las coordenadas de nuevo al sistema de base original (es decir, la base estándar)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dd497d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El resultado general es una escala anisotrópica en direcciones de vectores propios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca388e7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las transformaciones lineales que se pueden representar de esta manera corresponden a matrices diagonalizables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9433d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una matriz diagonalizable $d \\times d$ representa una transformación lineal correspondiente al escalado anisotrópico en $d$ direcciones linealmente independientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0006bb4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cuando las columnas de la matriz $V$ son vectores ortonormales, tenemos $V^{−1} = V^{T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e818568",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En tal caso, el escalado se realiza a lo largo de direcciones mutuamente ortogonales y la matriz $A$ es siempre simétrica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a131d5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto se debe a que tenemos $A^{T} = V \\Delta^{T} V^{T} = V\\Delta V^{T} = A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851d5490",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los dos casos de escalado anisotrópico con sistemas de base ortogonal y sistemas de base no ortogonal se muestra en la Figura 3.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020a180",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/taca/blob/main/ref/charu/3_ch/fig_3_1.PNG?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c7581d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aquí, los factores de escala en las dos direcciones son 0,5 y 1, que corresponden a la contracción y la dilatación, respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df848092",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este capítulo estudia las propiedades de los vectores propios, las matrices diagonalizables y sus\n",
    "aplicaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d7a6e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El concepto de determinante se introduce en la Sección 3.2. Los conceptos de diagonalización, autovectores y autovalores se analizan en la sección 3.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d50cf7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El caso especial de matrices simétricas también se analiza en esta sección. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78b35e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aplicaciones de aprendizaje automático y en la Sección 3.4 se dan ejemplos de matrices simétricas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1974ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los algoritmos numéricos para encontrar autovectores y autovalores de matrices diagonalizables se discuten en la Sección 3.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677014e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Se ofrece un resumen en la Sección 3.6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73cef32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.2 Determinants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6402fb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b0f8532",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Referentes  \n",
    "\n",
    "* Charú. Linear Algebra and Optimizacion for Machine Learning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2415006f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Charu C. Aggarwal es miembro distinguido del personal de investigación (DRSM) del IBM T. J. Watson Research Center en Yorktown Heights, Nueva York.   \n",
    "\n",
    "Completó su licenciatura en Ciencias de la Computación del Instituto Indio de Tecnología en Kanpur en 1993 y su Ph.D. en Investigación de Operaciones del Instituto de Tecnología de Massachusetts en 1996.  \n",
    "Ha publicado más de 400 artículos en conferencias y revistas arbitradas y ha solicitado o obtenido más de 80 patentes.   \n",
    "\n",
    "Es autor o editor de 19 libros, incluidos libros de texto sobre minería de datos, redes neuronales, aprendizaje automático (para texto), sistemas de recomendación y análisis de valores atípicos. Debido al valor comercial de sus patentes, tres veces ha sido designado Master Inventor en IBM. Ha recibido varios premios internos y externos, incluido el premio EDBT Test-of-Time Award (2014), el premio IEEE ICDM Research Contributions (2015) y el ACM SIGKDD Innovation Award (2019). Se ha desempeñado como editor en jefe de ACM SIGKDD Explorations, y actualmente se desempeña como editor en jefe de ACM Transactions on Knowledge Discovery from Data. Es miembro de SIAM, ACM e IEEE, por \"contribuciones al descubrimiento de conocimiento y algoritmos de minería de datos\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa1e16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "rise": {
   "enable_chalkboard": true,
   "theme": "sky",
   "transition": "zoom"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
